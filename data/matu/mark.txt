Mittels einer Corporate Web Homepage präsentiert die seerow GmbH Ihr Geschäft im optimalen Licht. Mit dem Einsatz von up-to-date Softwarelösungen erhalten Sie mit mit uns über einen kompetenten Partner für brilliante Weblösungen. Bei der Umsetzung von Unternehmens Homepages sützt die seerow GmbH sich auf performante und hochwertige CMS Solutions. Bei der Umsetzung versichern wir Ihnen höchste technologisch fortschrittliche Stabilität verknüpft mit unkomplizierter Bedienung des Content Management Systems. Mit überzeugendsten Ci Websites erhalten Sie mehr Aufmerksamkeit für Ihre Unternehmung. Durch eine Implementierung mit Perspektive auf Mobilen Geräten präsentiert sich jede Corporate Homepage geräteunabhängig perfekt.Wir programmieren mit modernsten Frameworks: Website erstellen Aarau Website erstellen Aarau: Laravel, Go und AngualrJs sind unsere lieblings Arbeitsumgebungen. Mit diesen bauen wir von der seerow GmbH Ihnen reaktionsfähige Frontendsysteme, welche einfach skalierbar sind und allen Funktionalitäten gewachsen sind. Angular finden wir ein vortreffliches Javascript Framework. AngularJs stellt für uns ein Toolset bereit, mit diesem wir Web Applikationen flexibel entwickeln können. Dieses geht einwandfrei mit unzähligen Libraries im Einklang und bietet einen einzigartigen Arbeitsablauf. Zusätzlich arbeiten wir auch extensiv mit dem Opensource Framework Laravel und möchten uns dieses nicht mehr aus dem seerow Alltag herausdenken. Durch dieses PHP Framework bekommt jedes Backendprojekt performante Anpassungsfähigkeit und das Homepage Toolkit ermöglicht eine effiziente Programmierung von allen Website Projekten. seerow setzt komplexe Webseitestrukturen im Frontend mit Frameworks wie AngularJs, KnockoutJs, oder React um. Wir besitzen produktives Software-Engineering und können dabei auf langjährige Erfahrungen im Webbereich zurückgreifen, welches wir mit Architekturstrukturen der Applikationsentwicklung verknüpfen. Wir entwickeln up-to-date Webbasierte Applikationen und entwickeln individuelle Weblösungen, welche skalierbar, performant und fortschrittlich sind. Backend: Konzeptabwicklung für agile Backendsysteme Laravel Zuerich: Bei jeglichen Projekten bieten wir unseren Kunden hochwertige Content Management Systeme für die umstandslose Eingabe von Web Daten. Auf Anfrage bieten wir das Know-How individuelle CMS Systeme aus eigener Entwicklung zu verwenden, welche Skalierbarkeit aufweisen und für grössere Projekte erstellt werden können. Die verwendeten Technologien sind dabei PHP, Wordpress, PHP Laravel, Go Lang, wie auch NoJs Backends für die Datenverwaltung. Sollen beispielsweise performante API Schnittstellen erstellt werden, haben wir mit unserer Erfahrung in Go Lang die ideale Voraussetzung für eine erfolgreiche Projektumsetzung. zufriedene Kunden & erfolgreiche Projekte Eine Zusammenarbeit mit seerow bedeutet die Abwicklung von erfolgreichen Projekten, eine kundenzentrierte Zusammenarbeit mit dem Auge fürs Detail und der Erfahrung für fristgerechte Erfüllung. Kontaktieren Sie uns für Ihre individuelle Softwareentwicklung. Webapps im Appstore Appentwicklung in Solothurn: seerow bietet langjährige Erfahrung in Web- und App Entwicklung. Durch Hybride App Entwicklung wurde es Möglich das Know-how des Webs mit in den Entwicklungszyklus von Apps zunehmen. Zusammen wurde es möglich eine umfängliche Abdeckung zu bieten und einen Kundenzentrierten Produktionszyklus zu entwickeln. Zusammen mit seerow wird es möglich Prototypen orientierte Apps zu entwickeln, welche schnelle Entwicklungszeiten bieten und beliebig erweiterbar sind. Appentwicklung in Solothurn: Was ist Cordova cross platform development Cordova ist eine Open Source API, welche es erlaubt native Apps in der Architektur des Webs zu bauen. Darin eingeschlossen ist CSS3, HTML5, wie auch Javascript. Die Idee besteht darin, eine App Codebase zu erstellen, welche danach in die Zielplattform Cross Kompiliert wird. Diese Cross Kompilierung erlaubt nicht nur kürzere Entwicklungszyklen, sondern macht die Erstellung, oder die Anpassung an Zielsysteme obsolet. Der Rollout von Enterprise Systemen wird dadurch deutlich vereinfacht und Updates können gleichzeitig an alle Zielsysteme gesendet werden. Native Erfahrung mit AngularJs entwickelt Der Vorteil von AngularJS wurde durch viele erfolgreiche Projekte bestätigt. MVC, Two Way Data-Binding, Template Engine, Dependency Injection und Directives sind die Key Features von AngularJS, durch welche es ein überragendes Front-end Framework wurde. Dies für eine App Entwicklung zu benutzen bringt mit sich, dass Technologien, welche in der Web Entwicklung zur Perfektion herangereift sind nun auch in nativen Apps zum tragen kommen. Dies gibt Front-end Entwicklern die Möglichkeit Webentwicklung nun einfach auch auf Apps anzuwenden, diese nach Webstandards zu verfeinern und dennoch alle Vorteile von Nativen Apps zu erfüllen. Produktionsreife Webentwicklung mit Appintegration Appentwicklung in Solothurn: seerow entwickelt plattformunabhängige Applikationen und Websites für mobile Endgeräte. Die mobile Internet Nutzung hat in den letzten Jahren stark zugenommen und im Alltag etabliert. Die meisten Kunden verwenden dabei unterschiedlichste Geräte und Betriebssysteme von unterschiedlichen Herstellern. Damit diese innovative und nachhaltige Lösungen entstehen können erarbeiten wir gemeinsam mit unseren Kunden innovative und nachhaltige Lösungen. Cross-Plattform Entwicklung mit Ionic Ionic ist eine Open Source Bibliothek, welche auf HTML, CSS und Javascript Komponenten optimiert ist. Ionic eignet sich besonders für hoch interaktive, native Web Applikationen. Eingebunden mit Sass und optimiert mit AngularJS können robuste und performante Webapps gebaut werden, welche nicht nur sehr gut aussehen, sondern auch eine robuste Architektur bieten. Vorteile von Ionic mobile apps Appentwicklung in Solothurn Performance orientiert Einbindung von Angular & Ionic Fokus auf Nativer Einbindung Überragendes Design Funktionale Optimierung Mobile Webapp Integration möglich Jegliche Designmöglichkeiten Vorteile von Hybrider Entwicklung Tiefe Unterhaltkosten Schnelle Prototypisierung Umfassende Beratungsleistung Der Begriff Codebasis (englisch codebase) im Bereich der Softwaretechnik bezeichnet die Gesamtheit der zu einem Projekt gehörenden Quelltextdateien sowie eventuell dazugehöriger Konfigurationsdateien. Darunter fallen aber auch diverse Dateien anderer Art, welche für den Vorgang des Kompilierens benötigt werden, z. B. sog. Makefiles. Hinreichend große Codebasen werden i. d. R. mittels Versionierungssystemen verwaltet. Im Falle von quelloffener Software sind die Codebasen meist in Form von entsprechenden Repositories öffentlich zugänglich. Der Begriff eignet sich aber auch dazu, um eine Aussage über die Beziehung bzw. Historie verschiedener, dennoch ähnlicher, Projekte zu machen. So spricht man z. B. im Falle einer Abspaltung (Fork) davon, dass es eine gemeinsame Codebasis gibt, während es im Falle einer unabhängigen Entwicklung eine eben solche nicht gibt. Als Login oder Log-in (von engl.: (to) log in = „einloggen“, „anmelden“ → „[Benutzer-]Anmeldung“, auch: Sign-in/Sign-on, Log-on usf.) wird der Vorgang bezeichnet, sich in einem Computersystem bei einem speziellen Dienst anzumelden (einzuloggen). Gewöhnlich dient der Vorgang dazu, dem System mitzuteilen, dass nun eine Sitzung beginnt und dass der Benutzer mit einem Benutzerkonto verknüpft werden möchte.Das Einloggen in ein System erfolgt oft dadurch, dass ein Benutzername und ein Passwort abgefragt werden. Nach erfolgter Authentifizierung erhält der Benutzer einen personalisierten Zugang zu dem System, mit Berechtigungen, die durch ein Benutzerprofil definiert werden. Mit dem Login beginnt eine sogenannte Sitzung, die durch ein Logout beendet wird.Neben der Form des interaktiven Logins wird der Begriff auch in Bezug auf das Anmelden eines Rechners in einem Netz via DHCP, SMB, RADIUS oder einem anderen Protokoll verwendet. In Quizsendungen wird der Begriff einloggen auch als „verbindliche Eingabe für Quizantworten in den Computer“ verwendet. .NET Framework ist ein Teil von Microsofts Software-Plattform .NET und dient der Entwicklung und Ausführung von Anwendungsprogrammen. Das .NET Framework besteht aus einer Laufzeitumgebung (Common Language Runtime), in der die Programme ausgeführt werden, sowie einer Sammlung von Klassenbibliotheken, Programmierschnittstellen und Dienstprogrammen (Services). .NET Framework ist auf verschiedenen Plattformen verfügbar und unterstützt die Verwendung einer Vielzahl von Programmiersprachen. .NET-Programme werden zum Kompilierungszeitpunkt zunächst in eine Zwischensprache (Common Intermediate Language) übersetzt. Werden die so entstandenen Kompilate ausgeführt, wird der Code von der .NET-Laufzeitumgebung in die eigentliche Maschinensprache des Zielsystems übersetzt. Diese Übersetzung geschieht mit Hilfe eines Just-In-Time-Compilers. Für die Entwicklung von .NET-Programmen vertreibt Microsoft die Entwicklungsumgebung Visual Studio..NET Framework ist ein monolithisches Framework, wohingegen das aus ihm hervorgehende, neu erscheinende Framework .NET Core modular aufgebaut ist. Die Entwicklung der .NET-Plattform wurde als notwendig angesehen, um die in die Jahre gekommenen Konzepte von Windows durch neue zu ersetzen, war jedoch auch das Ergebnis des Rechtsstreits von Microsoft mit Sun über Java. Microsoft hatte das von Sun entwickelte Java-System adaptiert und es nach eigenen Bedürfnissen erweitert, was die Plattformunabhängigkeit von Java-Applikationen beeinträchtigte. Als Sun das unter anderem durch Gerichtsverfügung unterband, änderte Microsoft seine Strategie. Zudem war es Microsoft bis zur Entwicklung von .NET nicht gelungen, im lukrativen Markt für mobile Kleingeräte Fuß zu fassen. → Hauptartikel: Visual J++ Zudem hatten sich mit der Zeit verschiedene, zueinander inkompatible Softwaresysteme für Windows entwickelt. Die drei für Windows meistverwendeten Programmiersprachen C++, Visual Basic sowie die Microsoft-Implementierung einer Java-Syntax, J++, waren zueinander nicht kompatibel und die Zusammenarbeit über verschiedene Brücken erwies sich als sehr kompliziert. Zeichenketten und ANSI/Unicode[Bearbeiten | Quelltext bearbeiten] Die Datentypen für Zeichenketten (engl. „strings“) waren nicht binärkompatibel zueinander. Wollte man solche über zwei Softwaresysteme hinweg schreiben, so musste man Laufzeiteinbußen wegen Konvertierungsfunktionen hinnehmen. Verschärfend kam die Koexistenz von ANSI und Unicode hinzu. Viele Programme unterstützten kein Unicode oder wurden dafür noch nicht ausgerüstet. .NET verwendet einheitlich Unicode für Zeichenketten. Speicherverwaltung[Bearbeiten | Quelltext bearbeiten] Jede Entwicklungsplattform besaß ein eigenes System für die Verwaltung des Speichers. J++ und Visual Basic besaßen eine automatische Speicherverwaltung, das heißt, der Programmierer überließ (weitgehend) dem System die Verwaltung des Speichers. Visual C++ hingegen besaß keine Speicherverwaltung, der Programmierer musste sich selbst darum kümmern. Offenlegung des Quellcodes[Bearbeiten | Quelltext bearbeiten] Am 17. Januar 2008 veröffentlichte Microsoft den Quelltext des Frameworks unter der restriktiven Microsoft Reference License. Zu diesem Schritt entschloss sich Microsoft bereits im Oktober 2007, als Sun Microsystems sein Produkt Java unter der GNU GPL mit eigenen Zusatzklauseln zur Verfügung stellte. Ende 2013 gründeten Microsoft, Xamarin (Mono) und andere die .NET Foundation als neuer Rechteinhaber und Lizenzgeber des .NET Frameworks. Seitdem sind fast alle Rechte an der .NET Klassenbibliothek von Microsoft an die .NET Foundation übertragen worden. Unter dem Dach der .NET Foundation werden derzeit 30 Projekte verwaltet.[4] Siehe auch: .NET Core Verhältnis zu Mono[Bearbeiten | Quelltext bearbeiten] Teile der Open-Source-Community[5] sehen in der Offenlegung unter der restriktiven Lizenz eine Gefahr für das Projekt Mono, welches .NET-Anwendungen unter Linux teilweise verfügbar macht. Microsoft hatte 2007 noch behauptet, das Projekt enthalte Quellcode aus dem .NET-Framework. Da das Framework und Mono gleichermaßen .NET implementieren, befürchtet man nun zwangsweise starke Ähnlichkeiten im Quellcode. Das umstrittene Patentabkommen zwischen Microsoft und Novell[6] (dem ehemaligen Projektträger von Mono) schützt derzeit sowohl die Community unter Novell als auch Microsoft vor gegenseitigen Patentansprüchen. Mit der Gründung der .NET Foundation und der Übertragung der Rechte und Quellcodes an die Foundation arbeitet Microsoft mit Xamarin (Mono) aktiv zusammen um .NET auf unterschiedlichen Plattformen bereitzustellen. Durch die Offenlegung der Quellcodes unter der MIT-Lizenz bzw. Apache 2.0 Lizenz ist der Quellcode des .NET Frameworks nahezu beliebig – sprich auch in Closed-Source-Projekten – verwendbar. Lizenz- und Patentrechtliche Auseinandersetzungen sind somit kaum noch möglich und somit auch nicht mehr zu befürchten.[7] Das Zend Framework ist ein komponenten-orientiertes Framework für PHP. Klassen und Pakete können unabhängig voneinander und auch in Kombination mit den Lösungen anderer Hersteller genutzt werden. Hersteller ist das Unternehmen Zend Technologies. Die Funktionalität ist komplett objektorientiert realisiert. Version 2.0 wurde in großen Teilen neu entwickelt und nutzt flexiblere Paradigmen, zum Beispiel Dependency Injection. Bedingt durch diese Anpassungen setzt das Framework mindestens PHP 5.3 voraus.[2] Die aktuelle Vorgängerversion 1.12.7 von Zend Framework 1 setzt mindestens PHP 5.2.11 voraus.[3] Version 1.12.0 war die letzte Version von Zend Framework 1, die neue Komponenten erhielt. Eine erste Version des Zend Framework 2.0 erschien am 6. August 2010, es folgte ein Update am 3. November 2010. Die erste offizielle Release von Zend Framework 2.0 wurde am 5. September 2012 veröffentlicht.[4] Das Zend Framework 3 war zunächst für das dritte Quartal 2015 angekündigt, und ist im Juli 2016 erschienen.[5] Symfony wird seit 2005 unter der Führung von Fabien Potencier entwickelt. Es entstand parallel zur steigenden Popularität von Ruby on Rails und dem Wunsch nach einem ähnlichen MVC-Framework auf PHP-Basis. Symfony versucht die Konfiguration auf ein Minimum zu beschränken. Wenn keine Konfiguration dafür angegeben ist, erfolgt die Zuordnung von z. B. Models zu Datenbanktabellen über die Namensgleichheit in Singular und Plural (Konvention vor Konfiguration). Durch die Konsolenanwendung können einfache Webseiten mittels Rapid Application Development entwickelt werden. Version 2 ist im Juli 2011 erschienen. Sie stellt eine Neuimplementierung dar, die sich gänzlich von Version 1 unterscheidet. Eine einfache Möglichkeit, Anwendungen von Version 1 nach Version 2 zu migrieren, existiert nicht. Das Framework besteht nun aus Modulen, sogenannten Bundles, welche voneinander vollkommen unabhängig lauffähig sind, aber nahtlos in den Framework-Prozess integriert werden können.[2] Durch die Verwendung eines Dependency Injection Containers ist die gesamte Anwendung modular aufgebaut. Dies bewirkt eine einfache Testbarkeit und Erweiterbarkeit. Außerdem werden Namespaces unterstützt. Somit erfordert Symfony 2 PHP 5.3 oder höher. Bundles[Bearbeiten | Quelltext bearbeiten] Bundles sind voneinander gelöste Einheiten einer Webapplikation. Ein fiktives Gästebuch-Bundle enthält demnach alle Daten, die zur vollständigen Lauffähigkeit des Gästebuchs dienen. Dazu gehört nicht nur die erforderliche Anwendungslogik, sondern auch Datenbankabfragen und Ressourcen (Grafiken, Scripts etc.). Entwickler können entwickelte Bundles veröffentlichen, so dass andere Nutzer von Symfony 2 diese Applikationseinheiten ohne direkte Codeänderungen integrieren können. Das Entwurfsmuster Model-View-Controller wird von Symfony wie folgt umgesetzt: Modell[Bearbeiten | Quelltext bearbeiten] Zur Speicherung der Objekte kommen zwei Plugins zur Auswahl (Doctrine und Propel) mit. Dabei handelt es sich um Bibliotheken zur objektrelationalen Abbildung, die PHP-Objekte in einer relationalen Datenbank speichern. Die Beziehungen zwischen verschiedenen Modellen werden über sogenannte Associations festgelegt. Ab der Version 1.1 von Symfony wurde das bisherige Standardframework Propel in ein Plugin ausgelagert und ist nicht mehr fester Bestandteil des Frameworks. View[Bearbeiten | Quelltext bearbeiten] Unter einem View versteht man die Präsentationsschicht der Applikation. In der View-Schicht werden mit Hilfe des Controllers Inhalte bereitgestellt, die oftmals Templates für die Ausgabe einbeziehen. Dabei kann beliebiger PHP-Code in einem Template eingebettet werden. Symfony bringt sogenannte Helper mit, welche bestimmte Ausgaben in Templates erstellen oder formatieren können, etwa Links oder Formulare. Für sich wiederholende Ausgaben kann man auf Partials zurückgreifen, welche innerhalb der Templates iterierbar sind. Controller[Bearbeiten | Quelltext bearbeiten] Der Controller formuliert die Geschäftslogik und bietet Schnittstellen, um mit dieser zu interagieren. Diese Schnittstellen werden wie in vielen anderen Frameworks auch Actions genannt. Es können auch Parameter für die Methode übergeben werden. Die Struktur der URLs kann mit Hilfe von Routen flexibel angepasst werden. Ruby on Rails, kurz Rails, früher auch oft kurz RoR, ist ein ursprünglich von David Heinemeier Hansson in der Programmiersprache Ruby geschriebenes und quelloffenes Web Application Framework. Es wurde im Juli 2004 zum ersten Mal der Öffentlichkeit vorgestellt. Rails ist geprägt von den Prinzipien „don’t repeat yourself“ (DRY) und „Konvention vor Konfiguration“: Das heißt, statt einer variablen Konfiguration sind Konventionen für die Namensgebung von Objekten einzuhalten, aus denen sich deren Zusammenspiel automatisch ergibt. Diese Funktionen ermöglichen eine rasche Umsetzung von Anforderungen.Before we can investigate in detail all the components of Zend Framework, we must get our bearings, and this is best done by building a simple website that uses the MVC) components. For a standard PHP application, the code to display the text “Hello World” constitutes just one line in one file: <?php echo 'Hello World'; In this chapter, we will build a Hello World application using Zend Framework. We will also consider how to organize the website’s files on disk to make sure we can find what we are looking for, and we will look at Zend Framework files required to create an application that uses the MVC design pattern. NOTE Zend Framework requires many files to create the foundation from which a full website can be created. This means the code for our Hello This chapter covers ■ An introduction to the Model-View-Controller design pattern ■ Zend Framework’s controller components ■ The Zend_View component ■ Databases as models The Model-View-Controller design pattern 19 World application may appear unnecessarily verbose as we set the stage for the full-blown website that will follow in later chapters. This chapter will walk through all the files required to build Hello World. We will also discuss Zend Framework’s MVC design and the core components it provides for building the controller, view, and model in our application. Let’s dive right in and look at what the Model-View-Controller design pattern is all about. 2.1 The Model-View-Controller design pattern In order to enable you to make sense of a Zend Framework application, we need to cover a little bit of theory. Zend Framework provides an implementation of the ModelView-Controller software design pattern coupled to another design pattern known as Front Controller. A software design pattern is a standard general solution to a common problem. This means that while implementations will vary, the concepts used to solve problems using a given pattern will be the same. The Front Controller pattern is a mechanism that centralizes the entrance point to your application. The front controller handler (usually index.php) accepts all server requests and runs the correct action function within the action command. This process is known as routing and dispatching. Zend Framework implements the front controller pattern over a number of subcomponents. The important ones are the router and the dispatcher. The router determines which action needs to be run, then the dispatcher runs the requested action and any other action that may be required. The MVC pattern describes a way to separate out the key parts of an application into three main sections: the model, view, and controller. These are the sections that you write in order to create an application. Figure 2.1 shows how the Front Controller’s router and dispatcher are attached to the model, controller, and view to produce a response to a web browser’s request. Within the Zend Framework MVC implementation, we have five main areas of concern. The router and dispatcher work together to determine which controller is to be run based on the contents of the URL. The controller works with the model and the view to create the final web page, which is sent back to the browser. Let’s look at the model, view, and controller in more detail. Dispatcher Model View Controller Request from browser Response to browser Router Figure 2.1 Zend Framework’s Front Controller and MVC components work together to serve a web page. The router and dispatcher find the correct controller, which builds the page in conjunction with the model and view. 20 CHAPTER 2 Hello Zend Framework! 2.1.1 The model The model part of the MVC pattern is all the business-logic code that works behind the scenes in the application. This is the code that decides how to apply the shipping costs to an e-commerce order, or knows that a user has a first name and a surname. Retrieving data from and storing it in a database falls within the model layer. In terms of the code, Zend Framework provides the Zend_Db_Table and Zend_Service components. Zend_Db_Table provides table-level access to databases and allows for easily manipulating the data used by the application. Zend_Service provides a suite of components for easily accessing both public and private web services and integrating them into your application. 2.1.2 The view The view is the display logic of the application. For a web application, this is usually the HTML code that makes up the web pages, but it can include, say, XML that is used for an RSS feed. Also, if the website allows for exporting in CSV (comma-separated values) format, the generation of the CSV data would be part of the view. The view files are known as templates or scripts, because they usually have some code that allows for the display of data created by the model. It is also usual to move the more complex template-related code into functions known as view helpers, which improve the reusability of the view code. By default, Zend Framework’s view class (Zend_View) uses PHP within the script files, but another template engine, such as Smarty or PHPTAL, may be substituted. 2.1.3 The controller The controller is the rest of the code that makes up the application. For web applications, the controller code determines what should be done in response to a web request. As we have discussed, Zend Framework’s controller system is based on the Front Controller design pattern. which uses a handler (Zend_Controller_Front) to dispatch action commands (Zend_Controller_Action) that work in tandem. The dispatcher component allows for multiple action commands to be processed within a single request, which provides for flexible application architectures. The action class is responsible for a group of related action functions, which perform the real work required by the request. Within Zend Framework’s front controller, it is possible to have a single request result in the dispatch of multiple actions. Now that we understand a little about Zend Framework’s MVC implementation, we can start looking at the nitty gritty of how the files fit together. As we mentioned earlier, there are a lot of files in a Zend Framework application, so we need to organize them into directories. The anatomy of a Zend Framework application 21 2.2 The anatomy of a Zend Framework application A typical Zend Framework application has many directories. This helps to ensure that the different parts of the application are separated. The top-level directory structure is shown in figure 2.2. There are four top-level directories within an application’s folder: ■ application ■ library ■ public ■ tests The application, library, and public directories are used when processing a request from the user. The tests directory is used to store unit test files that enable you to ensure that your code operates correctly. 2.2.1 The application directory The application directory contains all the code required to run the application, and it is not directly accessed by the web server. In order to emphasize the separation between business, display, and control logic, there are three separate directories within the application directory to contain the model, view, and controller files. Other directories may be created as required, such as for configuration files. 2.2.2 The library directory All applications use library code because everyone reuses previously written code! In a Zend Framework application, the framework itself is obviously stored in the library directory. However, other libraries, such as a custom superset of the framework, a database ORM library such as Propel, or a template engine such as Smarty, may also be used. Libraries can be stored anywhere that the application can find them—either in a global directory or a local one. A global include directory is one that is accessible to all PHP applications on the server, such as /usr/php_include (or c:\code\php_include for Windows) and is set using the include_path setting within the php.ini configuration file. Alternatively, each application can store its libraries locally within the application’s directory. In a Zend Framework application, we use a directory called library, though it is common to see this directory called lib, include, or inc. 2.2.3 The tests directory The tests directory is used to store all unit tests. Unit tests are used to help ensure that the code continues to work as it grows and changes throughout the lifetime of the application. As the application is developed, existing code often needs to be changed Figure 2.2 A typical Zend Framework application’s directory layout groups the files by their role in the application, so it is easy to find the file you are looking for. 22 CHAPTER 2 Hello Zend Framework! (known as refactoring) to allow for the addition of new functionality or as a result of other code being added to the application. While test code is rarely considered important within the PHP world, you will thank yourself over and over again if you have unit tests for your code. 2.2.4 The public directory To improve the security of a web application, the web server should only have direct access to the files that it needs to serve. As Zend Framework uses the Front Controller pattern, all web requests are channeled though a single file, usually called index.php. This file is the only PHP file that needs to be accessible by the web server, so it is stored in the public directory. Other common files that are accessed directly are image, Cascading Style Sheets (CSS), and JavaScript files, so each has its own subdirectory within the public directory. Now that we looked at the directory system used by a Zend Framework web application, we can proceed to add the files required to create a very simple application that displays some text on the page. 2.3 Hello World: file by file To create a simple Hello World application, we need to create four files within our directory structure: a bootstrap file, an Apache control file (.htaccess), a controller file, and a view template. A copy of Zend Framework, itself, also needs to be added to the library directory. The final program will display the page shown in figure 2.3. The result is a web page with very little text, and the code required to do this seemingly simple task appears to be long and daunting. As an application’s complexity grows, the additional code required to provide the new functionality is relatively small, showing benefits of the MVC system that are not apparent in a small example like this. Let’s start with the bootstrap file, which is used to start the application. 2.3.1 Bootstrapping Bootstrapping is the term used to describe the code that initializes the application and configures it. With the Front Controller pattern, the bootstrap file is the only file Figure 2.3 The Hello World application produces the words “Hello World!” in our browser. A minimal Zend Framework application requires .htaccess, bootstrap, controller, and view files working together to produce this. Hello World: file by file 23 needed in the public directory, and so is usually called index.php. Because this file is used for all page requests, it is also used for setting up the application’s environment, setting up Zend Framework’s controller system, then running the application itself, as shown in listing 2.1. <?php error_reporting(E_ALL|E_STRICT); ini_set('display_errors', true); date_default_timezone_set('Europe/London'); $rootDir = dirname(dirname(__FILE__)); set_include_path($rootDir . '/library' . PATH_SEPARATOR . get_include_path()); require_once 'Zend/Loader.php'; Zend_Loader::loadClass('Zend_Debug'); Zend_Loader::loadClass('Zend_Controller_Front'); // set up controller $frontController = Zend_Controller_Front::getInstance(); $frontController->setControllerDirectory('../application/controllers'); // run! $frontController->dispatch(); Let’s look at this file in more detail. Most of the work done in the bootstrap file is initialization of one form or another. Initially, the environment is set up correctly b to ensure that all errors or notices are displayed. (Don’t do this on your production server!) PHP 5.1 introduced new time and date functionality that needs to know where in the world we are. There are several ways to set this, but the easiest method is to call date_default_timezone_set(). Zend Framework assumes that the library directory is available on the php_include path. The fastest way of setting this for a global library is to alter the include_path setting directly in php.ini. A more portable method, especially if you use multiple versions of the framework on one server, is to set the include path within the bootstrap file as we do here c. Zend Framework applications do not depend on any particular file, but it is useful to have a couple of helper classes loaded early. Zend_Loader::loadClass() is used to “include” the correct file for the supplied class name. The function converts the underscores in the class’s name to directory separators and then, after error checking, includes the file. As a result the code lines Zend_Loader::loadClass('Zend_Controller_Front'); and include_once 'Zend/Controller/Front.php'; have the same end result. Zend_Debug::dump() is used to output debugging information about a variable by providing a formatted var_dump() output. Listing 2.1 The bootstrap file, index.php, initializes and runs the application Sets up environment b Sets the path c Retrieves Zend_Controller_Front instance d 24 CHAPTER 2 Hello Zend Framework! The final section of the bootstrap file sets up the front controller and runs it. The front controller class, Zend_Controller_Front, implements the Singleton design pattern, so we use the getInstance() static function to retrieve it d. A Singleton design is appropriate for a front controller, as it ensures that there can only be one instance of the object processing the request. The front controller, by default, captures all exceptions thrown and stores them in the response object that it creates. This response object holds all information about the response to the requested URL, and for HTML applications this includes the HTTP headers, the page content, and any exceptions that were thrown. The front controller automatically sends the headers and displays the page content when it finishes processing the request. In the case of exceptions, Zend Framework’s ErrorHandler plugin will redirect the request to an action method called error in a controller called ErrorController. This ensures that we can control the error displayed to the user and possibly even offer additional help. We will implement this feature later. To run the application, we call the front controller’s dispatch() method. This function will automatically create a request and response object to encapsulate the input and output of the application. It will then create a router to work out which controller and action the user has asked for. A dispatcher object is then created to load the correct controller class and call the action member function that does the “real” work. Finally, as we noted earlier, the front controller outputs the data within the response object and a web page is displayed to the user. 2.3.2 Apache .htaccess To ensure that all web requests that are not for images, scripts, or stylesheets are directed to the bootstrap file, Apache’s mod_rewrite module is used. This can be configured directly in Apache’s httpd.conf file or in a local Apache configuration file named .htaccess that is placed in the public directory. Listing 2.2 shows the .htaccess file required for Zend Framework. # Rewrite rules for Zend Framework RewriteEngine on RewriteCond %{REQUEST_FILENAME} !-f RewriteRule .* index.php Fortunately, this is not the most complicated set of Apache mod_rewrite rules, so they can be easily explained. The RewriteCond statement and the RewriteRule command between them instruct Apache to route all requests to index.php unless the request maps exactly to a file that exists within the public directory tree. This will allow us to serve any static resources placed in the public directory, such as JavaScript, CSS, and image files, while directing any other requests to our bootstrap file, where the front controller can work out what to display to the user. Listing 2.2 The bootstrap file: public/.htaccess Continues only if requested URL is not a file on disk Redirects request to index.php on disk Hello World: file by file 25 2.3.3 Index controller The Front Controller pattern maps the URL requested by the user to a particular member function (the action) within a specific controller class. This process is known as routing and dispatching. The controller classes have a strict naming convention requirement in order for the dispatcher to find the correct function. The router expects to call a function named {actionName}Action() within the {ControllerName}Controller class. This class must be within a file called {ControllerName}Controller.php. If either the controller or the action name is not provided as part of the request, then the default, “index” is used. A call to http://zfia.example.com/ will result in the “index” action of the index controller running. Similarly, a call to http://zfia.example.com/test will result in the index action of the test controller running. As you will see later, this mapping is very flexible, but the default covers most scenarios. Within Zend Framework’s front controller implementation, the dispatcher expects to find a file called IndexController.php within the application/controllers directory. This file must contain a class called IndexController and, as a minimum, this class must contain a function called indexAction(). Listing 2.3 shows the IndexController.php required for our Hello World application. <?php class IndexController extends Zend_Controller_Action { public function indexAction() { $ this->view->assign('title', 'Hello World!'); } } As you can see, IndexController is a child class of Zend_Controller_Action, which contains the request and response objects for access to the date received by the application and to set the data that will be sent back to the user, along with a few useful helper functions to control the program flow. For Hello World, our indexAction() function needs to assign a variable to the view property, which is provided for us by an action helper called Zend_Controller_Action_ViewRenderer (known as ViewRenderer). NOTE An action helper is a class that plugs into the controller to provide services specific to actions. They expand a controller’s functionality without using inheritance and so can be reused across multiple controllers and projects. The ViewRenderer action helper performs two useful functions for us. First, before our action is called, it creates a Zend_View object and sets it to the action’s $view property, allowing us to assign data to the view within the action. Second, after our action Listing 2.3 The index controller: application/controllers/IndexController.php 26 CHAPTER 2 Hello Zend Framework! finishes, it automatically renders the correct view template into the response object after the controller action has completed. This ensures that our controller’s action functions can concentrate on the real work and not on the framework’s plumbing. What is the “correct view template” though? The ViewRenderer looks in the view/ scripts directory for a template file named after the action, with a .phtml extension, within a folder named after the controller. This means that for the index action within the index controller, it will look for the view template file view/scripts/index/ index.phtml. As we noted previously, the response’s body is automatically printed by the front controller, so anything we assign to the body will be displayed in the browser. We do not need to echo ourselves. Zend_View is the view component of the MVC troika and is a fairly simple PHP-based template system. As we have seen, the assign() function is used to pass variables from the main code body to the template, which can then be used within the view template file. 2.3.4 View template The view script for our application, index.phtml, is stored within the views/scripts/ index subdirectory. A useful convention that ViewRenderer follows is to name all view files with an extension of .phtml as a visual indication that they are for display only. Of course, this is easily changed by setting the $_viewSuffix property of ViewRenderer. Even though this is a simple application, we have a separate directory for each controller’s view templates, because this will make it much easier to manage as the application grows. Listing 2.4 shows the view template. <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"> <html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"> <head> <meta http-equiv="Content-Type" content="text/html;charset=utf-8" /> <title> <?php echo $this->escape($this->title);?> </title> </head> <body> <h1><?php echo $this->escape($this->title);?></h1> </body> </html> As Zend_View is a PHP-based template engine, we use PHP within the file to display data from the model and controller. The template file, index.phtml in this case, is executed within a member function of Zend_View, so $this is available within the template file, and it is the gateway to Zend_View’s functionality. All variables assigned to the view from within the controller are available directly as properties of $this, as can Listing 2.4 The view template: views/scripts/index/index.phtml Converts special characters to HTML entity representations How MVC applies to Zend Framework 27 be seen by the use of $this->title within index.phtml. Also, a number of helper functions are provided via templates, which makes them easier to write. The most commonly used helper function is escape(). This function ensures that the output is HTML-safe and helps to secure your site from cross-site scripting (XSS) attacks. All variables that are not expected to contain displayable HTML should be displayed via the escape() function. Zend_View is designed to encourage the creation of new helper functions. For maximum flexibility, the convention is that view helper functions return their data, and then the template file echoes it to the browser. With these four files in place, we have created a minimal Zend Framework application with all the pieces in place for a full-scale website. You should now have a fundamental understanding of how the pieces fit together. Next, we will look at what is happening within Zend Framework’s code, which provides the MVC foundation our code is built upon. 2.4 How MVC applies to Zend Framework While there appear to be many different ways of routing web requests to code within web applications, they can all be grouped into two camps: page controllers and front controllers. A page controller uses separate files for every page (or group of pages) that make up the website, and this is traditionally how most PHP websites have been built. This means that the control of the application is decentralized across lots of different files, which can result in repeated code, or, worse, repeated and slightly altered code leading to issues such as lost sessions when one of the files doesn’t do a session_start(). A front controller, on the other hand, centralizes all web requests into a single file, typically called index.php, which lives in the root directory of the website. There are numerous advantages to this system; the most obvious are that there is less duplicated code and it is easier to separate the URLs that a website has from the actual code that is used to generate the pages. Usually, the pages are displayed using two additional GET parameters passed to the index.php file to create URLs such as this to display a list page: index.php?controller=news&action=list As we explained in chapter 1, Zend Framework uses a Front Controller pattern coupled to the Model-View-Controller pattern to respond to a request from the browser. Each pattern is made up of multiple classes, as shown in figure 2.4. One important goal of most modern web applications is that the URLs should look “good,” so that they are more memorable for users and it is easier for search engines like Yahoo! or Google to index the pages of the website. An example of a friendly URL would be http://www.example.com/news/list, because this URL does not include any ? or & characters, and the user can infer what will be displayed (a list of news items). Zend Framework’s front controller uses a subcomponent known as a router that supports friendly URLs by default. 28 CHAPTER 2 Hello Zend Framework! 2.4.1 Zend Framework’s controller Zend Framework’s front controller code is spread over a number of classes that work together to provide a very flexible solution to the problem of routing a web request to the correct place. Zend_Controller_Front is the foundation, and it processes all requests received by the application and delegates that actual work to action controllers. THE REQUEST The request is encapsulated within an instance of Zend_Controller_Request_Http, which provides access to the entire HTTP request environment. What is a request environment? It is all the variables received by the application, along with relevant controller parameters, such as the controller and action router variables. The HTTP request environment contains all the superglobals ($_GET, $_POST, $_COOKIE, $_SERVER, and $_ENV) along with the base path to the application. The router also places the module, controller, and action names into the request object once it has worked them out. Zend_Controller_Request_Http provides the getParam() function to allow the application to collect the request variables, and so the rest of the application is protected from a change in environment. For example, a command-line request environment wouldn’t contain the HTTP-specific items, but would include the command-line arguments passed to the script. Thus, this code will work unchanged when run as a web request or as a command-line script: $items = $request->getParam('items'); In general, the request object should be treated as read-only to the application, because the values set by the user shouldn’t be changed. Having said that, Zend_Controller_Request_Http also contains parameters that can be set in the startup phase of the application and then retrieved by the action functions as Bootstrap file: index.php (Initialization) Request from browser Zend_Controller_Request_Http Zend_Controller_Response_Http Response to browser READS FROM Zend_Controller_Front (Front controller) Zend_Controller_Router_Rewrite (Chooses action to run) Zend_Controller_Dispatcher_Standard (Calls action) Concrete instance(s) of Zend_Db_Table (Model) Creates Zend_Db_Rows and Zend_Db_Rowsets Zend_View (View) Uses various Zend_View_Helper classes RENDERS TO Concrete instance(s) of Zend_Controller_Action Uses various Zend_Action_Helper classes Figure 2.4 The interaction of the various Zend Framework classes in an MVC application How MVC applies to Zend Framework 29 required. This can be used to pass additional information from the front controller to the action methods, if required. After the front controller has set up the request object from the superglobals, it then starts the router. ROUTING The router determines which code to run, based on the parameters within the request. This is done by a class that implements Zend_Controller_Router_- Interface. The framework also supplies Zend_Controller_Router_Rewrite, which handles most routing requirements. Routing works by taking the part of the URI after the base URL (known as the URI endpoint) and decomposing it into separate parameters. For a standard URL, such as http://example.com/index.php?controller=news&action=list the decomposition is done by simply reading the $_GET array and looking for the controller and action elements. It is expected that most applications built using Zend Framework (like other modern applications) will use pretty URLs of the form http://example.com/news/list. In this case, the router will use the relevant variables in the $_SERVER super-global array to determine which controller and action have been requested. After the controller and action have been determined, the correct controller action method is run, along with any other controller action methods that may be specified by the application. This is known as dispatching and is performed by the front controller after the router has completed. DISPATCHING Dispatching is the process of calling the correct method in the correct class. Like all components in Zend Framework, the standard dispatcher provides enough functionality for nearly every situation, but if you need something special, it is easy to write your own dispatcher and fit it into the front controller. The key things that the dispatcher controls are formatting the controller class name, formatting the action method name, and calling the action method itself. Zend_Controller_Dispatcher_Standard is where the rules concerning case are enforced, such that the name format of the controller is always TitleCase and only contains alphanumeric characters (and the underscore character). The dispatcher’s dispatch() method is responsible for loading the controller class file, instantiating the class and then calling the action method within that class. If you decided that you wanted to reorganize the structure so that each action lived in its own class within a directory named after the controller, you would need to supply your own dispatcher. At this point, the dispatcher passes control to the controller class’s action function. Action controllers are separate classes that are stored within the controllers subdirectory of the application directory. THE ACTION Zend_Controller_Action is an abstract class that all action controllers are derived from. The dispatcher enforces that your action controllers derive from this class to ensure that it can expect certain methods to be available. The action contains an 30 CHAPTER 2 Hello Zend Framework! instance of the request, for reading parameters from, and an instance of the response, for writing to. The rest of the class concentrates on ensuring that writing actions and managing changes from one action to another are easy to do. There are accessor functions to get and set parameters, and redirection functions to redirect to another action or another URL entirely. Assuming that the standard dispatcher is used, the action functions are all named after the action’s name with the word “Action” appended. You can therefore expect a controller action class to contain functions such as indexAction(), viewAction(), editAction(), deleteAction(), and so on. Each of these is a discrete method run in response to a specific URL. There are also a number of tasks that you will want to do regardless of which action is run. Zend_Controller_Action provides two levels of functionality to accommodate this requirement: init() and the preDispatch() and postDispatch() pair. The init() method is called whenever the controller class is constructed. This makes it very similar to the standard constructor, except that it does not take any parameters and does not require the parent method to be called. preDispatch() and postDispatch() are a complementary pair of methods that are run before and after each action method is called. For an application where only one action is run in response to a request, there is no difference between init() and preDispatch(), as each is only called once. If the first action method uses the _forward() function to pass control to another action method, preDispatch() will be run again, but init() will not be. To illustrate this point, we could use init() to ensure that only administrators are allowed access to any action method in the controller, and we could use preDispatch() to set the correct view script file that would be used by the action. After the action method has finished, control is returned to the dispatcher, which will then run other actions as required. When all actions are complete, the created output is returned to the user via the response object. THE RESPONSE The final link in the front controller chain is the response. For a web application, Zend_Controller_Response_Http is provided, but if you are writing a command-line application, Zend_Controller_Response_Cli would be more appropriate. The response object is very simple and is essentially a bucket to hold all the output until the controller processing is finished. This can be very useful when using front controller plug-ins, as they could alter the output of the action before it is sent back to the client. Zend_Controller_Response_Http contains three types of information: header, body, and exception. In the context of the response, the headers are HTTP headers, not HTML headers. Each header is an array containing a name, along with its value, and it is possible to have two headers with the same name but different values within the response’s container. The response also holds the HTTP response code (as defined in RFC 2616), which is sent to the client at the end of processing. By default, this is set to 200, which means OK. Other common response codes are 404 (Not How MVC applies to Zend Framework 31 Found) and 302 (Found), which is used when redirecting to a new URL. As we will see later, the use of status code 304 (Not Modified) can be very useful when responding to requests for RSS feeds, as it can save considerable bandwidth. The body container within the response is used to contain everything else that needs to be sent back to the client. For a web application, this means everything you see when you view the source on a web page. If you are sending a file to a client, the body would contain the contents of the file. For example, to send a PDF file to the client, the following code would be used: $filename = 'example.pdf'; $response = new Zend_Controller_Response_Http(); // set the HTTP headers $response->setHeader('Content-Type', 'application/pdf'); $response->setHeader('Content-Disposition', 'attachment; filename="'.$filename.'"'); $response->setHeader('Accept-Ranges', 'bytes'); $response->setHeader('Content-Length', filesize($filename)); // load the file to send into the body $response->setBody(file_get_contents($filename)); echo $response; The final container within the response object houses the exceptions. This is an array that can be added to by calling $response->setException(), and it is used by Zend_Controller_Front to ensure that errors within the code are not sent to the client, possibly exposing private information that could be used to compromise your application. Of course, during development you will want to see the errors, so the response has a setting, renderExceptions, that you can set to true so that the exception text is displayed. In order to extend the front controller, a plug-in system has been developed. FRONT CONTROLLER PLUG-INS The front controller’s architecture contains a plug-in system to allow user code to be executed automatically at certain points in the routing and dispatching process. Plugins allow you to change the functionality of the front controller’s routing and dispatching system in a modular fashion, and they are designed to be easily transferred from one project to another. All plug-ins are derived from Zend_Controller_Plugin_Abstract, and there are six event methods that can be overridden: ■ routeStartup() is called just before the router is executed. ■ routeShutdown() is called after the router has finished. ■ dispatchLoopStartup() is called just before the dispatcher starts executing. ■ preDispatch() is called before each action is executed. ■ postDispatch() is called after each action is executed. ■ dispatchLoopShutdown() is called after all actions have been dispatched. 32 CHAPTER 2 Hello Zend Framework! As you can see, there are three pairs of hooks into the process at three different points, which allow for increasingly finer control of the process. One problem with the current router is that if you specify a controller that does not exist, an exception is thrown. A front controller plug-in is a good way to inject a solution into the routing process and redirect the application to a more useful page. Zend Framework supplies the ErrorHandler plug-in for this purpose, and its use is very well explained in the manual. Now that we have looked in detail at the controller part of MVC, it’s time to look at the view part, as provided for by the Zend_View component. 2.4.2 Understanding Zend_View The Zend_View class keeps the view portion of an MVC application separate from the rest of the application. It is a PHP template library, which means that the code in the view scripts is in PHP rather than another pseudo-language, like Smarty for instance. However, it is easy to extend Zend_View to support any other template system. Let’s start our exploration of Zend_View by looking at how to assign data to the view. Zend_View’s assign() method is used to display data from the model. Simple variables can be assigned to a view variable like this: $view->assign('title', 'Hello World!'); This assigns the string “Hello World!” to the title variable. Alternatively, you can assign multiple variables simultaneously using an associative array: $music = array('title'=>'Abbey Road', 'artist'=>'The Beatles'); $music = array('title'=>'The Wall', 'artist'=>'Pink Floyd'); $view->assign($music); As we are using PHP5, we can also take advantage of the __set() magic method to write $view->title = 'Hello World!'; which will also assign the string to the title variable. Whichever of the preceding two methods you use for assigning data, the data from the model or controller is now available for use in the view script, which is the file that contains output HTML and code. THE VIEW SCRIPT A view script is just like any other regular PHP file, except that its scope is contained within an instance of a Zend_View object. This means that it has access to all the methods and data of Zend_View as if it were a function within the class. The data assigned to the view is public property of the view class and so is directly accessible. In addition, the view provides helper functions to make writing view scripts easier. A typical view script might look like this: <h1>Glossary</h1> <?php if($this->glossary) :?> <dl> <?php foreach ($this->glossary as $item) : ?> How MVC applies to Zend Framework 33 <dt><?php echo $this->escape($item['term']);?></dt> <dd><?php echo $this->escape($item['description']);?></dd> <?php endforeach; ?> </dl> <?php endif; ?> As you can see, this is a PHP script with an HTML bias, as the PHP commands are always contained within their own <?php and ?> tags. Also, we have used the alternative convention for control loops so that we don’t have braces within separate PHP tags— matching braces can be quite tricky when using lots of separate PHP tags. Note that we do not trust the glossary data that has been assigned to the script. It could have come from anywhere! In the code accompanying this book, the data is created using an array, but it could equally have come from the users of a website. To avoid any XSS vulnerabilities in our website, we use the escape() helper function to ensure the term and description do not have any embedded HTML. To avoid repeating lots of similar PHP code in multiple view scripts, view helper functions are used to provide common functionality. VIEW HELPER FUNCTIONS Zend_View contains a number of helpful methods to make writing your view scripts easier. These methods are known as view helpers and exist in their own classes within the application/views/helpers subdirectory. As we have already seen, the most common view helper is the escape() method, which is built into the Zend_View class itself. Every other helper exists in its own class and is automatically loaded by Zend_View. Let’s create a simple formatting helper for displaying a cash amount. Consider that we need to display a monetary value that may be negative. In the UK, for a value of 10, the display would be £10.00, and for a value of –10, the display would be –£10.00. We would use the helper in our view scripts like this: <p>He gave me <?php echo $this->formatCurrency(10);?>.</p> Which outputs the correctly formatted amount as shown in figure 2.5. All default view helpers use the class prefix Zend_View_Helper and are stored in the applications/views/helpers subdirectory. You can store view helpers elsewhere, in which case you would use your own class prefix. Our formatting helper class is called Zend_View_Helper_FormatCurrency and is stored in the file application/views/helpers/FormatCurrency.php, as shown in listing 2.5. In a break from the usual convention within the framework, this is one of the few cases where the class name is not the same as the file path. Figure 2.5 The FormatCurrency view helper is used to display the correct currency symbol in the correct place. 34 CHAPTER 2 Hello Zend Framework! class Zend_View_Helper_FormatCurrency { public function formatCurrency($value, $symbol='&pound;') { $output = ''; $value = trim($value); if (is_numeric($value)) { if ($value >= 0) { $output = $symbol . number_format($value, 2); } else { $output = '-' . $symbol . number_format(abs($value), 2); } } return $output; } } As you can see, if we don’t know that $value is a number, we do not return it as part of the output b. This helps to ensure that we do not inadvertently introduce an XSS vulnerability. The name of the method within the helper class is the same as the method that is called within the view script: formatCurrency() in our case. Internally, Zend_View has an implementation of the __call() magic function to find our helper class and execute the formatCurrency() method. TIP When you create your view helpers, the names are case-sensitive. The class name is in CamelCase with underscores, but the method name is in camelCase. The class name must start with an uppercase letter, and the method name starts with a lowercase one. View helpers are the key to extracting common code from your view scripts and ensuring that they are easy to maintain, so view helpers should be used whenever possible to simplify the view script files. As view script files contain the output of the application, it is important to always keep security issues in mind when sending data to the web browser. SECURITY CONSIDERATIONS When writing view code, the most important security issue you need to be aware of is XSS. Its vulnerabilities occur when unexpected HTML, CSS, or JavaScript is displayed by your website. Generally, this happens when a website displays data created by a user without checking that it is safe for display. This could happen when the text from a comment form contains HTML and is displayed on a guestbook page “as is.” One of the more famous XSS exploits is the Samy MySpace worm. This exploit used specially crafted JavaScript in the profile that was displayed on a page which lists all the user’s friends. The JavaScript would run automatically whenever anyone else viewed the victim’s friends page, and if that user was logged into MySpace, it made Samy their “friend” too. Thus, whenever anyone looked at your page, they were also made “friends” of Samy’s. This resulted in an exponential increase in friends for Listing 2.5 The FormatCurrency view helper Ignores $value if it is not a number b How MVC applies to Zend Framework 35 Samy—over one million MySpace profiles were infected within 20 hours. Fortunately, the code wasn’t too malicious and didn’t steal each user’s password along the way. The easiest way to prevent XSS vulnerabilities is to encode the characters that have special meaning in HTML. That is, you should change all instances of < to &lt;, & to &amp; and > to &gt; so that the browser treats them as literals rather than HTML. Within Zend Framework, you can use the escape() helper function to do this. Every time that you display a PHP variable within a template file, you should use escape() unless you need it to contain HTML. If it does need to contain HTML, you should write a sanitizing function to allow only HTML code that you trust. We have completed our look at the view, so now let’s look at the model. This is where the guts of the application are, including interaction with databases and files. 2.4.3 The model in MVC We have spent a lot of time in this chapter looking at the controller and the view, as these are the minimum required for a Hello World application. In a real application, though, the model element of the MVC pattern takes on more importance as this is where the business logic of the application resides. In most cases, the model is linked in some way to a database that holds data to be manipulated and displayed by the application. DATABASE ABSTRACTION WITH ZEND_DB Zend_Db is Zend Framework’s database abstraction library, which provides a suite of functions that insulate your code from the underlying database engine. This is most useful when you need to switch your application from using something like SQLite to MySQL or Oracle. Zend_Db uses the Factory design pattern to provide the correct database-specific class, based on the parameters passed into the factory() static method. For example, to create a Zend_Db object for MySQL, you would use code like this: $params = array ('host' => '127.0.0.1', 'username' => 'rob', 'password' => '******', 'dbname' => 'zfia'); $db = Zend_Db::factory('PDO_MYSQL', $params); The Zend_Db abstraction is mostly built upon PHP’s PDO extension, which supports a wide range of databases. There is also support for DB2 and Oracle outside of PDO; they all extend from Zend_Db_Adapter_Abstract, so the interface is essentially the same, regardless of the underlying database. What you do get in Zend_Db that you don’t get in PDO itself? Well, you get lots of helper functions to manipulate the database, and also a profiler to work out why your code is so slow. There are all the standard functions for inserting, updating, and deleting rows, along with fetching rows. The manual is particularly good at describing all these functions, so let’s move on and consider database security. 36 CHAPTER 2 Hello Zend Framework! SECURITY ISSUES WITH DATABASES The most common database security problems are known as SQL injection security breaches. These occur when your user is able to trick your code into running a database query that you didn’t intend to allow. Consider this code: $result = $db->query("SELECT * FROM users WHERE name='" . $_POST['name'] . "'"); This typical code might be used to authorize a user after they have submitted a login form. The coder has ensured that the correct superglobal, $_POST, is used, but hasn’t checked what it contains. Suppose that $_POST['name'] contains this string: ' OR 1 OR name = ' This would result in the following perfectly legal SQL statement: SELECT * from users where name='' OR 1 OR name= '' As you can see, the OR 1 in the SQL statement will result in all the users being returned from the database table. With SQL injection vulnerabilities like this, it is possible for an attacker to retrieve username and password information or to maliciously delete database rows causing your application to stop working. As should be obvious, the way to avoid SQL injection attacks is to ensure that the data you are putting into the SQL statement has been escaped using the correct functionality for your database. For MySQL, you would use the mysql_real_escape_ string() function, and for PostgreSQL, you would use pg_escape_string(). As we are using Zend_Db, we can use the quote() member function to take care of this issue. The quote() method will call the correct underlying database-specific function, and if there isn’t one, it will escape the string using the correct rules for the database involved. Using it is very easy: $value = $db->quote("It's a kind of magic"); An alternative solution is to use parameterized queries, where variables are denoted by placeholders and the values are substituted by the database engine. Zend_Db provides the quoteInto() function for this: $sql = $db->quoteInto('SELECT * FROM table WHERE id = ?', 1); $result = $db->query($sql); Parameterized queries are generally considered best practice, as they result in faster database accesses, especially if you use prepared statements. Within the Zend_Db component, Zend Framework provides a higher-level access to the database using the Zend_Db_Table component, which provides an object-oriented interface to a database table and its associated rows, thereby avoiding the necessity of writing common SQL statements in every model. HIGHER-LEVEL INTERACTION WITH ZEND_DB_TABLE When coding the model of an MVC application, we don’t tend to want to work at the level of database queries if we can help it because we are thinking about the business How MVC applies to Zend Framework 37 logic of the application, rather than the nitty gritty of how to interact with a database. The framework provides Zend_Db_Table, an implementation of the Table Data Gateway pattern that provides a higher-level abstraction for thinking about data from the database. Zend_Db_Table uses Zend_Db behind the scenes and provides a static class function, setDefaultAdapter(), for setting the database adapter to be used for all instances of Zend_Db_Table. This is usually set up in the bootstrap file like this: $db = Zend_Db::factory('PDO_MYSQL', $params); Zend_Db_Table::setDefaultAdapter($db); We don’t use Zend_Db_Table directly. Instead, we create a child class that represents the database table we wish to work with. For the purposes of this discussion, we will assume that we have a database table called news with the columns id, date_created, created_by, title, and body to work with. We now create a class called News: Class News extends Zend_Db_Table { protected $_name = 'news'; } The $_name property is used to specify the name of the table. If it is not provided, Zend_Db_Table will use the name of the class, and it is case sensitive. Zend_Db_Table also expects a primary key called id (which is preferably automatically incremented on an insert). Both these default expectations can be changed by initializing the protected member variables $_name and $_primary respectively. Here’s an example: class LatestNews extends Zend_Db_Table { protected $_name = 'news'; protected $_primary = 'article_id'; } The LatestNews class uses a database table called news that has a primary key called article_id. As Zend_Db_Table implements the Table Data Gateway design pattern, it provides a number of functions for collecting data, including find(), fetchRow(), and fetchAll(). The find() function finds rows by primary key, and the fetch methods find rows using other criteria. The only difference between fetchRow() and fetchAll() is that fetchRow() returns a single row object, whereas fetchAll() returns an object, known as a rowset, that contains a set of rows. Zend_Db_Table also has helper functions for inserting, updating, and deleting rows with the functions insert(), update(), and delete(). While Zend_Db_Table is interesting in its own right, its usefulness becomes apparent when we add business logic to it. This is the point when we enter the realm of the model within MVC. There are lots of things you can do, and we’ll start with overriding insert() and update() for our News model. First of all, let’s assume that our news database table has the following definition (in MySQL): 38 CHAPTER 2 Hello Zend Framework! CREATE TABLE `news` ( `id` INT NOT NULL AUTO_INCREMENT PRIMARY KEY , `date_created` DATETIME NOT NULL , `date_updated` DATETIME NULL , `title` VARCHAR(100) NULL , `body` MEDIUMTEXT NOT NULL ) The first business logic that we will implement in our News class, which is our model, will automatically manage the date_created and date_updated fields when inserting and updating records, as shown in listing 2.6. These are “behind the scenes” details that the rest of the system doesn’t need to worry about, so they are ideal for placing in the model. class News extends Zend_Db_Table { protected $_name = 'news'; public function insert($data) { if (empty($data['date_created'])) { $data['date_created'] = date('Y-m-d H:i:s'); } return parent::insert($data); } public function update($data) { if (empty($data['date_updated'])) { $data['date_updated'] = date('Y-m-d H:i:s'); } return parent::update($data); } } This code is self-explanatory. When inserting, if date_created hasn’t been supplied by the caller, we fill in today’s date and call Zend_Db_Table’s insert() function b. For updating, the story is similar, except we change the date_updated field instead. We can also write our own functions for retrieving data according to the business logic required by the application. Let’s assume that for our website, we want to display the five most recently created news items (within the last three months) displayed on the home page. This could be done using $news->fetchAll() in the home page controller, but it is better to move the logic down into the News model to maintain the correct layering of the application, so that it can be reused by other controllers if required: public function fetchLatest($count = 5) { $cutOff = date('Y-m-', strtotime('-3 months')) $where = array('date_created > ?' => $cutOff); $order = "date_created DESC"; Listing 2.6 Automatically maintaining date fields in a model Calls Zend_DB_Table’s b insert() function Sets date field if not already set Summary 39 return $this->fetchAll($where, $order, $count); } Again, this is very simple functionality that becomes much more powerful when placed in the right layer of the MVC triumvirate. Note that we use a parameterized array for the $where variable, which ensures that Zend_Db will protect us against SQL injection attacks. 2.5 Summary We have now written a simple Hello World application using Zend Framework, and we have explored the way the Model-View-Controller design pattern is applied to our applications. You should now have a good idea of how Zend Framework can make our applications maintainable and easy to write. One ideal that the framework developers try to adhere to is known as the 80/20 rule. Each component is intended to solve 80 percent of the problem space it addresses, and provides flex points to enable developers who need to work with the other 20 percent. For example, the front controller system provides a router that covers nearly all requirements. Ifyou need a more specialized router, it is very easy to insert your own into the rest of the front controller setup. Similarly, Zend_View_Abstract allows for adding other template engines, such as Smarty or PHPTAL, if the supplied Zend_View is not suitable for your application. We will now move on to build a fully functioning community website that will utilize most of the components supplied with the framework. Data-driven and continuous development and deployment of modern web applications depend critically on registering changes as fast as possible, paving the way for short innovation cycles. A/B testing is a popular tool for comparing the performance of different variants. Despite the widespread use of A/B tests, there is little research on how to assert the validity of such tests. Even small changes in the application’s user base, hard- or software stack not related to the variants under test can transform on possibly hidden paths into significant disturbances of the overall evaluation criterion (OEC) of an A/B test and, hence, invalidate such a test. Therefore, the highly dynamic server and client run-time environments of modern web applications make it difficult to assert correctly the validity of an A/B test. We propose the concept of test context to capture data relevant for the chosen OEC. We use pre-test data for dynamic base-lining of the target space of the system under test and to increase the statistical power. During an A/B experiment, the contexts of each variant are compared to the pre-test context to ensure the validity of the test. We have implemented this method using a generic parameterfree statistical test based on the bootstrap method focussing on frontend performance metrics.A/B testing is a common pattern for gradient-based, datadriven optimization of user experience. Interpreting the results of A/B tests pose challenges concerning statistical inference and the reduction of the variability of the results. As has to be done for every statistical test and even before analyzing an A/B test, its validity must be asserted [5]. To validate an A/B test for variants of some feature X, all metrics have to be checked that might impact the overallContext-based A/B Test Validation Michael Nolting Sevenval Technologies GmbH Köpenicker Str. 154 10997 Berlin, Germany michael.nolting@sevenval.com Jan Eike von Seggern Sevenval Technologies GmbH Köpenicker Str. 154 10997 Berlin, Germany eike.seggern@sevenval.com ABSTRACT Data-driven and continuous development and deployment of modern web applications depend critically on registering changes as fast as possible, paving the way for short innovation cycles. A/B testing is a popular tool for comparing the performance of different variants. Despite the widespread use of A/B tests, there is little research on how to assert the validity of such tests. Even small changes in the application’s user base, hard- or software stack not related to the variants under test can transform on possibly hidden paths into significant disturbances of the overall evaluation criterion (OEC) of an A/B test and, hence, invalidate such a test. Therefore, the highly dynamic server and client run-time environments of modern web applications make it difficult to assert correctly the validity of an A/B test. We propose the concept of test context to capture data relevant for the chosen OEC. We use pre-test data for dynamic base-lining of the target space of the system under test and to increase the statistical power. During an A/B experiment, the contexts of each variant are compared to the pre-test context to ensure the validity of the test. We have implemented this method using a generic parameterfree statistical test based on the bootstrap method focussing on frontend performance metrics. Keywords Frontend performance monitoring; A/A testing; A/B testing; bootstrapping; dynamic base-lining 1. INTRODUCTION A/B testing is a common pattern for gradient-based, datadriven optimization of user experience. Interpreting the results of A/B tests pose challenges concerning statistical inference and the reduction of the variability of the results. As has to be done for every statistical test and even before analyzing an A/B test, its validity must be asserted [5]. To validate an A/B test for variants of some feature X, all metrics have to be checked that might impact the overall Copyright is held by the author/owner(s). WWW’16 Companion, April 11–15, 2016, Montréal, Québec, Canada. ACM 978-1-4503-4144-8/16/04. http://dx.doi.org/10.1145/2872518.2889306. evaluation criterion (OEC) and are not dependent on X. If any of these metrics differs significantly between the samples for variants A and B the A/B test might be invalid. For example, consider an A/B test of a backend feature of a web application using the conversion rate as OEC. Such a test should not influence the Javascript error rate of the client application. If the error rates differ between samples A and B, e.g. due to different browser-variant shares in the samples, this might affect the conversion rate and, thus, enhance, balance or even invert the effect of the backend feature that is the subject of the test—that is invalidating the test. We propose the concept of a test context to assert the validity of an A/B test. This concept is intended to assist other A/B testing frameworks (e.g. PlanOut [2]). A context will contain the metrics that might impact the overall evaluation criterion (OEC) of the A/B test. A pre-test sample is used to define the target space of the tested system (dynamic base-lining) because, first, this procedure relieves the user from defining the target space by hand (a tedious and complicated task even for a test context containing only a handful of metrics) and, second, it increases the statistical power of the validation, i.e. the required sample size to detect a given effect size if one of the metrics worsens [3]. In this paper, we present our implementation of this concept, history-diagnostics [1]. Focussing on frontend centric metrics, we will shortly introduce the method in Section 2 followed by an exemplary application in Section 3 and our conclusions and outlook in Section 4. 2. CONTEXT-BASED TEST VALIDATION Using pre-experiment data to improve the statistical power of A/B tests has been used elsewhere [3]. But to our knowledge, pre-experiment data has not been used to improve the validation of A/B tests. In the following we will give a general overview of the method followed by an exemplary application to frontend-centric metrics. The method expects three different data samples: the preexperiment sample STS to define the target space and the test-variant samples SA and SB. Each sample can contain data from different sources, e.g. front- and backend. Based on these samples, the metrics relevant to the test under validation are calculated. The user can define a set of functions of the samples to be used as metrics. Each metric m is expected to be scalar and ordinal, i.e. there is a single direction in which the metric improves. Possible metrics are the performance or error rates of the front- and backend. The method consists of the following steps: 277 1. Dynamic base-lining of the target space by drawing bootstrap samples from STS and estimating the distributions of the metrics [4]. 2. Calculate the metrics for samples SA and SB and, based on the bootstrapped distributions, the probability to find worse metric values. 3. Report the smallest of these probabilities, min m∈metrics PBS [M worse than m(S)] , (1) for SA and SB individually. If one of these two reported figures is below 5 %, we have a strong indication that the test is not valid. This method is flexible in two ways: It makes no assumptions about the distribution of the sampled data or metrics1 and it can take into account data from many different sources. Focussing on frontend-centric metrics, the data samples consist of frontend requests. For each request, we record its time t, the frontend performance p and any number of events vk, e.g. Javascript errors that occurred during the request.2 Typically, the event records are binary (1 or 0), i.e. event k did occur or not. A possible set of metrics based on this data is the following: To estimate the overall performance, we use the median, π(S) = median(p1, . . . , pn), (2) to be more robust against outliers. The traffic rate is estimated by τ (S) = n/T , (3) where n is the number of requests in sample S and T is the sampling duration. We estimate the event rates similarly but normalize them to the observed traffic, φk(S) = Xn i=1 vk,i/n . (4) When drawing bootstrap samples care has to be taken that not only the performance and event rates vary but also the traffic rate. In our implementation this is achieved by reweighting the sampled data with Poissonian weights. 3. EXEMPLARY APPLICATION To illustrate the performance of context-based A/B test validation, we consider an A/B test in which partial page loading (PPL) was enabled for a medium-traffic (160 page views per hour) web application to improve the application’s frontend performance. In this test, the application’s OEC did not improve significantly. Fig. 1 shows the response of the minimal probability, Eq. (1), for the frontend-centric metrics defined above in a semi-logarithmic plot. The only event taken into account was the occurrence of Javascript errors. The curve of the PPL-enabled test case B drops below the significance threshold of 5 % (black line in Fig. 1) early on during the test, while the original variant A stays at values ∼ 1, indicating no significant change. Further investigations of this case showed, that the drop of variant B was due an increased frontend error rate, which counter-acted improvements of the application’s OEC due to an improved frontend performance. 1Except for metrics being scalar and ordinal. 2Our repository [1] contains Javascript code to collect the frontend data. Nov 28 2015 Nov 29 2015 Nov 30 2015 Dec 01 2015 Dec 02 2015 Dec 03 2015 10-6 10-5 10-4 10-3 10-2 10-1 100 min Pworse A B Figure 1: Exemplary application of context-based A/B test validation. Variant A is un-modified, B is with PPL enabled. 4. CONCLUSIONS AND OUTLOOK We presented a method for context-based A/B test validation that uses pre-experiment data to increase statistical power. Our implementation is available on Github [1] together with a tutorial. We use this method in our frontendperformance monitoring dashboard FDX3 to validate A/B tests. From that usage, we selected an exemplary use case that shows the power of this method to detect disturbances in the context of an A/B test that invalidate this test. We plan to improve the method’s power by adding more metrics and using statistical tests more suiting to specific metrics in addition to the generic non-parametric bootstrap method. Furthermore, we explore a tighter integration with PlanOut [2] to simplify usage. 5. ACKNOWLEDGMENTS This work was partially funded by the ProFIT program of the Investitionsbank Berlin. 6. REFERENCES [1] https://github.com/sevenval/history-diagnostics. [2] Bakshy, E., Eckles, D., and Bernstein, M. S. Designing and Deploying Online Field Experiments. In Proceedings of the 23rd International Conference on World Wide Web, (2014), ACM, pp. 283–292. [3] Deng, A., Xu, Y., Kohavi, R., and Walker, T. Improving the sensitivity of online controlled experiments by utilizing pre-experiment data. In Proceedings of the sixth ACM international conference on Web search and data mining (2013), ACM, pp. 123–132. [4] Efron, B., and Tibshirani, R. J. An Introduction to the Bootstrap. Chapman & Hall, New York, 1993. [5] Kohavi, R., and Longbotham, R. Online controlled experiments and A/B tests. In Encyclopedia of Machine Learning and Data Mining, C. Sammut and G. Webb, Eds. 2015. 3FDX: Frontend Data Analytics 278Axiomatic Bootstrapping: A Guide for Compiler Hackers ANDREW W. APPEL Princeton University If a compiler for language L is implemented in L, then it should be able to compile itself. But for systems used interactively commands are compiled and immediately executed, and these commands may invoke the compiler; so there is the question of how ever to cross-compile for another architecture. Also, where the compiler writes binary files of static type information that must then be read in by the bootstrapped interactive compiler, how can one ever change the format of digested type information in binary files? Here I attempt an axiomatic clarification of the bootstrapping technique, using Standard ML of New Jersey as a case study. This should be useful to implementors of any self-applicable interactive compiler with nontrivial object-file and runtime-system compatibility problems. Categories and Subject Descriptors: D.3.4 [Programming Languages]: Processors—compilers; D.2.4 [Software Engineering]: Program Verification; D.2.6 [Programming Environments]: Interactive; D.4.9 [Operating Systems]: Systems Programs and Utilities—linkers; loaders General Terms: Verification Additional Key Words and Phrases: Bootstrapping 1. INTRODUCTION A conventional C compiler, written in C, is said to be “bootstrapped” if it compiles itself. Now, suppose a new version of the compiler source is written, that uses different registers for passing arguments. The old compiler can compile this source, yielding a new compiler. But look! The executable version cc’ of the new compiler uses the old parameterpassing style, but generates code that uses the new style. One can use the new compiler, however, to recompile all the libraries (and the new compiler itself) and get a “new new” executable that both uses and generates the new parameter-passing style. There is not much else to be said about bootstrapping C compilers (though see Section 6). But in a language with an interactive read-eval-print loop, commands are typed by the user, compiled immediately, and executed. Such a command, when compiled, may be a recursive call to the compiler itself, this time to compile a specified source file into a binary file. The compiler processing interactive commands This work was supported in part by NSF Grant CCR-9200790. Author’s address: Department of Computer Science, Princeton University, 35 Olden Street, Princeton NJ 08544-2087; e-mail: appel@princeton.edu. Permission to make digital/hard copy of all or part of this material without fee is granted provided that the copies are not made or distributed for profit or commercial advantage, the ACM copyright/server notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery, Inc. (ACM). To copy otherwise, to republish, to post on servers, or to redistribute to lists requires prior specific permission and/or a fee. ACM Transactions on Programming Languages and Systems, Vol. 16, No. 6, November 1994, Pages 1699–1718. 1700 · Andrew W. Appel is the same one that compiles a source file; this makes it difficult to cross-compile for a different target architecture! In fact, in a sufficiently feature-laden interactive compilation system, there are many constraints on the retargeting and bootstrap process. This paper is a case study of the Standard ML of New Jersey (SML/NJ) system, explaining the difficulties and how to manage them. Bin Files. Source files are transated by SML/NJ into “bin” files;1 each bin file contains the executable machine code for the corresponding source, and the exported static environment for that source [Appel and MacQueen 1994]. For example, if a source file defines two structures S and T , each with several components, then the static part of the bin file is a description of the structures S and T : the names and types of their components and substructures. The static part of the bin files is an ML data structure, complete with pointers, datatype constructors etc., created by the compiler and written in binary form to the bin file. Ordinarily, when a program is compiled to bin files by the interactive system, the bin files (including static part) will be read back into the same version of the system. But in bootstrapping, the compiled program is the “new” compiler, and we want to discard the “old” compiler. Thus, the new compiler executes, and loads in static information (from the bin files) about itself. For this to work, the representation of static environments in the old and new compilers must agree. This representation has two parts: the ML data types E (and their interpretation) chosen by the programmer; and the representation D of these data types as pointers and records in memory. Constraints on E might occur in any compiler that stores digested static information; constraints on D are a consequence of the fact that SML/NJ uses a pickler to write relocatable pointer data structures to the binary file just as they appear in memory. In-line Primops. When the SML/NJ compiler first executes, it initializes its static environment by constructing a special primitive basis containing the in-line primitive operations (such as +, :=, etc.). This static environment is built using the same ML datatypes as ordinary static environments, but it is directly constructed without parsing any input. The primitive environment is then imported and used by the ML code that implements the initial basis. When compiling a new version of the compiler, one can augment or change the primitive environment. But one cannot make use of the changed primitives until the new compiler compiles a “new new” compiler. Initial Basis. ML programs can assume an “initial basis” [Milner et al. 1990, p. 77], an environment in which certain types and values are defined. The compiler itself also relies upon the initial basis. Furthermore, the source code for the initial basis is part of the source code for the compiler. Finally, the source code for the initial basis uses, in places, elements of the primitive basis containing names of specially implemented in-line functions. This means that if some new version of the initial basis is desired, there are potential interactions with the rest of the system that must be considered. Interactive System. The normal mode of operating SML/NJ is to compile both interactive input and ML programs from source files, and run the compiled pro- 1This is true of SML/NJ versions since 0.96. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. Axiomatic Bootstrapping · 1701 grams in the same process as the compiler. It is particularly convenient to have an “interactive” compiler during the compiler development process, where individual compiler modules can be replaced “on-the-fly.” The compiled programs use the same initial basis as the compiler. The compiled programs must be callable from the compiler, and must be able to call the same initial functions that the compiler calls; thus, compiled programs must use the same calling conventions (etc.) as the compiler itself. This makes it difficult to bootstrap a new version of the compiler that uses different calling conventions. These interactions between the compiler and the executing program make for complications when (a new version of) the compiler is the executing program. Example The terminology of this example will be explained in Section 3, but the point here is to illustrate what can go wrong if one is not careful. Suppose there is a version “1” of the system as a set of ML source files σ1 and bin files β1. The bin files are compiled object files (like “.o” files in a C system), and are the result of compiling the source files using some compatible version of the compiler. We wish to use β1 to compile σ1, yielding a new version of the executable compiler. First, boot builds an executable µ1. This is like a link-loading step in a conventional system, but it must also load from β1 the digested description of the initial static environment, so that compilations in µ1 will have access to library modules (and the compiler itself) shared with µ1. This sharing is essential, if only so that µ1 and the interactive commands running within mu1 do not have separate copies of runtime-system management data that would trip over each other. boot(β1) = µ1 Now µ1 compiles the source σ1 into binary object files β0 1: compile(µ1, σ1) = β0 1 Now we hope that β1 ∼= β0 1, whatever that means. However, suppose one edits the source files to produce σ2, a new version of the compiler that uses a different calling convention. One might try the following steps: compile(µ1, σ2) = β2 boot(β2) = ⊥ The boot step fails because β2 is not self-consistent. The code generated by β2 from a top-level interactive expression (using the new calling conventions) is able to call functions in the Basis within β2 (compiled using the old conventions), so the first top-level command will dump core. On the other hand, some changes are harmless: if the only change is a different algorithm for code optimization, boot will probably succeed. Finally, some selfinconsistencies will manifest themselves at stages other than boot. How can one tell whether a change is harmless? And, since “nonharmless” changes are often necessary, how can one compile and bootstrap them correctly? The rest of the paper addresses these questions. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. 1702 · Andrew W. Appel 2. CHARACTERIZATION I will use the following symbols to describe characteristics of a “version” of the compiler: A. Architecture for which the compiler generates code, or on which it runs. C. Calling conventions for which the compiler generates code: which registers are used for what purpose; how end-of-heap is detected; whether a stack is used; etc. D. Datatype layout: how ML data types are laid out in memory. E. Environment representation: how static environments are described in terms of ML data types. B. Basis: the signature of the initial environment available to ordinary programs (and the compiler) upon startup. P. Primitive basis: the static environment created by the compiler, describing in-line primitive operations and data types. Normally the primitive basis is used only in compiling the source code for the initial basis. 2.1 Source Characteristics These characteristics are now used to describe the source code σ for some version of the compiler. The equations in this section just explain, in informal terms, the meaning of notation that will be used in the axioms of Section 3. a ∈ Agen(σ). The compiler σ may contain code generators for several different target architectures; architecture a is a member of this set. Cgen(σ) = c. The compiler σ generates code that uses the c calling conventions. Dgen(σ) = d. The compiler σ generates code that uses the d datatype layout scheme. Egen(σ) = e. The compiler σ uses (and writes to bin files) the e static environment representation. Buse(σ) = b. The nonbasis part of the system σ (that is, the compiler proper) is a program that makes use of functions in Basis b. Bimp(σ) = b. The basis part of the system σ implements the basis b. Puse(σ) = p. The basis part of the system σ is a program that makes use of the functions in primitive basis p. Pgen(σ) = p. The compiler σ defines a primitive environment p for its compiled code. 2.2 Binary-File Characteristics One can describe these aspects of compiled binary files in much the same way: Arun(β) = a. The program β runs on architecture a. a ∈ Agen(β). The compiler β generates code for the a architecture. Crun(β) = c. The program β follows the c calling conventions. Cgen(β) = c. The compiler β generates code that uses the c calling conventions. Drun(β) = d. The internal data structures of program β obey the d datatype layout scheme. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. Axiomatic Bootstrapping · 1703 Denv(β) = d. The static environments in bin files β use the d datatype layout scheme. Dgen(β) = d. The compiler β generates code that uses the d datatype layout scheme. Eenv(β) = e. The static environment’s bin files β are in the e environment representation. Egen(β) = e. The compiler β uses and generates the e environment representation. Buse(β) = b. The nonbasis part of β (that is, the compiler proper) is a program that makes use of functions in basis b. Bimp(β) = b. The basis part of β implements the basis b. Pgen(β) = p. The compiler β1 defines a primitive environment p1 for its compiled code. 2.3 Executable-File Characteristics The bin files are linked with a runtime system (and static environments are read from the bin files to initialize the compiler’s user-visible “initial basis”) to form an executable file µ, whose characteristics are just like those for bin files β, except that: —Executable files do not have separate static environment sections as bin files do, so Eenv and Denv do not apply. —Executable systems generate code for only one machine, so Agen(µ) is a single architecture rather than a set of architectures. 3. AXIOMS I will give axioms describing the procedures of compiling, bootstrapping, retargeting, and elaboration; these axioms will then be used to prove theorems in Section 4. 3.1 Compiling To compile source code, one executes the interactive system µ, and gives commands to compile source files σ into binary files β: compile(µ, σ) = β for which the following equations must hold: Buse(σ) v Bimp(σ) Puse(σ) v Pgen(µ) The relation v expresses the ML signature-matching relation. That is, Puse(σ) v Pgen(µ) means that the source files σ can be compiled in a primitive environment created by µ: every identifier looked up will be present and have an appropriate type. The “basis” (B) part of σ defines modules that are then used by the “compiler” part of σ, so the first equation is straightforward. But the “primitives” (P) containing special in-line function definitions must be specially constructed by µ. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. 1704 · Andrew W. Appel Hash compile Cgen Dgen Buse Bimp Puse Pgen Agen β Egen σ Arun Agen Crun Cgen Drun Dgen Egen Buse Bimp Pgen µ Arun Agen Crun Cgen Drun Denv Dgen Eenv Egen Buse Bimp Pgen Fig. 1. “T-diagram” for compile. The binary files (i.e., the files in the bin directory) β are then characterized by the following equations. Arun(β) = Agen(µ) Agen(β) = Agen(σ) Crun(β) = Cgen(µ) Cgen(β) = Cgen(σ) Drun(β) = Dgen(µ) Dgen(β) = Dgen(σ) These first six equations are unremarkable, and would occur in practically any compiler. Denv(β) = Drun(µ) This equation results from the use of a “pickler” for writing the static type information (pointer data structures) to a file in the same binary format that is used in core. Eenv(β) = Egen(µ) Egen(β) = Egen(σ) These two equations on E would hold in any compiler that writes digested static type information, with or without the use of a pickler. Bimp(β) = I(Bimp(σ)), where I is a hash function that computes “persistent identifiers” from the static environment exported by a source program. The persistent identifiers are then used for linking the machine code of different modules together, in a guaranteed type-safe way. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. Axiomatic Bootstrapping · 1705 Agen Crun Cgen Drun Denv Dgen Eenv Egen Buse Bimp Pgen β Arun µ boot Arun Agen Crun Cgen Drun Dgen Egen Buse Bimp Pgen Fig. 2. Schematic for boot. Buse(β) = Bimp(β). The two equations on B are a consequence of sharing library code and data between the interactive system and the compiled user code. Pgen(β) = Pgen(σ). True in any compiler that defines functions that look ordinary to the user but are compiled specially (e.g., in-line). These equations are summarized schematically in Figure 1. 3.2 Bootstrapping The bootstrapper is a part of the C language runtime system. It knows just enough to extract the dynamic part (machine code) from bin files β; but not the format of static environment representations, which only the compiler understands. However, the machine code within β is the compiler; once it starts running, it can load the static part of β to form an environment (symbol table of the initial basis) for compiling user programs. The result is an interactive compiler µ: boot(β) = µ. For this to work, the following equations must hold: Arun(β) ∈ Agen(β). So that the compiler and top-level interactive commands can both run on the same computer. Crun(β) = Cgen(β). So that top-level interactive commands can call and be called by the compiler and initial basis. Drun(β) = Dgen(β). for the same reason. Denv(β) = Drun(β). So the bootstrapping compiler can read static environments from bin files. Eenv(β) = Egen(β). For the same reason. The remaining equations characterize the output µ: Arun(β) = Arun(µ) = Agen(µ) Crun(β) = Crun(µ) ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. 1706 · Andrew W. Appel Cgen(β) = Cgen(µ) Drun(β) = Drun(µ) Dgen(β) = Dgen(µ) Egen(β) = Egen(µ) Buse(β) = Buse(µ) Bimp(β) = Bimp(µ) Pgen(β) = Pgen(µ) These equations are summarized in Figure 2. Now, for example, one can see that the boot failure described in Section 1 is because Crun(β2) 6= Cgen(β2) violating one of the preconditions for boot. 3.3 Retargeting Because it is impossible to bootstrap using compile and boot if the new compiler uses a new calling sequence or environment representation, two special procedures are provided. The first of these is called retarget: Run an interactive compiler µ1, and load the bin files β for a different version of the compiler as a “user program.” Since β may include code generators for many machines, one can also specify which target architecture a’s code generator should be selected from β. retarget(µ1, β, a) = µ2 The compiler originally present in µ1 will be used in µ2 for compiling top-level interactive commands, but the compiler β will be used in µ2 for turning source files into bin files. Ordinary user programs do not provide their own implementation of the initial basis, so the basis portion of β (corresponding to the that implement Bimp(β)) will not be loaded: Bimp(β) is irrelevant. However, the nonbasis portion of the compiler β must be compatible with the basis already running in µ1, so that β can call upon standard I/O functions (etc.) built into µ1. As an ordinary user program, the bin files β executing under the supervision of µ1 can generate code for any architecture or any calling sequence. This is because the code is not going to be executed in the current process, so it need not be compatible with the instruction set or calling conventions that µ1 itself is using. This freedom is crucial for cross-compilation (compilation for a different target architecture or calling convention). The following restrictions apply (see Figure 3): a ∈ Agen(β) Arun(β) = Arun(µ1) Crun(β) = Crun(µ1) Drun(β) = Denv(β) = Drun(µ1) Eenv(β) = Egen(µ1) Buse(β) = Bimp(µ1) ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. Axiomatic Bootstrapping · 1707 Agen Crun Cgen Drun Dgen Egen Buse Bimp Pgen µ Arun µ a retarget Arun Agen Crun Cgen Drun Denv Dgen Eenv Egen Buse Bimp Pgen β Arun Agen Crun Cgen Drun Dgen Egen Buse Bimp Pgen Fig. 3. Schematic for retarget. The last equation is an exact signature match. In particular, it means that the intrinsic persistent identifiers generated from the compilation of the initial basis (files in the src/boot directory) in building the bin files within µ1 must be identical to the corresponding identifiers in the initial basis portion of β. 2 The following equations characterize the output µ2: Arun(µ2) = Arun(µ1) Agen(µ2) = a Crun(µ2) = Crun(µ1) Cgen(µ2) = Cgen(β) 2This can be guaranteed by producing β and µ1 from the same compiler µ0, in the following way: compile(µ0, σ1) = β1 boot(β1) = µ1 compile(µ0, σ2) = β where the source codes for the Bimp portions of σ1 and σ2 are identical. This works because I is really a function: (x = y) ⇒ (I(x) = I(y)). In versions 0.96–0.98 of the SML/NJ system, the “persistent identifiers” were just timestamps, so that I would return different results at different times. Therefore, with the procedure outlined at the top of this footnote, B(β) and B(β1) would not export the same “persistent identifiers” even though the source code was identical. Instead, one would have to create a new, empty bin directory; copy just the bin files for the initial basis from β1 to the new bin directory β; and then proceed with a compile that would use these files as a starting point. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. 1708 · Andrew W. Appel Drun(µ2) = Drun(µ1) Dgen(µ2) = Dgen(β) Egen(µ2) = Egen(β) Buse(µ2) = Buse(µ1) Bimp(µ2) = Bimp(µ1) Pgen(µ2) = Pgen(β) This is funny hybrid indeed. 3.4 Elaboration Finally, elab is a special variation on boot that reparses the source files to build the static environment, instead of reading it from the bin files: elab(β, σ) = µ. Now, given the two steps compile(µ0, σ) = β elab(β, σ) = µ β must satisfy all the equations given for boot above except for the ones involving Denv(β) and Eenv(β), because the environments will not be read from the bin files. Another requirement for elab is that σ and β must be related by the compile command shown (see Figure 4). The resulting executable µ is defined by the same equations as for boot(β). In fact, with elab there is no need for boot, except that elab is much slower because it reparses all the source. 4. STABLE VERSIONS Definition. (σ, β) form a stable version if the following equations hold: Arun(β) ∈ Agen(σ) = Agen(β) Cgen(σ) = Crun(β) = Cgen(β) Dgen(σ) = Drun(β) = Dgen(β) = Denv(β) Egen(σ) = Eenv(β) = Egen(β) Buse(σ) v Bimp(σ) Bimp(β) = I(Bimp(σ)) Buse(β) = Bimp(β) Puse(σ) v Pgen(σ) = Pgen(β) Remark: If compile(boot(σ, β0 ), σ) = β0 then (σ, β) is a fixed point, a stronger property. But we cannot prove fixed-point properties from the axioms in this paper. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. Axiomatic Bootstrapping · 1709 elab Drun Dgen Egen Buse Bimp Pgen µ Cgen Dgen Buse Bimp Puse Pgen Agen Egen σ Arun Agen Crun Cgen Drun Denv Dgen Eenv Egen Buse Bimp Pgen β Arun Agen Hash compile Crun Cgen Fig. 4. Schematic for elab. Suppose one starts with a stable version (σ1, β1) and creates a new source σ2. How can one obtain bin files β2 to make a stable version with the new source? All of the “theorems” in this section are proved using only the axioms of Section 3 and the definition of a stable version. 4.1 New Primitive Basis Suppose Pgen(σ2) 6= Pgen(σ1), Puse(σ2) v Pgen(σ2), but all other characteristics (Agen, Cgen, Dgen, Egen, Buse, Bgen, Puse) are identical from one source to another. Then boot(β1) = µ1 compile(µ1, σ2) = β2. The reader can verify that both of these steps succeed, and that (σ2, β2) is stable. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. 1710 · Andrew W. Appel 4.2 New Initial Basis Now suppose σ2 differs from σ1 in the initial basis (e.g., standard library, which is implemented in and used by the compiler, and is also used by client programs); and perhaps also in Puse, the set of primitives used in the initial basis. boot(β1) = µ1 compile(µ1, σ2) = β2 The reader can verify that these steps succeed and result in (σ1, β2) stable. 4.3 New Environments Suppose σ2 uses a different environment representation (Egen) from σ1. The “ordinary” procedure will not work: boot(β1) = µ1 compile(µ1, σ2) = β2 boot(β2) = ⊥ Now Egen(β2) 6= Eenv(β2), so β2 cannot be used in boot. There are two ways to build a stable version: elab(β2, σ2) = µ2 compile(µ2, σ2) = β0 2 or retarget(µ1, β2, Arun(µ1)) = µ0 2 compile(µ0 2, σ2) = β00 2 Now, (σ2, β0 2) is stable, and so is (σ2, β00 2 ); β0 2 and β00 2 are equivalent in all properties. 4.4 New Datatype Layout If the new compiler uses a different datatype layout (that is, Dgen(σ2) 6= Dgen(σ1)) then the following steps will build a stable version. boot(β1) = µ1 compile(µ1, σ2) = β2 elab(β2, σ2) = µ2 Retarget will not do the job; for suppose boot(β1) = µ1 compile(µ1, σ2) = β2 retarget(µ1, β2, a) = µ0 2 compile(µ0 2, σ2) = β0 2 then Denv(β0 2) = Dgen(σ1), while Drun(β0 2) = Dgen(σ2). Thus β0 2 cannot be used as input to either boot or retarget. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. Axiomatic Bootstrapping · 1711 4.5 New Calling Conventions Suppose σ2 uses new calling conventions: Cgen(σ2) 6= Cgen(σ1). The procedure is: boot(β1) = µ1 compile(µ1, σ2) = β2 retarget(µ1, β2, Arun(µ1)) = µ2 compile(µ0 1, σ2) = β0 2 Now (σ2, β0 2) is stable. The reader can verify that retarget is necessary, and that elab would not suffice. 4.6 New Target Architecture Given (σ, β) stable, Arun(β) = a1, suppose one wishes to make a compiler that runs on architecture a2, for a2 ∈ Agen(β). boot(β) = µ1 retarget(µ1, β, a2) = µ2 compile(µ2, σ) = β2 Now (σ, β2) is a stable compiler running on, and generating code for, architecture a2. 4.7 Getting from Here to There Suppose there is a stable version (σ1, β1), and a compiler µ1 = boot(β1). The programmer makes a new source σ2 that differs in every characteristic from σ1. Let us assume, however, that Puse(σ1) v Pgen(σ2). There may well exist a β2 such that (σ2, β2) is stable, but we do not have such a β2. How is it to be obtained? The first problem is that compile is inapplicable, since we cannot assume either Buse(σ2) v Bimp(σ2) or Puse(σ2) v Pgen(µ1). The procedures boot, retarget, and elab are not useful, since they just take the binary files β1 that we already have. elab(β1, σ2) is illegal (as the reader may verify), and the author cannot even imagine why it might be useful. The trick is to make some intermediate versions of the source code: σx is like σ1 but defines augmented primitives P; σy is like σx, but makes use of the augmented primitives and provides an augmented basis Bimp. So, σx is as follows: Agen(σx) = Agen(σ1) Cgen(σx) = Cgen(σ1) Dgen(σx) = Dgen(σ1) Egen(σx) = Egen(σ1) ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. 1712 · Andrew W. Appel boot boot Arun Agen Crun Cgen Drun Denv Dgen Eenv Egen Buse Bimp Pgen β a retarget Arun Agen Crun Cgen Drun Dgen Egen Buse Bimp Pgen µ Arun Agen Crun Cgen Drun Dgen Egen Buse Bimp Pgen µ Hash compile Crun Agen elab Arun Agen Crun Cgen Drun Dgen Egen Buse Bimp Pgen µ Arun Agen Crun Cgen Drun Denv Dgen Eenv Egen Buse Bimp Pgen β Arun Agen Crun Cgen Drun Denv Dgen Eenv Egen Buse Bimp Pgen β Cgen Arun Dgen Cgen Buse Bimp Puse Pgen Agen Egen σ Arun Agen Crun Cgen Drun Denv Dgen Eenv Egen Buse Bimp Pgen β Hash compile Hash compile Hash compile Hash compile Cgen Dgen Buse Bimp Puse Pgen Agen Egen σ Drun 2 Cgen Dgen Buse Bimp Puse Pgen Agen Egen σ2 Cgen Dgen Buse Bimp Puse Pgen Agen Egen σ Denv 2 Cgen Dgen Buse Bimp Puse Pgen Agen Egen σ Dgen Hash compile Cgen Dgen Buse Bimp Puse Pgen Agen Egen σ Eenv Egen Buse Bimp Pgen β Arun Agen Crun Cgen Drun Dgen Egen Buse Bimp Pgen µ Arun Agen Crun Cgen Drun Dgen Egen Buse Bimp Pgen µ Arun Agen Crun Cgen Drun Dgen Egen Buse Bimp Pgen µ y x 1 x x y y y z z z ’ 2 2 2 Fig. 5. Getting from here to there. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. Axiomatic Bootstrapping · 1713 Buse(σx) = Buse(σ1) Bimp(σx) = Bimp(σ1) Puse(σx) = Puse(σ1) Pgen(σx) = Pgen(σ2) Then: compile(µ1, σx) = βx boot(βx) = µx Now version σy is another intermediate version: Agen(σy) = Agen(σ1) Cgen(σy) = Cgen(σ1) Dgen(σy) = Dgen(σ1) Egen(σy) = Egen(σ1) Buse(σy) = Buse(σ2) Bimp(σy) = Bimp(σ2) Puse(σy) = Puse(σ2) Pgen(σy) = Pgen(σ2) Now: compile(µx, σy) = βy boot(βy) = µy compile(µy, σ2) = βz retarget(µy, βz) = µz compile(µz, σ2) = β0 z elab(β0 z, σ2) = µ2 compile(µ2, σ2) = β2 Now (σ2, β2) is stable. The proof is just simple (but tedious) equational reasoning, checking that the preconditions of each step are satisfied and characterizing the intermediate results βx, µx, βy, µy, etc. Figure 5 presents this proof schematically, where grey lines indicate the σ2 version of each characteristic. It does seem amazing that five compilations are required to get from stable version 1 to stable version 2. But I have not found a shorter sequence. 5. GENERALITY In what sense do the “characteristics” A, C, D, E, B, P form, in any sense, a complete set? ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. 1714 · Andrew W. Appel The axioms cannot assure the correctness of the compiler. Specifying that a 50,000-line program implements faithfully the 100-page Definition of Standard ML[Milner et al. 1990] is not something that can be done with eight or ten simple equations in the style shown in this paper. The axioms are meant as abstractions of only those aspects of bootstrapping that often prove problematical. Many other aspects of ML compilation, though difficult or interesting, pose no special problems when bootstrapping and are entirely ignored by the axioms. However, perhaps there are other important issues related to bootstrapping that are not accurately characterized by any of the axioms. 5.1 Runtime System ML requires a runtime system, to do garbage collection, to handle system calls, and to provide various functions implemented in C or assembly language. The runtime system must know the format of ML data types (to do garbage collection) and must satisfy other constraints. A runtime system ρ has the properties Arun(ρ), the architecture on which it runs; Crun(ρ), the calling conventions for ML-callable entry points; and Drun(ρ), the ML datatype layout that it understands. To model runtime systems, we extend boot with a runtime-system argument: boot(ρ, β) = µ with extra preconditions Arun(β) = Arun(ρ) Crun(β) = Crun(ρ) Drun(β) = Drun(ρ) Elaboration also requires a particular runtime system: elab(ρ, β, σ) with the same three preconditions. The implications of these constraints turn out to be quite trivial; runtime system issues cause no bootstrapping problems, except as described in the next subsection. 5.2 Structured I/O Format For example, John Reppy recently rewrote the “pickler” in the runtime system, that writes pointer data structures to files (and reads them back). In particular, the pickler writes static environment representations to bin files β. Reppy’s new pickler uses a different file format from the old one. The implementation of (either version of) the pickler, and any knowledge about file format, is entirely within the runtime system. We could characterize this as Fgen(ρ), the format that a given runtime system uses to write ML datatypes to a file. Then the bin file β would have a characteristic Fenv(β), the format in which static environments have been written; and executables µ would have the characteristic Fgen(µ) based on the format that µ’s runtime system uses. Then we have the following additional axioms. For compile(µ, σ) = β we have Fgen(µ) = Fenv(β). For boot(ρ, β) = µ we have Fenv(β) = Fgen(ρ) Fgen(µ) = Fgen(ρ) ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. Axiomatic Bootstrapping · 1715 (the first is a precondition, the second characterizes the output µ). For retarget(µ1, β, a) = µ2 we have Fenv(β) = Fgen(µ1) Fgen(µ2) = Fgen(µ1). And finally, for elab(ρ, β, σ) = µ we have only Fgen(µ) = Fgen(ρ), and Fenv(β) irrelevant. Clearly, Reppy will need to use elab in order to bootstrap his new structureblaster format, since boot and retarget are too restrictive. This example has illustrated that the axioms of Section 3 do not necessarily form a complete set, but the axiomatic method is easily extensible to meet new challenges. 5.3 New Module-Field Layout Older versions of SML/NJ sorted the value fields of a signature into alphabetical order before generating code. This meant that the translation of this module S structure S = struct val b = 5 val a = 7 end would be as a record in memory in which a (7) appeared first, followed by b (5). Current versions of SML/NJ do not sort into alphabetical order. Thus, bin files compiled by the new version should be incompatible with executables of the old version. Consider the axiomatization. We say that: Ggen(σ) is the sorting (nonsorting) technique used for structure fields by source code σ; Grun(β) is the structure-field layout algorithm that had been used in compiling β; Ggen(β1) is the structure-field layout algorithm that β uses in generating output code; Grun(µ) is analogous to Grun(β); Ggen(µ) is analogous to Ggen(β); Grun(ρ) is the ordering that ρ uses for interfacing its own “primitive” structures visible from the ML program. The next step is to write axioms for G. This is not trivial, as it involves an understanding of how the compiler and generated code work. It turns out, however, that the equations for G in the steps compile, boot, retarget, elab are exactly parallel to the equations for C. This implies that G was not necessary at all, and that C expresses (among other things) the ordering of structure fields. This is a measure of the robustness of the original axioms. 5.4 Record Field Ordering Record fields are also sorted by label in SML/NJ. Sorting is required by the semantics of the language, but any consistent ordering will do. The sorting could, in ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. 1716 · Andrew W. Appel principle, be done in some nonstandard (i.e. nonalphabetical) order. Since records are used directly in the implementation of static environments, and structures are not, the effect of record fields turns out to be axiomatized exactly like datatype layouts D, not like calling sequences C. This should not be surprising, as the record type {a,b} is indeed a kind of type constructor (just like a datatype) and the layout into bits of ML data types is exactly what D was supposed to characterize. 6. RELATED WORK Lecarme et al. [1982] present a good explanation of a theory of bootstrapping using T-diagrams, a notation invented by Bratman [1961] and formalized by Earley and Sturgis [1970]. This theory is simple and elegant, and the diagrams are pretty to look at. It is very successful in describing the steps needed to produce a compiler from source language SL to object language OL written in implementation language WL, when one has (for example) a machine executing instruction set XL, a translator from WL to XL implemented in XL, an interpreter for OL written in AL, and a translator for AL written in ..., and so on. The T-diagrams seem more compact, and easier to read once one learns how, than the corresponding equational theory. In fact, Earley and Sturgis provide an algorithm to construct a bootstrap sequence: given a set of translators and interpreters (characterized by source, object, and implementation languages), and a desired translator (similarly characterized) their algorithm can either show how to construct the desired result or prove that it cannot be done. Perhaps an algorithm such as this could be devised to prove the theorems of Sections 4.1–4.6. Lecarme goes further, with a flowchart that provides hints about what existing translator should be modified “by hand” (to produce a different target language, or to accept a different source language, or to run in a different implementation language) to get to the desired result. Note the similarities with the hand-made intermediate versions σx, σy needed in Section 4.7. The added problem in SML/NJ (and in similar interactive systems, especially those that have predigested type information) is that there are extra constraints between the implementation language and the object language that “opaque” Tdiagrams do not express. Furthermore, the different languages in question are all quite similar: executable code described by (in this case) six characteristics, where many of the characteristics are likely to match between any two versions. In using opaque T-diagrams, the similarities between two executables (e.g., identical data type representation) are lost, and would have to be expressed separately in a set of equations. This paper has demonstrated T-diagrams with internal structure (representing equational constraints), which are more powerful than “opaque” Tdiagrams. Thompson’s Turing award lecture [Thompson 1984] describes from a different point of view how bugs (and viruses) can propagate through the bootstrapping process. 7. CONCLUSION When compiled code shares important parts of the environment with the compiler itself, bootstrapping new versions can be complicated, and previous theories of ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. Axiomatic Bootstrapping · 1717 bootstrapping do not seem to extend well. Clearly written axioms can help the poor compiler hacker deal with the complexity. Certain choices made in the SML/NJ system complicate bootstrap process: —The SML/NJ system uses a “pickler” to write static environments to binary files in (almost) exactly the same format that’s used in memory. The resulting constraints on Denv(β) cause the procedures of Sections 4.4 and 4.7 to take extra elab steps. —The sharing of code and data between the compiler and user programs requires the compiler to load its own static environments, causing constraints on E and B. —The use of the same compiler for compiling interactive commands and source files for the compiler itself requires a special retarget mechanism for relaxing constraints on A and C. However, each of these features is useful in its own way. The axiomatization of bootstrapping makes it easier to tolerate complexity in the process, so that these features can be more easily supported. Appendix: Command realization Each of the abstract functions compile, boot, retarget, elab corresponds to a sequence of operating-system commands (a shell script) or ML commands. Let µ be an interactive sml executable with the Compilation Manager (make system) loaded, called sml-cm. Let σ be a set of source files for the compiler in directory src. Then compile(µ, σ) is just cd src; echo "Batch.make()" | sml-cm Supposing that the target architecture is sparc (Agen(µ) = sparc), this creates a directory β = bin.sparc containing bin files. Bootstrapping (boot(ρ, β)) is done by two shell scripts: makeml compiles the runtime system ρ (written in C and assembly language) from the src/runtime subdirectory, runs it to load the bin files β to create an executable sml, and makecm executes sml to load the Compilation Manager, creating an executable µ =sml-cm: cd src; makeml -bin bin.sparc; makecm Retargeting is done by instructing the compilation manager µ = sml-cm to load bin files for an alternate compiler in directory β = alt/bin.sparc (for example) and to select the alpha code generator within those files: echo ’retarget("alt/bin.sparc",".alpha"); exportML("sml-a")’ | sml-cm The result of this retarget(µ, β, α) is µ2 =sml-a. Finally, elaboration is just like boot but with an extra command-line flag -elab to makeml. REFERENCES Appel, A. W. and MacQueen, D. B. 1994. Separate compilation for Standard ML. In Proceedings of SIGPLAN ’94 Symposium on Programming Language Design and Implementation. ACM Press, New York, 13–23. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994. 1718 · Andrew W. Appel Bratman, H. 1961. An alternate form of the UNCOL diagram. Commun. ACM 4, 3 (Mar.), 142. Earley, J. and Sturgis, H. 1970. A formalism for translator interactions. Commun. ACM 13, 10 (Oct.), 607–617. Lecarme, O., Pellissier, M., and Thomas, M.-C. 1982. Computer-aided production of language implementation systems: A review and classification. Softw. Pract. Exp. 12, 9, 785–824. Milner, R., Tofte, M., and Harper, R. 1990. The Definition of Standard ML. MIT Press, Cambridge, Mass. Thompson, K. 1984. Reflections on trusting trust. Commun. ACM 27, 8 (Aug.), 761–763. Received January 1994; revised March 1994; accepted March 1994. ACM Transactions on Programming Languages and Systems, Vol. 19, No. 4, November 1994.PHP: Supporting the New Paradigm of Situational and Composite Web Applications Andi Gutmans Zend Technologies Inc. 19200 Stevens Creek Blvd. Cupertino, CA 95014 +1-408-253-8800 andi@zend.com ABSTRACT In this paper, I describe what we see as a paradigm shift in software development and how PHP plays into this change. 1. INTRODUCTION The software industry is going through a rapid transformation. The shift towards Service Oriented Architectures, especially valuable data exposed via Web Services is leading to a new set of applications and a new paradigm in their development. These applications tend to be both composite and situational in their nature, and require platforms that support iterative and flexible development. Due to these requirements, the industry is increasingly looking towards dynamic languages, also known as "glue" languages, to better support these requirements. PHP with its Web-centric approach, strong Web Services support, and ability to simplify tasks, makes it an ideal language for supporting this paradigm shift. 2. THE DYNAMIC NATURE OF PHP PHP’s dynamic nature is a major reason for its ability to simplify Web development. There are numerous aspects of this dynamic nature which will be discussed. 2.1 Dynamic typing In PHP, data types are automatically juggled depending on the context they are used in. This is especially useful in Web development where input and output are almost exclusively received and sent as string types. A language where these string types are automatically converted to other data types (e.g. integers) as needed can therefore save a lot of time and frustration. This fundamental nature of PHP including its emphasis on text processing dramatically reduces development time. 2.2 Overloading Capabilities Probably the biggest reason for why PHP’s support for Web Services excels are its unique set of overloading capabilities. By allowing method calls and property accesses to be intercepted and dynamically routed, PHP enables the creation of extremely simple programming interfaces where external object models can elegantly be mapped to native object syntax. One of the most popular uses of this feature is with XML. Among its rich XML support, PHP features a SimpleXML extension, which allows developers to access XML data through the native PHP object syntax. In addition, many of the PHP Web Services implementations, including SOAP, REST and syndication protocols are taking advantage of these overloading capabilities, and expose simple yet powerful APIs. 2.3 Other supporting functionality In addition to the mentioned language features, there are many additional features, mostly tied to PHP’s dynamic nature, which allow it to elegantly act as a glue language for composite applications. Such features include PHP’s ability to overload the foreach() iterator construct, its ability to fetch files in a transparent manner whether on local disk or a remote URL, or additional capabilities related to its dynamic nature such as reflection and the ability to self-inspect source code comments. 3. SUMMARY This paper only briefly describes the various traits of the PHP language which make it so suitable for use in composite and situational applications. In the talk itself, you will learn in greater detail what these PHP features look like and why they truly have the ability to simplify the development of composite applications. Examples include simplified ways for consuming and exposing Web Services, OO-RDMBS mapping, Search, XML and others. 4. ACKNOWLEDGMENTS Thanks goes to the vibrant PHP developers’ community, many of who have directly contributed to developing, documenting and testing some of the mentioned PHP features. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGMOD 2006, June 27-29, 2006, Chicago, Illinois, USA. Copyright 2006 ACM 1-59593-256-9/06/0006…$5.00. 707Analyzing PHP Frameworks for Use in a Project-Based Software Engineering Course Lisa Lancor & Samyukta Katha Southern Connecticut State University 501 Crescent Street New Haven, CT 06515 USA 203.392.5890 lancorl1@southernct.edu, kathas1@southernct.edu ABSTRACT Given the popularity of PHP frameworks used in developing webbased applications, a comparative study is conducted to determine which framework is best suited for incorporation into the curriculum of an undergraduate software engineering course that uses project-based learning. The top six PHP frameworks (Zend, Yii, CakePHP, CodeIgniter, PRADO, and Symphony) were initially considered and then narrowed down to two (CakePHP and CodeIgniter) based on their alignment with common functionality in previous class projects, framework complexity for those new to frameworks (learning curve), and developer friendliness (availability of documentation and online resources). An in-depth comparative study is conducted by developing a functionally-equivalent web application using each of the two frameworks as well as plain PHP (no framework). This work was motivated by the difficulties that were encountered in an evolving, content-rich software engineering course and discusses the educational changes that were made to align student learning with sound software engineering principles and current software development practices used in the computing industry. Categories and Subject Descriptors D.2.3 [Software Engineering]: Coding Tools and Techniques – object-oriented programming General Terms Design, Experimentation, Languages. Keywords Software design and development; project-based learning; PHP frameworks; software engineering. 1. INTRODUCTION In the last 10+ years, there has been a pedagogical shift in undergraduate software engineering classes away from learning by lecture on concepts related to the software development life cycle (SDLC) towards requiring students to experience the SDLC as they develop an actual application for a real client [1] [2]. It has been argued that this project-based learning pedagogy can better prepare students for software development positions beyond graduation as they are put in a position to learn how to gather and document client requirements, design and implement a solution (often collaboratively for those courses that require teamwork), and finally deliver an application that meets the client’s needs. All of this is done within a limited, semester-based timeframe thereby emphasizing additional software engineering concepts such as planning, project management, and software estimation. Adopting industry-based software tools and development technologies can further align student experiences in the classroom with workplace practices. PHP-based web frameworks are examples of a reusable software platform that has become prevalent for developing robust web applications. Incorporating PHP frameworks into the curriculum can help to emphasize key software engineering principles such as software architecture, reusability and unit testing. But, which PHP framework can be best incorporated into the curriculum of a project-based undergraduate software engineering course? This work performs a comparative study of the development of a small-scale web application (similar to previous student projects) using two popular PHP frameworks (CodeIgniter and CakePHP) as well as an implementation of the same application using plain PHP (without a framework). Both quantitative and qualitative analyses were conducted on the three development approaches. The quantitative analysis compares the web application frameworks in terms of performance, security and development effort (measured by lines of code). The qualitative analysis compares the web application frameworks in terms of ease of installation, developer friendliness, and web application extensibility. 2. BACKGROUND 2.1 The Course CSC330: Software Design and Development is a course that is required by all computer science students and is typically taken in the last two years of the major. CSC330 follows a typical onesemester software engineering curriculum that focuses on the entire software engineering process. Students are broken into teams of 4-5 students and remain on the same team throughout the semester to develop a real project for an on-campus client. Prior to the start of the semester, a “Request for Projects” is sent out to the entire campus community and after careful analysis, the professor chooses suitable projects for the class to develop. A sampling of completed projects which have benefitted the SCSU community include an on-campus employment website for the Career Services Department, a grant tracking application for the Department of Sponsored Programs and Research, an online competency exam for the Psychology Department and an alumni phone-a-thon application for the Office of Institutional Advancement. Almost all projects have been developed as a web application using PHP with MySQL. While not a formal prerequisite course, most students have taken our database course prior to taking this Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGCSE'13, March 6–9, 2013, Denver, CO, USA. Copyright © 2013 ACM 978-1-4503-1868-6/13/03...$15.00. 519 course. There are however, no prerequisite courses that teach PHP; students are expected to learn the language on their own. From a curricular perspective, CSC330 is normally taken one year prior to our capstone experience course (CSC400). Students are expected to incorporate the concepts from this software engineering course into their final individual capstone project in CSC400. 2.2 The Problem This course has been taught as a project-based course for more than 10 years. During this period, alumni and our Technical Advisory Committee, composed of local IT leaders, have provided significant feedback that has led to a number of curricular components that have been incorporated into the course. At this point however, the course has clearly reached its saturation point. As more material has been added to the course, students are becoming overwhelmed and as a result the final projects are suffering leaving some clients with unusable products. A detailed review of student experiences in the course reveals that learning a new language in a short period of time, working collaboratively in a team for an entire semester, completing the numerous written assignments associated with the different software phases, in addition to building a software solution for a real client from scratch, is daunting for most, if not all, students. Most troubling however, is that students were experiencing a severe disconnect between the software engineering concepts covered in the course and the development of the client’s project. Students admitted that they chose to “take the grade hit on software design, software architecture and security" because they were under pressure to deliver some level of working code to their client. When asked, "if your team had more time to develop the project or had a smaller project to work on, would you have focused more on design, architecture and security?" The overwhelming responses were either "maybe" or "probably not." This obviously was contradictory to the learning goals of the course. Clearly something had to be done. 2.3 Revamping the Course Attempting to remedy the problems identified in the course has led to a 4-phased approach. The first phase required a systemic look at the overall CS curriculum and the identification of areas in which content from the course could be either completely removed or at least introduced in other courses required in the major. Ideally, we would have liked to extend the course to a twosemester sequence separating the software engineering principles from the client project however, the major had already reached its maximum credit load. By removing project management concepts from the course and requiring students to take a project management course offered in the School of Business, we were able to make room in the course. This project management course requirement did not add additional credits to the major since it was formerly listed as an option in a group of other business course electives. Additionally, testing concepts are now emphasized in all CS prerequisite courses starting as early as our CS1 course thereby freeing up several course sessions that were dedicated to covering general testing concepts. The second phase of our approach occurred at the course level. Within this phase, we wanted to focus on, not only those areas that overwhelmed students (learning a new language independently, team collaboration, and the documentation requirements), but also on the areas that are critical to the course (software design and development is, in fact, the name of the course). Three new tools have been added to the course to address the student concerns related to teamwork and the time spent on documentation. For documentation, both Visual Paradigm for UML, Standard Edition along with Visual Paradigm's Agilian, Standard Edition (both free for academic institutions) is currently used in the course. Students can now do detailed UML diagrams (Use-Cases, Class Diagrams and Sequence Diagrams) and then use the Report Generation tool (available in the Standard Edition) to generate formalized reports of their software in HTML, PDF or Word with the touch of a button. This saves a considerable amount of time previously spent on documentation and allows for more time and effort to be spent on software design and development. With respect to team collaboration, BaseCamp [3] has been successfully used in the course to enhance overall collaboration. In addition, a GitHub [4] educational account (free) was recently acquired so that teams now have their own private development repository. The inclusion of GitHub into the course attempts to streamline the development bottleneck that was previously caused by the lack of a revision control system while at the same time, introduces students to a common tool used in industry. The use of all of these tools helps to alleviate the difficulties that students have had with respect to working collaboratively. Requiring students to independently learn a new language has created some serious issues in the course. Despite the fact that particular resources are recommended for learning PHP, students end up using a variety of other learning resources – some good and some bad. For example, many web-based tutorials and PHP books do not emphasize object-oriented design or an MVC architecture - two critical components that are emphasized throughout the course. As a result, the implementations of the final projects do not follow an object-oriented design and have an ad-hoc architecture at best. Additionally, while software reuse is talked about frequently in the course, few students embrace the concept in their final projects. Students typically attempt to manually develop their own PHP code for common web application features such as form validation, database connectivity, authentication, and session control. It is here that significant security weaknesses are often detected. With the increasing number of security breaches discovered in web applications based on poor programming techniques today, it is critical that our students develop sound programming practices that lead to robust software applications and not just working deliverables. In an attempt to remedy this problem, the newly revised course will now spend time covering PHP with an emphasis on OO design and web development using a framework and its built-in libraries to handle common web-based features (e.g. authentication, session management). The first offering of the newly modified course, which will include an emphasis on PHP and frameworks, is expected in the Spring of 2013. We are hopeful that the incorporation of frameworks into the course will help to emphasize sound software design architecture (MVC) and code reusability while at the same time yield more robust applications that will better meet the needs of our on-campus clients. The fact that many PHP frameworks exist leads us to the third phase of our project which tries to determine which PHP-based framework would be most appropriate for our project-based software design and development course. In this phase, the top 6 520 PHP-based frameworks were reviewed and the two frameworks that seemed most appropriate for the course were chosen for comparison. These two frameworks were then compared and contrasted during the development process of a web application that contains typical features found in student projects. The following sections of this paper describe this phase in detail. The fourth and final phase, yet to be completed, is an assessment of the revamped course that uses the "winning" PHP framework along with the other software development tools previously described in an attempt to determine if the course goals are being met. 3. PHP FRAMEWORKS 3.1 Background Web application frameworks are collections of libraries aimed at promoting high code reusability that can lead to faster development of web applications. Web application frameworks contain code for common functionalities like database access, session management and security. They help to reduce the development and testing overhead of web applications. They also support folder organization and structure, thereby promoting easier code maintainability. The Model–View–Controller (MVC) architectural pattern forms the basis for many of the existing web application frameworks. The MVC pattern allows for a well-defined separation of user interface, business logic and data model components of a web application. In addition, the MVC based design also defines the interactions between these components. Since the responsibilities of each component are well defined, it leads to better code modularization. The features of web application frameworks and the MVC architectural pattern are in perfect alignment with the goals of the course. We are hopeful that the time spent teaching these topics will not only result in the development of more robust applications for our on-campus clients but will also provide students with very marketable skills after graduation. 3.2 Choosing a PHP Framework Web application frameworks are available for most web programming languages including PHP, ASP.NET, C++, Perl, Python, Java and Ruby. PHP is a widely used, general-purpose scripting language that is especially suited for Web development [5]. It offers several advantages for quick development of dynamic web applications and promotes easy integration with various popular frameworks across multiple platforms. It is open source and also freely available. Because of PHP’s popularity, there are many PHP based frameworks that are currently available. Each framework has its specific advantages and disadvantages. Based on the Framework Usage Statistics [6], PHP frameworks are clearly the most popular web application frameworks in use today. In fact, the PHP framework distribution currently captures almost 40% of the top million sites on the Internet as shown in Figure 1. After reviewing three years of the final projects submitted by students, we identified a common list of web application functions/features that would need to be supported by the PHPbased framework chosen for the course. These include: form handling, form validation, session management, authentication and authorization, database interactivity (supporting CRUD: create, read, update and delete), CAPTCHA, and easy interaction with third party API's (e.g. Google Maps). These functional requirements are in line with the list of common functions that should exist in the domain of web frameworks [8]. Figure 1.Framework Distribution in Top Million Sites [6]. In selecting the candidate frameworks, we also considered support for the MVC architecture, popularity among the web development community, job marketability for those with particular framework skills, clearly written support documentation, as well as the steepness of the learning curve. At the time of this writing, the top six PHP-based frameworks [7] (listed in order of popularity) were the Zend Framework, the Yii Framework, CodeIgniter, CakePHP, Symfony, and PHP Rapid Application Development Objectoriented (PRADO) and are shown in Figure 2. All of these frameworks support the MVC architecture. Figure 2. PHP-based frameworks [7] We then narrowed down the top six frameworks to only two so we could perform a full comparative analysis. While Zend [9] and Yii [10] showed high promise, especially with their increasing support in the web development community and job marketability, the complexity of these frameworks and their high learning curve, led us to exclude them in this comparative study. Because we wanted students to obtain marketable framework development skills for outside employment, we looked at the top IT Job Search sites (dice.com and monster.com) and ranked the number of job postings for each of the remaining four frameworks. At the time of this writing, Symfony and PRADO fell to the bottom of the ranked job listings. Thus, the selected frameworks that were compared in this study are CodeIgniter 2.1.0 by Ellis Lab [11], Inc. and CakePHP 2.1 by Cake Software Foundation, Inc. [12]. 521 3.3 The Comparative Study To conduct the study, three versions of the same (functionallyequivalent) web application were developed by a graduate student who was adept at PHP programming but had no previous experience with web-based frameworks (PHP or otherwise). The applications were all developed using Eclipse PDT (version 3.0.2) with Eclipse Indigo [13] as the integrated development environment (IDE). This IDE provides intelligent code completion by listing all the possible related methods that allows developers to code faster and helps students to learn more about the language’s options. Additionally, this IDE provides features like modifying, compiling, deploying and debugging in the PHP perspective. It also highlights and colors the basic PHP, JavaScript and HTML syntax variables which can lead to faster application development. The first version of the web application was developed in PHP 5.3.10 from scratch without a framework and did not use any reusable code. There was also no MVC architecture involved in this version. The second and third versions of the application were developed using CakePHP 2.1 and CodeIgniter 2.1.0 both of which incorporate the MVC architecture and support reusable code. All three versions of the code were developed and run on a local WAMP server. After development of all three implementations, a quantitative and a qualitative analysis was conducted. The quantitative analysis looked at measurements of development/coding effort, performance and security. The qualitative analysis was performed based on the ease of the installation, developer friendliness (availability of documentation and online resources), and the extensibility for adding new features within the frameworks. 4. RESULTS 4.1 Quantitative Results The quantitative analysis that was conducted across the three implementations in this comparative study focused on coding effort (based on the number of lines of code), performance and security. 4.1.1 Coding effort The comparative study of coding effort involved a static analysis conducted on the number of lines of code (LOC) written to develop each version of the web application. The number of lines of code is often used to predict the amount of effort required to develop an application. Additionally, it helps estimate programming productivity and maintainability for the application. The number of lines of code for each of the three versions was analyzed using the program Count Lines of Code (CLOC) [14]. CLOC is a program that counts the comment lines, blank lines and the physical lines of source code. The LOC comparison considers only the actual code that was written by the developer of the web application. For comparison purposes, the lines of code for the framework libraries are also reported to show the amount of code included in the framework. PHP frameworks aim at achieving high code reusability and this is clearly indicated in the results. In the plain PHP implementation, the entire functionality is implemented by manually writing all the application code. PHP frameworks reuse some of the functionality that is built into the framework. The results in Table 1 show that CakePHP and CodeIgniter included libraries and helper classes that were reused in this application. Additionally, because accessing the underlying databases is done automatically through model mapping in CakePHP, CakePHP developers write considerable less code than CodeIgniter developers. This can be seen in the PHP lines of code for CakePHP (1408 LOCs with 16 files) versus CodeIgniter (2321 LOCs with 36 files). Table 1. Comparative analysis of Lines Of Code for all 3 implementations PHP CSS # of PHP files # of CSS files Plain PHP 3002 650 42 13 CodeIgniter 2321 621 36 13 CakePHP 1408 680 16 13 CI with framework 26714 990 183 14 Cake with framework 139595 1373 655 19 The results clearly indicate that when using CakePHP, the required functionality for student projects requires comparatively fewer lines of code. Fewer lines of code can lead to faster development, easier maintainability (less code to maintain) and comparatively fewer chances of software faults. 4.1.2 Performance Performance of the three implementations was measured using a PHP extension called XDebug [15]. XDebug provides debugging information such as stack traces, function traces and memory allocation, as well as profiling capabilities that can report on the time consumed for performing tasks in a PHP script. Because XDebug profiler output files are not humanly readable, they had to be passed through WinCacheGrind [16]. WinCacheGrind is an open source Windows tool used to parse XDebug profiler output files and display them in a human readable format. The performance results are shown in Table 2. As can be seen, there is a significant amount of performance overhead when using frameworks. For each functional component of the web application, the CodeIgniter and CakePHP implementations take significantly longer to complete. For example, the login and logout functions take 96ms in plain PHP whereas it takes more than 23 times that (2291ms) in CodeIgniter and 16 times that (1159ms) in CakePHP. This overhead is related to the fact that PHP frameworks need to load their built-in libraries prior to executing the required functionality of the application. Hence, for student projects, plain PHP provides better performance than PHP frameworks. Table 2. Comparative analysis of performance by time (ms) and Lines of Code (LOC) for all 3 implementations Functionality Plain PHP CodeIgniter Cake PHP Time LOC Time LOC Time LOC Login/Logout 96 121 2291 192 1559 83 Session Mgmt 91 132 739 118 1309 102 Database 143 83 762 89 1492 52 Form Handling 487 209 1338 385 2262 191 From Validation 321 292 966 406 2411 310 4.1.3 Security With respect to our security analysis, we used an all-purpose security tool as well as a specific SQL injection tool since all of the student projects have an SQL database on the backend. Thus, the security of the web applications across all of the 522 implementations was tested using Nessus [17], one of the top vulnerability scanners used in penetration testing, and sqlmap [18], an open source penetration testing tool that automates the process of detecting and exploiting SQL injection flaws and taking over of back-end database servers. On our initial security analysis, Nessus was able to fingerprint the underlying database server for all three implementations by analyzing the error messages that were returned from various Nessus probes. Additionally, as shown in Figure 3, Nessus was able to identify several vulnerabilities in the version of PHP that was installed. After upgrading to PHP 5.4.1 these vulnerabilities were cleared in all three implementations. Figure 3. Nessus report showing vulnerabilities 4.1.3.1 SQL Injection When Nessus was pointed at the plain PHP (no framework) implementation, an error from the underlying database was reported suggesting that the web application might be vulnerable to a SQL injection attack. This error was identified only with the plain PHP implementation. The same was not observed when using CakePHP or CodeIgniter as they provide built-in filtering as a security feature. We then pointed our sqlmap tool at all three applications using URL parameters as well as form parameters. Below are the results for each implementation: Plain PHP: The SQL injection attack was successful. Sqlmap was able to extract the information about the databases, tables and columns in the underlying database server. In order to fix this critical vulnerability, query binding was used. CodeIgniter: Although Nessus did not flag the CodeIgniter implementation as being potentially vulnerable to a SQL injection attack, a SQL injection attack by sqlmap was successful. Sqlmap was able to extract the information about the databases, tables and columns in the underlying database server. As was the case in the plain PHP implementation, query binding was used to fix this critical vulnerability. CakePHP: All SQL injection attacks attempted on the CakePHP implementation were unsuccessful. Further analysis discovered that CakePHP provides a built-in security component (SecurityComponent) to thwart SQL injection attacks. Additionally, this component automatically provides restriction on which HTTP methods the application accepts, cross site request forgery (CSRF) protection, form tampering protection and limits on cross controller communication. CakePHP has an additional component (AuthComponent) to handle identifying, authenticating and authorizing users which is also appears to be resilient to SQL injection attacks. 4.1.3.2 Cross site scripting Nessus also has the capability to check if a web application is vulnerable to potential cross-site scripting (XSS) attacks. On the plain PHP version, Nessus was able to inject innocuous strings into passed parameters and read them back in the HTTP response. The ability to do this means that a site is potentially vulnerable to XSS attacks. This can be remedied by neutralizing invalid characters in the input URL parameters. When the same Nessus attack was used against the CakePHP and CodeIgniter versions, it failed. This is because CakePHP and CodeIgniter provide built-in security features like the Security component in CakePHP and the filtering capability for cross site scripting which can be enabled in CodeIgniter’s config.php file. 4.1.3.3 Security summary Based on our security findings, it appears that CakePHP is more secure than the implementations in plain PHP and CodeIgniter. CakePHP provides built-in security components that can prevent many common security attacks. While CodeIgniter provides similar security features (CSRF protection and cross site scripting filtering), our results show that CodeIgniter, with its default configuration, failed to handle common SQL injection attacks. If a framework other than CakePHP is chosen for the course, query binding needs to be discussed to alleviate the possibility of SQL injection attacks. 4.2 Qualitative Results While CakePHP appeared to be the framework of choice with respect to development effort, security and performance, it loses favor in the qualitative analysis that was performed on the ease of installation, developer friendliness (based on the availability of documentation and online resources) and ease of extensibility for adding new features. These results were carefully reviewed to determine the best framework for a course that is typically enrolled with framework newbies. In an attempt to remove subjectivity from our analysis, we used the document analysis qualitative research method [19] [20]. Our data collection was focused on http://php.net for plain PHP, http://codeigniter.com/forums/ for CodeIgniter and http://www.cakephpforum.net for CakePHP. Posts in the online forums regarding each of the qualitative measures were collected, analyzed and categorized into High, Medium and Low. Our assumption with respect to categorization was that the more posts that were found related to a particular qualitative measure, then the higher the ranking. For example, with respect to extensibility, there were 83 posts related to extensibility in the plain PHP forums, 35 posts in the CodeIgniter forums and 42 posts in the CakePHP forums. In this example, plain PHP would be given a High ranking indicating a high level of extensibility relative to CodeIgniter (Medium) and CakePHP (Low). Table 3 shows a summary of all of the qualitative measures and their results across all three implementations. Table 3. Summary of qualitative analysis results Quality Measure Plain PHP CodeIgniter Cake PHP Ease of installation High Medium Medium Built‐in CAPTCHA support None Yes None Built‐in support for security None Medium High Extensibility High Medium Low Documentation/Community Support High High Medium Learning Curve Low Medium High Developer friendliness Medium High Low Development Flexibility High High Medium 523 5. CONCLUSION Although plain PHP has certain advantages in areas of performance and flexibility, the development cycle in plain PHP is lengthy as all of the features of the application need to be coded from scratch. Manually coding common web application functions such as session management and authentication can lead to significant security vulnerabilities if not done correctly. Code reuse is essential for these types of critical functions to ensure the development of robust applications. Additionally, development without a framework does not enforce an MVC architecture that is critical in web design. While CakePHP showed initial promise in the quantitative analysis of our study, when qualitative measures were considered, it lost favor. Indeed, the built-in features available in CakePHP libraries are extensive, but the usage of all those features for student projects may not be applicable. Additionally, CakePHP is comparatively inflexible in terms of naming conventions and folder organization and has a much higher learning curve particularly due to its lack of strong development community at the time of this writing. And finally, the job marketability for those with skills in CakePHP is not near as high as the number of positions that are advertised for skills in CodeIgniter. Based on our qualitative and quantitative results, we believe that CodeIgniter is best suited for student projects. Although CodeIgniter and CakePHP support many of the same functionalities, the simplicity of CodeIgniter makes it preferable. Additionally, CodeIgniter has advantages in areas of ease of use, quick development, support for components like CAPTCHA, community support, very low learning curve and built in support for security. As previously stated, this work represents the third phase of a four-phased process to revamp our undergraduate software engineering course. Now that an appropriate PHP framework has been selected, it will be incorporated into the next offering of CSC330 (Spring 2013) and assessment of the course will continue.App Development in User Services: Oxymoron or Incubator? Lisa Barnett Director, User Services 40 Washington Sq South, Suite B7 New York, NY 10012 +1 (212) 992-8923 lisa.barnett@nyu.edu Darin Phelps Director, Technology Development 40 Washington Sq South, Suite B9 New York, NY 10012 +1 (212) 998-6138 darin.phelps@nyu.edu Brian Yulke Associate Director, User Services 40 Washington Sq South, Suite 211 New York, NY 10012 +1 (212) 998-6283 brian.yulke@nyu.edu ABSTRACT User Services attracts all types. This talent develops differently, and opportunity can knock on many doors. At New York University (NYU) School of Law, we had a few Helpdesk technicians who took a web development class. When our new exam software came without robust administrative/management tools, opportunity did knock. We started off in PHP, scarfed some data from our database guys, and cobbled together a working web app in about 6 weeks. It wasn't pretty, but it got the job done. One year later, our next iteration implemented a framework (Zend), version control (Git), and staged environments (development/testing/production). Yet another year later, we added new features, satisfying both administrative and student clients. Today we have a small, versatile team that handles lightweight projects, and fits them into a Helpdesk schedule to accommodate an ever-growing demand for new web applications. Remarkably, we do this all without sacrificing our core mission of providing top-notch user support. While it does require better time management, it’s amazing to see how flexible your Helpdesk can be with their time. Coming from the user services group, we have a solid relationship with the end users and consequently understand their needs better than traditional programmers. Working with the core development team allowed our Helpdesk programmers to learn faster, and helped the real code-monkeys understand valuable user support fundamentals, which makes them better, too! Finally, this created professional development opportunities within the organization. While we found equilibrium in User Services, you could also have internal moves within your larger organization. Either way, you preserve organizational knowledge and make the entire IT department stronger. While this kind of professional development opportunity may not be for everyone in your group, you can reward individuals who take initiative, have an interest in learning new technologies, and show strong timemanagement skills. Categories and Subject Descriptors •Software and its engineering~Programming teams General Terms Management, Professional Development Keywords Professional development, programming, career opportunities 1. INTRODUCTION In client services, all too often our rising stars rise faster than we like and move on all too quickly. But what if you could leverage this talent and ambition? What if you could reward success with tangible professional development and give them a reason to stay longer and grow here? What if you could foster their curiosity, the desire for something more, and create a nurturing environment to stretch their skillsets beyond those expected of a productive Helpdesk member? How can we create this opportunity and not adversely affect the quality of work at the Helpdesk? In technology, Helpdesk is a great career launching pad. Where else can you develop customer service skills, get in touch with client needs, and gain exposure to every nook and cranny that your user base has questions or problems with? Those with a mix of tech savvy, leadership, operations improvement, and good communications skills may ultimately seek out management roles in client services. Others with heavier technical prowess can still cut their teeth en route to more focused positions in a systems, networking, or programming job. The road through Helpdesk is paved with the footprints of both recent graduates and others embarking on a 2nd (or 3rd!) career. Many are eager and willing to take additional responsibilities and projects to stretch their skills and grow their resumes. NYU School of Law is a relatively small school within New York University. Our Helpdesk supports around 2,200 students and 800 FTE. While NYU has a large central ITS organization, all of the professional schools and many of the other schools within NYU also have their own IT staffs that have a dedicated focus on the individual schools’ needs. As such, NYU School of Law has an ITS Department with 35 fulltime members, 8 of whom are dedicated Helpdesk staff (including training/documentation and management). Our Helpdesk does not target a particular part of the Law School community; rather, it supports all students (who do have a laptop requirement), faculty, administrators, and staff. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGUCCS’13, November 3–8, 2013, Chicago, IL, USA. Copyright © 2013 ACM 978-1-4503-2318-5/13/11…$15.00. 27 The Law School’s ITS organization also has its own Technology Development Office of very busy programmers. The list of important projects that they focus on is long and there isn’t any excess capacity within this technical team to work on short-term, smaller-scaled new challenges that require immediate attention. So, one challenge to the organization was how to create space for an agile team of programmers who can focus on some of these quick wins. Two frontline staff and the leadership in Helpdesk are now handling these specific programming projects, the first of which is discussed here. 2. INITIATIVE The land of Academia is a place of learning and exploration, of discoveries both in the world around us and within us. There exists a degree of freedom often not found in the corporate world. It attracts people not with high salaries and corner offices, but with opportunities for learning and growth. We are all here with a common goal in mind: to create an environment to foster learning. This shouldn’t be limited just to students and faculty, but expanded to encompass everyone, to give anyone who wants it a chance for growth. 2.1 Putting in the Effort Academic institutions can have a wealth of knowledge and offer many avenues for learning. Many schools offer tuition benefits in the form of discounted classes and degrees. Though tuition discounts may not be available everywhere, there is often the possibility to audit classes with no more effort than a letter to the professor. Academic libraries allow access to books, journals, and databases, and most have some form of inter-library loan system connected with numerous other institutions. The Internet is full of the collaborative efforts of so many others from all over the world: Khan Academy, Tuts+, Massive Open Online Courses (MOOCs) to name a few. These resources are sitting there just begging to be tapped. The decision to explore them is up to us. It is important that our department members know what is available to them if they seek to expand their skills on their own. 2.2 Recognizing the Work Interest and/or hard work should not go unnoticed or unacknowledged. Taking notice of the interests and efforts of your peers and direct reports is key. In this instance, two of the Helpdesk staff members had recently finished up some programming classes. They were eager to share what their class projects covered and their enthusiasm was obvious to the leadership of the Law School’s ITS. Additionally, the Associate Director of User Services who oversees the Helpdesk had enrolled in a combined Master’s program between the business school and the computer science department. Here was an interesting subset of people with expanded skills who were ripe for the opportunity to meet a new challenge. It is a credit to the Law School’s CIO that he was willing to give these tech-newbies a chance to try out their new skills in the context of a real new need the Law School faced. 3. OPPORTUNITY Some background: Most schools typically include regular homework assignments, projects, class participation, and other factors as part of their evaluation. Law schools have a somewhat unique exam pedagogy; there is typically a single final exam on which the entire course grade is based. Additionally blind grading is instituted, where anonymous ids are used in place of student names or ids. This creates both a higher-stakes environment for final exams, and additional complications in administering them. Opportunity arose when the decision was made to switch to a new exam software vendor. This new software brought with it new levels of reliability and usability, but lacked an administrative backend that was desperately needed, as NYU Law is one of the larger law schools in the country. We were left with the prospect of receiving nearly 7,000 exam response files each semester and no way to automatically or easily track who uploaded what, when, or where. We needed an administrative companion to the software, but none existed. The vendor didn’t offer one and there were no 3rd party solutions. A solution needed to be provided, and quickly. By the time the decision to switch was made, contracts negotiated, software tested, etc. the start of the exam period was less than two months away. NYU Law has a committed team for developing internal applications, so why not use them? They were already engaged with other projects, and the timeline we were looking at was aggressive. There was no way they could produce what we needed in so little time. What about an outside vendor? Someone would have to write a functional spec, negotiate price and delivery time, find money in the budget, test, and deploy. Our experience with vendors told us they would have taken too long, cost too much, and our only success would be in brokering a product that only met a semblance of what our needs were. The outlook was bleak. There were visions of text files and Excel spreadsheets filled with lists of student course registrations and extensive formulae for manipulating exam filenames into anonymous grading IDs and course names. A task nobody would look forward to, fraught with clerical error, peril, and certain irreversible ocular damage! Helpdesk was destined to share this pain with Academic Services. But what choice did we have? Where else could we turn? Enter our two Helpdesk technicians: they had shown interest in projects outside the scope of user support, had taken initiative on other Helpdesk projects, and conveniently taken a recent webdevelopment course. While it seemed like a monumental task (and while there would be no hard feelings if the mission were not accepted, or completed on time), it seemed overly convenient. We considered:  Helpdesk had been supporting exams for years, since the first semester that students started taking them on laptops. Our technicians were present in exam rooms prior to the start of each exam, and back in the office assisting Academic Services.  ITS assisted the Academic Services with the upload of data into similar management tools for previously used products; we knew the raw material that fueled such an application.  Since Helpdesk assisted students in uploading exams when they had technical issues, and helped them track their exam file uploads, we ended up being just as much end-users as either the students or the administration.  Consequently we knew better than anyone what such an application would need to do. We may not have realized it all at once, but we were perfectly poised to try writing this application. What we lacked in development experience, we made up for in a combination of domain knowledge and rookie confidence! 28 4. THE FIRST PROJECT As our pioneering effort, we certainly stumbled along the way. Some valuable lessons were learned, and some wheels were likely reinvented. After some time we realized the following:  Resources are cheap! In a Helpdesk, there are always old machines lying around not good enough for deployment, but certainly good enough for a skunkworks project!  VMs are even cheaper! That’s right; they run on the hardware you’re already using.  Open source is also cheap! Not only that, but the documentation and online support communities are usually at least as helpful as their pricy commercial counterparts. While this may not be universally true of all open source products, those that were used (Linux, Apache, MySQL and PHP) are exemplars of the opensource community that are as popular as commercial alternatives (or even more so). Since we weren’t part of an established development team, we weren’t fighting with established standards. Anything could have been used, even a new technology or programming language learned. Mercifully the choice was made easy by what was known: PHP, HTML, and MySQL. 4.1 Developing Development A few choices had to be made and much work had to be done before even the first line of code could be written. The team had to create their own development environment capable of handling multiple contributors to the project. This manifested in the form of a Debian Linux-based server built on an old desktop PC and a process of moving code from a central file share using sFTP. Within a day or two of fumbling and searching Google, a development server was up and running and coding could begin. We started collaboration with very low-tech protocols: word-ofmouth across the room to make sure we weren’t working on the same files. Copying latest versions to a test server, and crossing our fingers we didn’t break anything. This first iteration was far from perfect and would often lead to the same file being edited by two people at once. Now we’re using a distributed version control system, Github, to manage our code, and to keep different branches for development, testing, and production. 4.2 Wrangling the Data What was being developed was, in essence, a reporting application. The problem with reporting applications is that they aren’t worth much without actual data. In this case there were two main sources of data. The Student Information System contained names, IDs, courses, and registrations. An exam file receiver was needed for exam upload information such as creation and upload dates, course identification, and anonymous grading IDs. We approached our database folks, who were appropriately skeptical but also curious. They had been given a polite nudge from management to support this new team and the project. It was important that the CIO gave the database folks a heads-up, as we wanted this project to succeed and not set up the nascent team to be viewed as a threat to anyone in the department’s development team. Ultimately, they were able to help us refine our suggested database schema and supply needed data. Getting real, live data was invaluable. Skeleton code with madeup test cases will get you only so far. With real data we finally got to see the application in action and how the ‘diversity’ (or less politely, the lack of quality) of real-world data can break what seemed very straightforward code (e.g., names with international characters, or course names with odd punctuation). 4.3 Testing in Production The Helpdesk team was able to leverage their domain knowledge of the exam process while testing the application, possibly to a greater extent than traditional developers. The real gold was having both Academic Services and Records and Registration (our clients) test the application with us and provide candid feedback that was built on our already-established working relationship. 4.4 Happy Customers Imagine our delight when the application worked well for all users during the first exam period that we used it. Academic Services and Records and Registration were very happy with the launch. Exams were administered and tracked successfully using the new exam software. Additional functionality and refinements were added to the application over time. Students now have a portal to find information about their exams, including exam schedules, room locations and anonymous exam ids. The latest feature addition includes a workflow for exam postponement request/approval/rescheduling, which continues to improve customer experience and reduce administrative overhead. 5. RESULTS The three Helpdesk employees involved were able to successfully implement a new web application providing value to an array of customers. They were able to do so without compromising the service mission of Helpdesk by remaining efficient in their tasks and organizing their time well enough to tend to both programming tasks and Helpdesk support. In addition to gruntling customers, the success of the project both strengthened the relationship between User Services and Academic Services, and fostered a new relationship between User Services and the ITS development team. The latter has shown additional benefits through more collaboration between the two groups, and better overall communication and understanding of projects the department is involved in. We realized an overall net gain from the successful completion of ExamReporter. Even though we spent considerable time developing it, 5 separate departments continue to benefit from its functionality each and every exam period. Perhaps even more importantly, we have been able to help provide professional development opportunities within the organization for those involved. They have been given experience in programming a detailed web application, and in the setup and maintenance of the infrastructure involved. Because of the success of ExamReporter, the team has been given other projects that fit within the scope of their availability and helped continue their growth as programmers. The two frontline Helpdesk personnel involved in this programming project have been promoted to Information Technology Specialists. They are still responsible for some Helpdesk work, but they now also work on a variety of programming projects. Their time-management skills have 29 improved, as they continue to contribute to Helpdesk and work on projects. The Helpdesk manager was promoted from Associate Director of User Services to Lead Solutions Architect and now successfully leads a hybrid team at the Helpdesk that works on a handful of programming projects in addition to providing front-line client support. Going forward, we will try to identify candidates with such inclinations who might be able to benefit from similar professional development opportunities. We want to foster a broader User Services culture moving forward; one where a Helpdesk developer is not viewed as an oxymoron, and can be looked to as an incubator of ideas and talent. 6. CONCLUSIONS By taking advantage of interested Helpdesk staff for a needed project, we were able to successfully meet client needs, promote career development and improve the overall quality of service from the department. Helpdesk staff should stay active in their education, looking for opportunities to learn, and also for opportunities to use their newfound knowledge. Management should be supportive of any projects that may help foster growth and learning, even if off the beaten path. When opportunities arise, make sure to be supportive. Also take advantage of inexpensive resources; they may range from recyclable hardware to open source software to supportive colleagues in other areas of the department. Your Helpdesk environment need not suffer a drop in service. Indeed, you may see increased productivity as employees are further engaged by a new opportunity at work, and you may find you can provide more and better service from your organization. 7. ACKNOWLEDGMENTS Special thanks to our CIO, Tolga Ergunay, for having faith in us, and taking a keen interest in our career development. 30 Recovery Guarantees for Internet Applications ROGER BARGA and DAVID LOMET Microsoft Research and GERMAN SHEGALOV and GERHARD WEIKUM Max-Planck-Institut f¨ur Informatik Internet-based e-services require application developers to deal explicitly with failures of the underlying software components, for example web servers, servlets, browser sessions, and so forth. This complicates application programming, and may expose failures to end users. This paper presents a framework for an application-independent infrastructure that provides recovery guarantees and masks almost all system failures, thus relieving the application programmer from having to deal with these failures—by making applications “stateless.” The main concept is an interaction contract between two components regarding message and state preservation. The framework provides comprehensive recovery encompassing data, messages, and the states of application components. We describe techniques to reduce logging cost, allow effective log truncation, and permit independent recovery for critical components. We illustrate the framework’s utility via web-based e-services scenarios. Its feasibility is demonstrated by our prototype implementation of interaction contracts based on the Apache web server and the PHP servlet engine. Finally, we discuss industrial relevance for middleware architectures such as .Net or J2EE. Categories and Subject Descriptors: C.2.4 [Computer-Communication Networks]: Distributed Systems—Distributed applications; C.4 [Performance of Systems]—Fault tolerance, Reliability, availability, and serviceability; H.2.4 [Database Management]: Systems—Transaction processing General Terms: Design, Management, Performance, Reliability Additional Key Words and Phrases: Exactly-once execution, application recovery, interaction contracts, communication protocols 1. INTRODUCTION 1.1 Motivation and Problem Statement Internet-based e-services, ranging from B2C (business-to-consumer) ecommerce and B2B (business-to-business) supply chains to advanced services Authors’ addresses: R. Barga and D. Lomet, Microsoft Research, One Microsoft Way, Redmond, WA 98052; email: {barga,lomet}@microsoft.com; G. Shegalov and G. Weikum, Max-Planck-Institut fur¨ Informatik, Stuhlsatzenhausweg 85, Saarbruken, FRG; email: ¨ {shegalov,weikum}@mpi-sb.mpg.de. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 1515 Broadway, New York, NY 10036 USA, fax: +1 (212) 869-0481, or permissions@acm.org. C 2004 ACM 1533-5399/04/0800-0289 $5.00 ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004, Pages 289–328. 290 • R. Barga et al. such as web-based auctions, tele-teaching, collaborative authoring, or electronic tax preparation, require a sophisticated software infrastructure that includes user browsers, web application servers with servlet engines, workflow engines, and backend database servers [Debull 2001]. Since all of these components are failure-prone, application developers have to take measures for dealing with failures. Database recovery does not mask failures to applications and users. Transaction atomicity guarantees all-or-nothing but not exactly-once execution, where the latter means 1) no output to the user is duplicated (to avoid confusion), 2) the user provides input only once (to avoid irritation), and 3) the user intent, for example, buying airline tickets, is carried out exactly once. To achieve this exactly-once behavior, application programs need explicit code for retrying failed transactions. Often such code is incomplete or missing, exposing failures to users. Or even worse, a failure occurs with no notice provided. For an e-commerce service, for example, this can lead to user inconvenience and lost sales when this happens during shopping cart checkout. However, the application program or user must not blindly re-initiate a transaction when no result is received, as the transaction may have succeeded and re-execution is not usually idempotent. Some e-services warn users not to hit the checkout/buy/commit button twice when a long delay occurs. Users who do not heed this warning may unintentionally purchase two seats on the same flight or two copies of the same book. The current approach to coping with these problems is to combine explicit failure handling code with the notion of stateless applications. This puts a major burden on the application programmer, and generally hampers application development productivity and maintainability. Rather it is much more desirable to factor failure handling out of the applications and provide a generic solution for “universal recovery” in the surrounding run-time infrastructure. This way application programs could be written as if there were no failures in the Internet and the components of an e-service. TP monitors, exploiting transactional message queues or a database, and “stateless” applications, have long been the preferred solution for failureresilient business applications. Stateless applications are not without state, however. Rather, they are written so that each application step executes entirely within a transaction. A step begins by retrieving prior state from a queue or database, executes, and then stores the resulting end state into a queue or database. Thus, such an application does not have “control state” outside of a transaction, though of course, the state saved in the queue or database lives across transaction boundaries. With the advent of web-based e-services, however, prior solutions have not been fully carried over to multitier web architectures. Indeed, how they might be adapted to deal with middle-tier web application servers (e.g., Apache/Tomcat or IIS) is complex and still speculative. Some e-services are even more complex, with layers of application servers (e.g., web server, workflow server, activity server) accessing several database servers but also directly maintaining persistent data in files and requesting services from external providers. For example, the Expedia travel service integrates services from travel industry providers such as Amadeus or Sabre. Another example is one-stop shopping services that ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 291 provide a unified view of a diversity of individual e-commerce sites. A somewhat futuristic e-service of this kind would be an integrated real-estate purchase service that automatically handles the entire workflow for the necessary steps with notaries, land registry offices, banks, insurances, and so on. For multitier Internet applications with communicating components, a comprehensive form of data, component state, and message recovery is needed, going beyond traditional database recovery. Designing protocols to accomplish this entails a number of issues:  To what extent can failures be masked to the end users, and which logging actions are necessary to this end?  Which component logs which messages or state to be recoverable, mask failures, and provide exactly-once semantics to a user?  How are logs managed, when is a log force written to disk, and how are logs coordinated for log truncation, crucial for fast restart and thus for high availability?  How are critical components, for example, database servers, kept from being “hostage” to other components (applications servers, clients) that may hamper or block their independent recovery or normal operation? 1.2 Our Framework for Recovery Guarantees We have developed a framework for multitier applications in the context of the Phoenix project [MSR PHOENIX] that answers the above questions [Barga et al. 2002]. It masks from users all failures of clients, application servers, or data servers (e.g. database servers) such that all user requests can have exactly-once semantics (see Subsection 1.1). We identify the logging required for nondeterministic events so that after a failure, an application component can be replayed from an earlier installed state (in an extreme case its initial state) and arrive at the same (abstract) state as in its prefailure incarnation. Our framework exploits interaction contracts between two components. For example, a committed interaction contract between persistent components requires sender and receiver guarantees to ensure that the interaction persists at both components across system failures. There are also contracts for persistent component interactions with external components (including users) and transactional components, which provide all-or-nothing state transitions (but not exactly-once executions). The bilateral contracts are composed to make persistent components provably recoverable with exactly-once execution semantics. We separate contract obligations from their implementation in terms of logging. As a result, many internal interactions can avoid forced logging or other expensive measures. We present implementation techniques to a) minimize logging cost, especially forcing the log to disk, b) allow effective log truncation to limit restart cost and thus provide high availability, and c) permit independent recovery of certain components. We have built a prototype implementation of our framework as a proof of concept. Our prototype directly implements interaction contracts for an ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 292 • R. Barga et al. Fig. 1. Components in a four-tier electronic travel service. application server based on Apache and PHP, via changes to the PHP interpreter. It provides browser extensions to enable browser session state to also be included in our framework, thus extending our recovery guarantees to the client desktop. The result is a system that can provide an end-to-end exactly-once execution guarantee, successfully recovering from and masking system failures from the user. And it achieves this without the application programmer needing to take any explicit programming steps to deal with failures. 1.3 Sample Scenario We illustrate here how our framework deals with an advanced multitier e-services scenario, the travel services provided by companies such as Expedia.com and Travelocity.com. The system architecture, illustrated in Figure 1, can be characterized as a four-tier system with clients using Internet browsers, two tiers of application servers in the middle, and a suite of backend database servers. A client sends a travel request to an upper-tier Expedia web server. The client, whose state is extended via cookies or applets for personalization (e.g., frequent flyer numbers, etc., which could be stored locally at a client-side database or file server), sends the request and related personalization information to the Expedia web server. The web server runs workflow-style servlets in sessions on behalf of client requests. This level hosts business logic and is in charge of building and maintaining tentative itineraries for users’ travel plans. To this end, it keeps user state that spans conversational interactions with the client for the duration of a user session, typically using session objects whose only job is to hold shared data on the web server. For querying flight fares, hotel rates and availability, and so on, the web server interacts with lower-tier application servers. These include servers operated by autonomous travel companies with their own application servers and database servers, for example, Amadeus and Sabre. One lower-tier application server is an integrated part of ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 293 Expedia, again a server running servlets that communicate with a database for long-term information about customers. In this scenario, client, Expedia web server, application server, Amadeus and Sabre application servers, and database servers all support sessions that retain state. These sessions are established as a result of user initiation. While servers are typically multi-threaded, the sessions they support are single-threaded. Thus, all nondeterminism (outside of data accesses within database servers, which is captured by database logging) resides in the interactions between sessions. Our contracts are targeted precisely at these interactions. Each component in this picture provides guarantees to the other components via our framework. This permits them all to be jointly recoverable should a system failure interrupt execution of a user request. And, importantly, this recovery ensures that a user will see exactly-once execution of his request. We return to this application scenario in Subsection 5.4, showing the excellent performance of the framework in providing its guarantees. The rest of the article is organized as follows. Section 2 provides background on transactional recovery, as used by database systems and TP monitors, and discusses related work. Section 3 introduces the system architecture and computational model that underlie our work, and it provides an overview of the key concepts of our approach. Section 4 introduces the fundamental notion of interaction contracts between two components and their four flavors, and discusses its ramifications. Section 5 elaborates the implementation techniques for realizing interaction contracts, and provides a performance analysis for our sample application scenario. Section 6 describes our prototype implementation of interaction contracts for an application server based on the Apache web server and the PHP servlet engine. To emphasize the industrial relevance of this work we discuss, in Section 7, its applicability to widely used middleware architectures such as .Net or J2EE. Section 8 provides our conclusions. 2. BACKGROUND AND RELATED WORK 2.1 Recovery Basics In database systems and TP (transaction processing) monitors, which form the backbone of most mission-critical business applications (both traditional clientserver as well as Internet-based multi-tier), recovery after hardware and software failures is based on the fundamental notion of an atomic transaction [Gray and Reuter 1993, Weikum and Vossen 2001]. Transactions guarantee all-or-nothing atomicity in the sense that a failure in the middle of an ongoing transaction does not leave any traces and persistence in the sense that the effects of a committed (i.e., successfully completed) transaction will survive future failures. These system-provided guarantees simplify the task for application developers whose code simply needs to specify the begin and end of a transaction during program execution. Atomicity and persistence are usually implemented by logging data modifications on stable storage, typically, one or more dedicated disks. For effi- ciency, database updates in the server’s database page cache are not written ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 294 • R. Barga et al. to disk immediately; rather log entries are generated that contain sufficient information for either undoing a modification, if the transaction aborts or a failure occurs before the corresponding transaction’s commit, or redoing it, if the transaction commits and a failure occurs afterwards. The implementation may choose to record the state of a data object before and after the modification, or may merely record the change to the object in a log entry. Care must be taken to avoid excessive disk I/O for writing log entries onto disk. To this end, log entries are first collected in a log buffer and the log buffer is written to disk only when it is necessary for recoverability (e.g., for a transaction’s commit log entry). When correctness requires that a log entry be written, it is referred to as a forced log write, otherwise as a non-forced log write. During recovery after a failure, log entries are sequentially read from the stable log file. In a redo pass, all updates that are not reflected in database pages on the disk are redone by replaying log operations. Then an undo pass considers all log entries that belong to incomplete transactions and removes their effects. These procedures are complicated by the fact that, when reading a log entry, the recovery method does not know whether this update is already contained in the stable database. This uncertainty is a problem because updates and the corresponding redo/undo steps are not necessarily idempotent; redoing an update that is already in the database or redoing a lost update twice may lead to incorrect data. To handle these situations, the recovery method implants log sequence numbers (LSNs) in both the log entries and the data objects to be recovered. The LSN of an object (e.g., in the header of a disk page) tracks the state of the object and provides testable state to the recovery method: a simple comparison of the LSNs in a log entry and the corresponding data object tells us whether the log entry’s operation should be executed or not. For fast restart (and hence high availability) the recovery system limits the number of log entries to be read from the stable log file and the number of data objects to be fetched from disk during recovery. Database systems periodically truncate the log based on bookkeeping that identifies which pages may require recovery (as opposed to those that are known to be up-to-date in the disk-resident database). Some of this bookkeeping information is periodically written asynchronously to the log in a checkpoint. This does not require writing any cached data pages onto disk, rather it is combined with a background process that writes back modified pages in an asynchronous and I/O-optimized manner. This method allows continuous log truncation, a particularly effective way of garbage collecting unneeded log entries. Figure 2 depicts the key components that underlie an efficient data recovery method as discussed above. In the figure b, q, and z denote database pages, t17, t19, and t20 are transactions, and the other numbers are LSNs. 2.2 Application Recovery The database recovery approaches outlined above provide an efficient, industrial-strength solution for many systems requiring data recovery: file systems, mail servers, and so on. However, they do not immediately carry over to recovering applications after failures. In an Internet-based three-tier ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 295 Fig. 2. State-of-the-art data recovery. (or a simpler client-server) application, with an Internet browser as a client, an HTTP server and a servlet engine at the middle tier, and a database system as backend, database recovery only ensures the consistency of the data at the backend. It is then up to the application programs running at client and middle-tier server to test and handle return codes that may signal a transient failure (e.g., a code like ”unknown transaction” from the database system), to reinitiate requests, and, most critically, to ensure idempotence by implementing some form of testable state. To simplify the implementation of robust applications requires that failures be masked from application programs (and also from end users). This requires a more comprehensive approach to overall application recovery. The most successful technique for application recovery in mission-critical situations uses queued transactions, invented for OLTP (online transaction processing) [Bernstein et al. B. 1990, Gray and Reuter 1993, Bernstein and Newcomer 1996, Weikum and Vossen 2001] and supported by most TP monitors (e.g., IBM MQ Series, BEA Tuxedo, Microsoft MTS) and associated web application servers (e.g., IBM WebSphere, BEA WebLogic, Microsoft IIS). Messages between clients, the TP monitor acting as an application server, and the database server are held in transactional message queues whose operations, enqueuing or dequeuing messages, are embedded in an atomic transaction with its all-or-nothing guarantee. The queue manager usually is a separate resource manager with its own log and needs to support the two-phase commit protocol for distributed transactions that access both messages in queues and permanent databases. An application program can then read an input message from a queue, process the message by querying and updating one or more databases, and finally place a reply message into a queue—all in one atomic transaction. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 296 • R. Barga et al. Fig. 3. State-of-the-art message and process recovery based on transactional queues. Should the database transaction abort, the message that was originally dequeued from the input queue is automatically placed back into the queue (based on log entries that undo the transaction), so that the input message will eventually be processed even after an arbitrary number of failures. Further, the message will be processed exactly once (for the commit of the transaction also commits the dequeueing of the input message and the enqueuing of the output message). This principle is illustrated in Figure 3. While queued transactions have the desired exactly-once execution guarantee, they also have severe disadvantages that make them unattractive as a general-purpose solution. The queue-based message interaction between client and e-service involves three transactions per user request: to enqueue the request on the queue; to dequeue it, process it in the database server, and enqueue the reply; and to dequeue the reply. This incurs the high overhead of forced-logging for three commits. Even more importantly, queued transactions require “stateless” applications where the only application state between transactions is in a database or queue. For a multi-step application “session”, all information that needs to be kept across session steps is either kept in the database or encoded in the messages that are sent back and forth between client and application. Both options are expensive in terms of disk I/O. The second option resembles the web programming technique of maintaining session-related information in cookies, except that cookies are a much less reliable intermediate store than transactional queues. The queued transaction approach, with its shortcomings, may be acceptable for some e-commerce services but it can pose substantial difficulties. It requires significant programming effort and thus high costs to cast rich stateful applications into this stateless paradigm. There is some prior work on masking failures and providing recovery for stateful applications, but only in limited contexts. The papers Freytag et al. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 297 [1987], Lomet and Weikum [1998], Barga et al. [2000] are focused only on clientserver systems, and do not consider multi-tier Internet applications. Other work is restricted to applications embedded in the database server, like stored procedures [Lomet 1998]. It is not obvious how these protocols can be generalized to apply to multi-tier systems. The notion of interaction contracts, developed in the current paper, is the key for this generalization. Recovery for general systems of communicating processes has been extensively studied in the fault-tolerance community (e.g., Johnson and Zwaenepoel [1987], Cristian [1991], Alvisi and Marzullo [1995], Elnozahy et al. [2002]), where the main focus has been to avoid losing too much work in a long-running computations (e.g., scientific applications), usually using distributed checkpointing. Most of this work does not mask failures. Methods that do mask failures exploit “pessimistic logging” (see, e.g., Huang and Wang [1995]), with forced log I/Os at both sender and receiver on every message exchange. More expensive techniques, such as process checkpointing (i.e., writing process state to disk) upon every interaction, were used in the fault-tolerant systems of the early eighties [Bartlett 1981, Borg et al. 1989, Kim 1984]. So failure masking has been considered a luxury affordable only by mission-critical applications (e.g., stock exchanges). Fault tolerance is also being discussed for component middleware like CORBA [OMG: CORBA 2000] and EJB [SUN 2001], but the focus is on service availability for stateless interactions (i.e., restarting re-initialized application server processes). Products (e.g., VisiBroker, Orbix, BEA WebLogic, or Sun’s J2EE suite) at best support simple failover techniques that do not relieve the application programmer from having to either write failure handling logic or structure his application as “stateless,” and are not geared for masking process or message failures to users. More recently, failover techniques for web servers have been presented in Luo and Yang [2001], based on application-transparent replication and redirection of HTTP requests. The need for execution guarantees for e-services, raised in Tygar [1998]; Pedregal-Martin and Ramamritham [1999, 2001]; Dutta et al. [2001]; Fu et al. [2001]; Popovici et al. [2000]; Schuldt et al. [2000]), has been concerned with specific applications such as payment protocols or mobile data exchange and does not specifically address failure masking in general multi-tier architectures. Closest to our approach in terms of objectives is the work in Frølund and Guerraoui [2000] that presents a multi-tier protocol for exactly-once transaction execution based on asynchronous message replication and a distributed consensus protocol. However, this work focuses on stateless application servers and does not address the autonomy requirements of components, the optimization of logging, and the need for effective log truncation. The recovery framework and protocols we introduce improve the state of the art in a number of ways. Compared to traditional techniques based on pessimistic logging or frequent process state saving, our protocols reduce logging and state saving costs while providing very fast recovery. In contrast to solutions provided by TP monitors and CORBA- or EJB-oriented application servers, our approach handles stateful applications, removing the application programmer burden of making his application stateless. In addition, our method is unique in ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 298 • R. Barga et al. Fig. 4. Multi-tier system architecture and component interactions. providing an end-to-end solution for masking failures across an entire multi-tier federation of application and data servers while, to a large extent, preserving the autonomy of these servers. 3. SYSTEM ARCHITECTURE AND KEY CONCEPTS 3.1 Components We consider a group of interacting components: clients, application servers, data servers, and users (viewed as components). In a typical scenario a client (e.g., an Internet browser) submits a request on behalf of a user to a mid-tier application server (e.g., HTTP server, workflow server, or web service broker) that processes the request and generates further requests to other application servers or backend data servers. When the mid-tier server has received all necessary information from its subsidiary servers (including return messages for data updates that it may have initiated), it sends a reply message to the client, which in turn displays the result to the user. This either concludes the entire round-trip of a stateless interaction or is continued with another client request in a stateful conversational session (e.g., an extended workflow). The system architecture and the message exchange between the various components are illustrated in Figure 4 (for the time being ignore all events with non-integral numbers, they will be explained later). In the figure, application servers 1 and 2 are assumed to belong to the same domain and thus consider each other as trustworthy, whereas server 3 merely belongs to the same loosely coupled federation. We require that components be piecewise deterministic (PWD). A PWD component is deterministic between successive messages from other components. Thus it can be deterministically replayed from an earlier state by resending it the original messages, producing the same component end state. Replay starts from a previous component state on disk, perhaps the initial state. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 299 We call saved states “installation points” (IPs), not checkpoints, as they are often called in the fault-tolerant computing field, to avoid confusion with the different notion of checkpoint used in database recovery (see Section 2.1). What constitutes component state varies greatly; sometimes a compact abstract state is sufficient rather than the full image of a component’s address space. Usually, server state includes persistent data (e.g., files or a database), message buffers, and information about active sessions. After a failure, to replay a component from a previous installation point, the component may need access to a stable log with log entries that contain sufficient information to recreate all incoming messages. Components are mapped to processes or threads of the underlying runtime system (e.g., operating system or Java VM). Thus we can model a multithreaded server as a set of single-threaded components interacting with each other via shared data or messages. Alternatively, a server might be treated as a single component, but then we need to pay specific attention to the potential nondeterminism that may arise from thread interference on shared data. 3.2 Interactions, Contracts, and Logging We assume that component failures are 1) soft, that is, no damage to stable storage so that logged records are available after a failure; 2) fail-stop so that only correct information is logged and erroneous output does not reach users or persistent databases; and 3) the result of “Heisenbugs” [Gray and Reuter 1993] such as timing-related race conditions or overload conditions so that replay does not reproduce the failure deterministically. We make no assumptions about multiple failures; more than one component may fail at any time. Consider the scenario in Figure 4, with message-based interactions among user, client, and three application servers. We want the message flow and resulting states of the involved components to persist in any possible failures. When a component fails and is restarted, we want recovery to recreate exactly the state that it was in before the failure, including all received messages. We call a component with this ability a persistent component (Pcom). We will introduce two additional kinds of components in Section 4. The current discussion concentrates on Pcoms. A baseline algorithm to ensure persistence and thus failure masking might employ pessimistic logging (see Section 2.2). This requires that Pcoms log all incoming and outgoing messages and immediately force them to their stable logs before taking any subsequent actions. For example, in Figure 4, application server 1 dealing with messages 2, 3, 4, 5, 6, and 7, force-writes log entries numbered 2.1, 2.9, 4.1, 4.9, 6.1, and 6.9 for them. The numbering reflects the ordering between messages and log writes, that is, 2, 2.1, 2.9, 3, 4, 4.1, 4.9, 5, 6, 6.1, 6.9, and 7. Should the server fail, it can completely replay its prior execution starting from its initial state or some intermediate installation point (e.g., 4.5.1 after having received message 4). When replay reaches a point where the original execution received a message, the logged message is fed to the component’s message buffer, and replay is resumed. In this way, the replayed process will generate exactly the same outgoing messages as in its prior incarnation. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 300 • R. Barga et al. Pessimistic logging can be inefficient, leading to unnecessarily many forcedlog-writes. Overhead is at least as high with communication based on reliable message queues. This inefficiency is illustrated by considering the logging for a single message, as shown in Figure 4. For a sender-receiver pair, it suffices for the sender to force-log the message. Consider message 4. Suppose server 1 failed after receiving message 4; replay simply asks server 2 to provide the missing log entry. This shows the I/O saving potential. There is a substantial advantage to considering what is needed for successful exactly-once execution in the abstract, without assuming any specific technique for implementing the requirements. We use the following observations to guide us in defining interaction contracts (ICs) between pairs of components. Committed state: When a component sends a message, it externalizes, and thus commits its state. The message serves to reveal what state the sender is in. This is akin to committing a transaction, and requires that the sender state become persistent (durable). Since the sender must make its state durable, there is low cost of also taking responsibility for ensuring that the message is likewise persistent, as both message and state persistence can be recovered with the same deterministic replay. Deterministic replay: Replaying components requires that all nondeterministic events be captured so that these events can be replayed during recovery. We have already indicated that messages between components are logged. We need to ensure that all sources of nondeterminism are removed. Such nondeterministic events as system interrupts, and nonidempotent interactions with the system environment, need to be captured so that they can be replayed deterministically. For example, in Figure 4 the event 4.5.1 could indicate reading the system clock or a sensor’s real-time data stream. It is not necessary to make these events stable until we “commit” component state as described in (1) above. Duplicate detection: During replay, we may not know whether a message generated immediately preceding a system crash was sent or not. Even if we knew the message had been sent, we may not be sure that it arrived successfully. To guarantee that a message will eventually be received, we may need to send it multiple times. To ensure exactly-once semantics, we thus need to detect duplicate messages and respond appropriately, either by ignoring them or by replying to them with the same reply message as was generated originally. In all cases, we must avoid multiple executions of any nonidempotent receiving component. Autonomous recovery: If components are managed in an autonomous fashion, we cannot necessarily depend on (trust) other components to resend previously sent messages. In this case, we need to more promptly make stable the events (messages) that we receive from these undependable components. With these observations, it is possible to define what we call a committed interaction contract (CIC). This contract is sufficiently abstract to enable its satisfaction with techniques that exploit the optimization illustrated in the previous section: where server 1 asks server 2 to resend the message rather than forcing the message to its log immediately. Details will be given in Section 4. In particular, we will discuss when the obligations that the contract parties agree upon can be safely released, thus enabling garbage collection on log files. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 301 When autonomous recovery is an issue, we will incur higher logging costs. Returning to our example, when server 1 does not trust that server 2 will resend the original message, it will need to force log message 4. We refer to a contract covering this situation as an immediately committed interaction contract (ICIC), for it allows both interacting parties to recover independently and to perform garbage collection independently right after the message exchange. For the scenario of Figure 4, this suggests using CICs between the client and server 1 and also between server 1 and server 2, the latter two being mutually considered trustworthy. On the other hand, an ICIC would be in order between server 1 and the external server 3. Furthermore, if application server 1 did not want to become in any way dependent on the client, then an ICIC would also be used between client and server 1. Between user and client a special kind of contract is needed to be discussed in Section 4. This overall setup of interaction contracts translates into a specific choice of which log entries need to be forced to disk and which ones can be written to log buffers and lazily flushed to disk in the background. In Figure 4, the log-writes that require forcing are denoted by thick arrows. 3.3 Piece-Wise Deterministic Behavior Ensuring that a component is indeed PWD, is subtle. Application servers with multiple threads serving multiple clients that communicate with it and that asynchronously access shared data clearly have multiple sources of nondeterminism. But even simple single-threaded, synchronous components, for exapmle clients, can have “hidden” sources of nondeterminism that need to be expunged. We identify three sources of nondeterminism and indicate how the nondeterminism can be removed.  A component, for exapmle database or application server, may execute on multiple concurrent threads, accessing shared data such that access interleaving order is essential for successful replay (e.g., SAP-style ERP systems or eBay-style web sites). We assume there is no shared state among different components. Multiple components accessing common data require that data be in a component, for exapmle, a database server. Nondeterminism is removed by logging the interleaved accesses to the data. For a transactional database, the order of interleaving is the order in which transactions are serialized.  Component execution may depend on asynchronous events, for exapmle, received messages or interrupts that prompt component execution at arbitrary points. These events are not reproducible during replay. The order of asynchronous events needs to be logged to guarantee deterministic replay. Often short logical log entries are sufficient, for exapmle message receive order, if message contents can be recreated by other means (for example, from the sender). However sometimes, physical logging is inevitable, for example, when reading the real-time clock.  Component re-creation after a crash does not usually exploit the same system elements as the original execution, a form of nondeterminism. Ids for ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 302 • R. Barga et al. messages sent before a failure may differ from ids of replayed messages. Ids for processes, threads, or users may also change. To remove system resource mapping nondeterminism, we “virtualize” these resources, introducing logical ids for messages, component instances, and so on. These logical ids are mapped to different physical entities after a crash, but at the abstract level, the “logically identified” component becomes PWD. We log these mappings. 4. INTERACTION CONTRACTS 4.1 Components and Interactions 4.1.1 Component Guarantees. Guarantees specify the behavior of individual components upon failure and are the basis for interaction contracts between components. Mostly we are concerned about persistent components (Pcoms), for which we guarantee persistent state. To guarantee state persistence will also require persistence guarantees for messages. However, a multi-tier application is rarely composed solely of persistent components. We need to treat other components and their interactions with Pcoms. Some components are transactional (Tcoms), where state and messages are only guaranteed to persist when transactions commit. A transaction abort resets the Tcom state to the beginning of the transaction, losing intratransaction updates and messages. Finally, external (user) components (Xcoms) cannot usually provide any of the above guarantees. For example, when prompted to provide a previous input, a user does not necessarily deliver identical input. To implement persistence guarantees, we exploit a log and arecovery manager in the run-time environment to capture the order of all nondeterministic events and record the ones that cannot be replayed. During normal operation log entries are created in a log buffer for received messages, sent messages, and other nondeterministic events. The log buffer is written to the stable log on disk at appropriate points (forced) or when full. In addition, component state may be periodically “installed” (saved) to disk in an installation point to facilitate log truncation, frequently making log records preceding the installation point unnecessary. A data server that needs a stable log for the recoverability of persistent data can also use the log to hold message-related and other log entries. During restart after a failure, the recovery system scans the relevant parts of the stable log. A component is re-incarnated from its last installation point and replayed from there. The recovery system intercepts all messages and nondeterministic events; relevant information is reconstructed from the corresponding log entry and fed to the component in place of the original event. When log entries do not contain message contents, communication with the sender is required to obtain the contents. For this, a recovery contract with the sender ensures that the message can indeed be provided again. Outgoing messages which the replaying component knows, either directly or via inference, have been successfully and stably received (or more precisely, if the component knows that the receiver’s state as of message receipt is stable) prior to a failure, may be suppressed. However, if the component cannot determine this, then the message needs to be resent, and the receiver must test for duplicates. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 303 At this point we can establish an important property that relates the persistence guarantees of a component to the underlying implementation mechanisms. THEOREM 1. A component can guarantee a) persistent state as of the time of the last sent message or more recent and b) persistent sent messages from the last installation point up to and including the last sent message if it:  logs all nondeterministic events, such that these events can be replayed,  forces the log upon each message send (before actually sending it) if there are nondeterministic events that are not yet on the stable log, and  can recreate, possibly with the help of other components, the contents of all messages received since its last installation point. PROOF. By ensuring that all prior nondeterministic events are stable on the log upon each message send, the component can be replayed at least up to and including the point of its last send. This replay can be done for all nondeterministic events because the last installation point can be reconstructed from the log, and received messages can be accessed, perhaps locally, perhaps by request to their senders. Note that the latter implies that the component has not necessarily logged the contents of its received messages. Finally, all outgoing messages can be recreated during the component replay. Note that this does not require that the message send is itself logged; rather outgoing messages can be deterministically reconstructed provided all preceding nondeterministic events are on the log or already installed in the component state. 4.1.2 Interaction Contracts. An interaction contract specifies the joint behavior of two interacting components. It requires each of them to make certain guarantees, depending on the nature of the contract and components. Perhaps only one component can provide strong guarantees, whereas the other component cannot. Different contracts provide flexibility in the design space. An interaction contract specifies guarantees about a state transition. The guarantees are permanent, but log records needed to provide the guarantee can be garbage collected when both components agree they are no longer needed. Such agreements can be set up a priori, for example, by limiting the logging to the last state transition common to the two involved components, or dynamically negotiated. We consider three types of components as contract partners: persistent components whose state should persist across failures, transactional components, usually data servers, that provide all-or-nothing guarantees for atomic transactions, and external components, which can be used, for example, to capture human users, where there is usually no recovery guarantee. 4.2 Persistent-Persistent Component Interactions Persistent components, when they interact with other persistent components, must ensure the persistence of both state and message at each interaction. Committed interaction contracts are used for this purpose. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 304 • R. Barga et al. 4.2.1 Committed Interaction Contract. A committed state transition involves a pair of persistent components, whose states persist across system failures. One Pcom sends a message and another receives it. A committed interaction contract (CIC) is the fundamental building block for making entire applications persistent and masking failures to users. Definition 1. A committed interaction contract consists of the following obligations: Sender: S1: Persistent State. Sender promises that its state at the time of the message send or later is persistent. S2: Persistent Message. S2a: Sender promises to send the message repeatedly (driven by timeouts) until receiver releases it (perhaps implicitly) from this obligation. S2b: Sender promises to resend the message upon explicit receiver request until the receiver releases it from this obligation. This is distinct from S2a, typically longer lasting and usually more explicit. S3: Unique Messages. Sender promises that its messages have unique contents (including all header information such as timestamps, HTTP cookies, etc.). Sender obligations ensure that an interaction is recoverable, it is guaranteed to occur, though not with the receiver guaranteed to be in exactly the same state. Note that message uniqueness is required so that the receiver has a chance to detect duplicates (i.e., resent messages) and does not confuse them with another message with exactly the same content as a previous one (e.g., the same shopping cart contents with the same timestamp and the same cookies etc.). Receiver: R1: Duplicate Message Elimination. Receiver promises to eliminate duplicate messages (which sender may send to satisfy S2a). R2: Persistent State. R2a: Receiver promises that before releasing sender obligation S2a, its state at the time of message receive or later is persistent without the sender periodically resending. After S2a release, receiver must explicitly request the message from sender should it be needed. The interaction is stable, it persists (via recovery if needed) with the same state transition as originally. R2b: Receiver promises that before releasing the sender from obligation S2b, its state at the time of the message receive or later is persistent without the need to request the message from the sender. After S2b release, the interaction is installed, replay of the interaction is no longer needed. Note the contract asymmetry: The sender makes a strong immediate promise whereas the receiver merely promises to obey rules in releasing the contract. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 305 Fig. 5. Statechart for committed interaction contract. The sender exposes its current state and “commits” to that state and the resulting message. It doesn’t know the implications on other components or, ultimately, external users, that could (transitively) result from subsequent receiver execution. Therefore, the sender must be prepared to resend the identical message if needed by later recovery and also to recreate its exact same state during replay after a failure. Each CIC pertains to one message. However, to fully discharge the CIC may require several messages. The receiver need not immediately expose to any other component that it received the message. Only when the receiver itself later becomes a sender does it commit itself to the effects of the received message and to the newly sent one, but this involves a new CIC, perhaps with another component, perhaps with the original sender. Before this, receiverforced-logging is not required. Eventual CIC release is essential to free the sender from its obligations. The sender wants to garbage-collect data retained to provide persistence for previously sent messages, not only in-memory data structures, but also stable log, periodically truncating it to shorten restart time and reclaim disk space. Once a CIC is released, the sender can discard the interaction data; however, the sender still guarantees the persistence of its own state at least as recent as the interaction. This persistent state guarantee falls out naturally from our implementation techniques. Sender and receiver CIC behavior is depicted as a statechart [Harel and Gery 1997, OMG: UML 1999] in Figure 5. Ovals show sender and receiver states; transitions are labeled with “event [condition] / action” rules where each element is optional and omitted when not needed. A transition fires if the specified event occurs and its condition is true, then the state transition executes the specified action. For example, the label “/ stability notification” of the receiver’s transition from “interaction stable” state into “running” state specifies that this transition fires unconditionally (i.e., its condition is “[true]”) and its action is sending a stability notification. The sender transition labeled ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 306 • R. Barga et al. “stability notification” makes the corresponding state change when it receives the stability notification (i.e., when the event “stability notification” is raised). Sender and receiver return to their running states before making further steps towards a stable interaction. Unlike two-phase commit, a CIC allows intermediate states for the two components to exist for an extended period, enabling logging optimizations. Note that, for simplicity, we have omitted all transitions for periodic resends (e.g., sender’s periodic resend of the message until it receives the stability notification). While each message has its own contract, the nature of the interactions between two components can enable further optimizations. Request/reply interactions, as in the client-server setting, are an important situation because real protocols are frequently of that form, whether the reply contains application related information or is only an acknowledgement. Consider requestor Q and replier P. The precondition for the reply is that Q’s state is persistent and that Q will resend the request until P announces the commit of its state that includes the reply. Hence the reply message need not be sent periodically as Q has already committed to receiving the reply (i.e., is synchronously waiting for the reply). P need only resend the reply on request, which in this case is in response to resends of Q. 4.2.2 Immediately Committed Interaction Contract. In some situations, it is desirable to release a sender from its obligations all at once. This can be useful not only to the sender, but also the receiver, as it enables the receiver to recover independently of the sender. This is achieved by strengthening the interaction contract into an immediately committed interaction contract (ICIC). Definition 2. An immediately committed interaction is a committed interaction where sender is released from both message persistence requirements, S2a and S2b when receiver notifies sender (usually via another message) that the message-received state has been installed, without previously notifying sender that its state is stable. Receiver’s announcement thus makes the interaction both stable and installed simultaneously. An ICIC can be seen as a package of two CICs, the first one for the original message and the second one for a combined stability-and-install notification sent by the receiver component. The first CIC requires the sender to make its state persistent, and the second CIC, for the notification sent by the original receiver, requires the receiver to make its state persistent as well. With an ICIC for the entire interaction, the sender waits synchronously for this notification (rather than resuming other work in its “running” state), and the receiver’s part of the committed interaction is no longer deferrable. This is similar to an optimized form of two-phase commit between the components: it involves only two participants with the sender being the coordinator, and making its commitment right away, a form of “first agent optimization” (i.e. a “dual” version of the well known last agent optimization [Gray and Reuter 1993; Bernstein and Newcomer 1996; Weikum and Vossen 2001]). This a priori commitment is feasible here because the sender guarantees that it will resend the message ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 307 Fig. 6. Statechart for immediately committed interaction contract. until it eventually gets the receiver to also commit the interaction. Figure 6 depicts the ICIC behavior as a statechart. With a committed interaction, whether either party requires logging depends on whether there are nondeterministic events that need to be made repeatable. If not, then no logging is required, as the interaction, including the message contents, is made persistent via replay. With an immediately committed interaction, the receiver is required to make stable the message contents so that its state, which includes the receipt of the message, is persistent without needing the sender to resend the message. Thus, an immediately committed interaction can be more expensive than a committed interaction both in log-forces (when logging is used for message persistence) and in how much is logged (both message arrival and contents). Because ICICs always require forced logging by the receiver to immediately install the interaction, they are not always appropriate. It is the avoidance of this cost and its adverse impact on system throughput that makes the simple committed interaction useful. In traditional OLTP, expensive ICICs have been the method of choice. CICs substantially reduce the overhead of a recovery contract. ICICs do, however, ensure independent recovery of the receiver; otherwise the receiver must rely on the sender for recreating the message contents. We discuss such recovery dependencies in more detail in Section 5.3. 4.3 Persistent-External Component Interactions Sender and receiver must be Pcoms to engage in committed interactions. External components (Xcoms) may not be persistent, hence cannot have committed interactions. Importantly, one form of Xcom is a human user. Our intent is to come as close as possible to providing an immediately committed interaction with Xcoms, including users. This leads us to introduce external interaction contracts (XICs). ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 308 • R. Barga et al. Definition 3. An external interaction contract is a contract between a persistent component that subscribes to the rules for an immediately committed interaction, and an external component, that does not. The impacts on external sender or receiver (or users) of Pcom interactions with it are described below. Note that these are impacts on, not obligations of, the external component. X1: Output Message Send. A Pcom (usually a client machine) sends (displays) a message to an Xcom (e.g., external user), after having logged the message send. The sender Pcom crashes before knowing whether the message was seen. Hence it must resend the message. Because an Xcom might not eliminate duplicates, a user may see a duplicate message. X2: Input Message Receive. An external user (Xcom) sends a message, via keyboard, mouse, or other input device, to a (client) Pcom. The Pcom crashes before logging the message. On restart, the user must resend the message. But the user (an Xcom) has not promised to resend the message automatically; rather if makes only a “best effort” at this. Moreover, the failure is not masked. The property of interest here is that in the absence of a failure during the XIC interaction, the result of an XIC is an immediately committed interaction that masks internal failures from the external components. Importantly, a Pcom failure between the logging of the input message and the output message is known to be masked from the external component. 4.4 Persistent-Transactional Component Interactions Another form of contract covers interactions between a Pcom and a transactional component (Tcom), usually a data server. These are request/reply interactions, where either a) a Pcom request message initiates the execution of a transaction (e.g., invoking a stored procedure) against the server’s state and produces a reply reporting the transaction outcome, or b) a sequence of Pcom request/reply interactions (e.g., SQL commands) occurs, the first initiating a transaction and the last being the server’s final reply to a committransaction or rollback-transaction request. The Tcom’s state is transactional. During transaction execution, the changes made by the transaction are isolated, visible only to transaction participants. The Tcom’s state transition is atomic, all-or-nothing, based on whether the transaction commits (all) or aborts (nothing). Persistence makes no requirements on interactions between Pcom and Tcom during a transaction. And transaction isolation keeps intratransaction state changes from propagating elsewhere in the system. The critical interaction is when the Pcom sends a transaction-commit request message to the Tcom, and the Tcom sends a reply to this message. To provide persistence and exactly-once semantics for interactions between Pcoms and Tcoms, we place requirements on both Pcom and Tcom, as described below in the transactional interaction contract (TIC): ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 309 Definition 4. A transactional interaction contract between a Pcom and a Tcom consists of the following obligations. Pcom: PS1: Persistent reply-expected state. The Pcom’s state as of the time at which the reply to the commit request is expected, or later, must persist without its having to contact the Tcom to repeat its earlier sent messages. The persistent state guarantee thus includes the installation of all earlier Tcom replies within the same transaction, for example SQL results, and return codes. Persistence by the Pcom of its reply-expected state means that the Tcom, rather than repeatedly sending its reply (under TS1), need send it only once. The Pcom explicitly requests the reply message should it not receive it by resending its commit request message. PS2: Persistent commit request message. The Pcom’s commit request message must persist and be resent, driven by timeouts, until the Pcom receives the Tcom’s reply message. PS3: Unique message. The Pcom promises that its commit request message has unique contents (including all header information such as timestamps, etc.). PR1: Duplicate message elimination. The Pcom promises to eliminate duplicate reply messages to its commit request message (which the Tcom may send as a result of Tcom receiving multiple duplicate commit request messages sent by Pcom because of PS2). PR2: Persistent reply installed state. The Pcom promises that, before releasing Tcom from its obligation under TS1, its state at the time of the Tcom commit reply message receive or later, is persistent without the need to again request the reply message from the Tcom. Tcom: TR1: Duplicate elimination. Tcom promises to eliminate duplicate commit request messages (which Pcom may send to satisfy PS2). It treats duplicate copies of the message as requests to resend the reply message. TR2: Atomic, isolated, and persistent state transition. The Tcom promises that before releasing Pcom from its obligations under PS2 by sending a reply message, it has proceeded to one of two possible states, either committing or aborting the transaction (or not executing it at all— equivalent to aborting), and that the resulting state is persistent. TS1: Persistent (faithful) reply message. Once the transaction terminates, the Tcom replies, acknowledging the commit request, and guarantees persistence of this reply until released from this guarantee by the Pcom. The Tcom promises to resend the message upon explicit Pcom request, as indicated in TR1 above. The Tcom reply message identifies the transaction named in the commit request message and faithfully reports whether it has committed or aborted. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 310 • R. Barga et al. TS2: Unique message. The Tcom promises that its commit reply message has unique contents (including all header information such as timestamps, etc.). In current practice, for example with current database systems, the most dif- ficult requirement is to ensure the delivery of the transaction outcome message. The reply to a commit request message might not be delivered even though the transaction successfully commits, as a crash may occur between transaction commit and message send. We require a stronger guarantee for Tcoms, as expressed in TS1, namely that the reply must persist and be resent upon request from the Pcom. Furthermore, when the transaction aborts, a database may forget the transaction, which could pose difficulties for making the transaction outcome message persistent. When a transaction commits, as directed by a commit request message, the Tcom must ensure that the commit message is stable until it is released from this requirement by the Pcom. It is this stable reply message (reporting that the transaction committed) that provides the testable state preventing the transaction from multiple executions. We do not need the faithful reply guarantee, in which the Tcom is obligated to correctly report on the commit/abort outcome of the transaction, to ensure that Pcom state is persistent. This guarantee is necessary, however, to ensure exactly-once execution semantics, including the updates to databases. Note that PS1 applies only for commits, not aborts. PS1 removes the need for a Tcom to retain earlier messages of the transaction. TS1, in conjunction with PS1 means that the Tcom need only capture the transaction’s effects on its database and its reply to the commit request message, since earlier messages in the transaction are not needed for Pcom state persistence. Thus the Tcom supports testable transaction status so that the Pcom can inquire whether a given transaction was indeed committed. This is important as persistent committed state and persistent commit reply are what are usually guaranteed by queued transactions TP monitors and by Phoenix/ODBC [ Barga et al. 2000; Barga and Lomet 2001]. Thus, our recovery contracts work with existing infrastructure. When a transaction aborts, there are no guarantees except that the transaction’s effect on Tcom state is erased. If the Tcom aborts the transaction or the Pcom requests a transaction rollback, neither messages nor the Pcom’s intratransaction state need persist. There are two cases: 1. The Tcom fails or autonomously aborts the transaction for other reasons. If a transaction aborts following a sequence of request/reply interactions within the transaction, abort is signaled to the Pcom in reply to the next request (e.g., through a return error code). Abort can also result from a commit or a rollback request message. When the Tcom has no record of a transaction, this means that the transaction has been aborted. So persistence of an abort reply message is trivially achieved. The Pcom may choose to re-initiate the transaction, but the Tcom treats this as a completely new transaction, a standard practice in transaction processing. If the abort response is lost, a repeated commit request message either returns the abort response, or, if ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 311 the entire transaction is defined by the commit request message (e.g., when invoking a stored procedure with a single SQL call), causes the re-execution of the transaction, with a new outcome that is reported to the Pcom. 2. When the Pcom fails in the middle of the transaction, the Tcom will abort (e.g., driven by timeouts for the connection) and forget the transaction. When the Pcom later attempts to resume the transaction (on a new connection), the Tcom will respond with, for example, a “transaction/connection unknown” return code, and the Pcom proceeds as in the first case. In addition to commit and abort, a third scenario arises when the transaction is still active at the Tcom and the Pcom repeats the sending of the commit request message, its way of asking about the transaction status. This can happen if the Pcom times out waiting for a reply to its commit request, or fails after having sent a commit request but before receiving the commit reply. Should the Pcom require restart, it would be able to recreate the commit request (by PS1). In any event, the Tcom must detect a duplicate message and, if the activity of the original commit request is not finished, the Tcom must await the completion of this work and then report back the outcome as if it had received only a single message. 4.5 System-Wide Composition of Contracts Our aim is to transform from black art to engineering the method for guaranteeing system-wide recoverability for multi-tier systems. Our approach is to combine bilateral interaction contracts between pairs of components into a system-wide agreement that provides the desired guarantees to external users. The key to such a recovery constitution is the observation that the behavior of a multi-tier system is based on these different kinds of interactions: internal ones that do not involve user or data server, external ones between a user and a (client) component, and transactional ones between components and data server. Interaction contracts provide the following general solution:  Each internal interaction between a pair of Pcoms has a committed interaction contract (CIC or ICIC).  Each external interaction between Pcom and Xcom (user) has an external interaction contract (XIC).  Each request/reply interaction from Pcom to Tcom has a transactional interaction contract (TIC). Note: Tcoms are not allowed to call Pcoms (violating isolation and breaching the transactional all-or-nothing paradigm) or Xcoms. Our recovery constitution allows arbitrary interaction patterns between Pcoms, including, for example, asynchronous message exchanges, callbacks from a server to a client or among servers (e.g., to signal exceptions), or conversational message exchanges with either one of two components being a possible initiator (e.g., in collaborative work applications). The only restriction is that Tcoms not call Pcoms or Xcoms but only reply to requests from Pcoms. Then the following very general theorem holds: ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 312 • R. Barga et al. THEOREM 2. Consider an arbitrary (possibly cyclic) graph of message exchange relationships among a set of components with an arc from component A to component B if A sends a message that B receives. The graph must have no edges between Tcoms and Xcoms, and the only edges from Tcoms to Pcoms are Tcom replies to Pcom transactional requests. Then the following holds: If there is a CIC (or ICIC) for each pair of Pcoms that interact directly, an XIC for each message sent or received by an Xcom, and a TIC for each message sent or received from a Tcom, then all failures can be masked with the exception of failures during external interactions. PROOF. We first derive a total ordering of all messages in the entire system’s history from what is known as causal order in the distributed computing literature: 1. If a component receives message A and later sends message B, then A is before B; if a component sends message A and later sends message B, then A is before B; if A is before B and B is before C, then A is before C (i.e., we consider the transitive closure of the orderings according to (i) and (ii)). All messages that are unordered according to (i) through (iii) are arbitrarily ordered by some topological sorting of the partial order obtained from (i) through (iii). This yields a total message order that preserves message send order in each component. In addition, we assume that at each component, any nondeterministic events (including message receives) are totally ordered among each other and with message sends. This total message order does not guarantee that messages are delivered to the receiving components in causal order. We make no assumptions about the underlying communication system. This send message order merely serves as a basis for the following induction: we prove our claim by induction on the number of message exchanges in the system history, up to the last failure. Basis: The first sent message in the system history must be an input message as all other messages are causally dependent on input. This message is subject to an XIC. Thus, it is guaranteed to be recreatable once the contract is completed. However, since failures can occur at any time and repeatedly, there is no guarantee that this input might not need to be entered repeatedly, as permitted by our claim. Induction: Assume that the entire system history consists of n+1 sent messages and that our claim holds for the first n messages, where we ignore and discard any messages sent on behalf of uncommitted or aborted transactions between a persistent component (Pcom) and a transactional component (Tcom). (These transactional messages become relevant only upon the commit of the transaction.) We further assume that external interactions up to this point also satisfy our induction hypothesis. Thus all n messages have been received and processed with exactly-once semantics. Further assume that it is either the sender or the receiver of the n+1st message that fails after the completion of the n+1st committed interaction. Note that this does not rule out multiple failures ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 313 of one or both of these components, or other components, but we can assume that the recovery is performed for each failure separately and sequentially. All guarantees up to and including the nth message are satisfied by the induction assumption; so it is indeed only a failure by the n+1st sender or receiver that needs closer inspection. We can verify our claim by a case analysis with regard to the type of the n+1st message (i.e., internal versus external and persistent versus transactional component) and the component that fails (i.e., sender versus receiver): 1. Internal interaction among persistent components: If the n+1st message is between two persistent components then the following holds. The failed component is reincarnated from its last installation point and replayed from there. During replay all nondeterministic events that precede the last sent message are reconstructed from a local log. This is guaranteed because of the component’s state recreatability guarantee that is part of the last send’s committed interaction contract. All received messages, that precede the component’s own last send in the causal order, can be recreated by the induction assumption. a. Failed component is sender of n+1st message: The CIC for the send ensures that we can recreate and repeat the send. Since all receivers in a CIC can test for duplicate messages, this restart behavior preserves exactly-once semantics. b. Failed component is receiver of n+1st message: We know that, in its prior incarnation, it has not sent any messages after receiving the n+1st one, for then the n+1st message would then not be the last in the global order. So the receiver has not committed its state to any other component after its own last message send, which is recreatable by the induction assumption. Thus the fact that the received message was sent again and received (perhaps multiple times) in a state that possibly differs from that of the prior incarnation but has not been exposed to other components, does not unmask the failure. 2. External Interaction: The n+1st message is an external input or output message. Then the failed component must be either the internal receiver of external input or internal sender of external output. These interactions are subject to an XIC. Hence, once the interaction completes, the message is stably delivered (in either direction). Hence, no subsequent failure can affect the system’s ability to mask a failure in this XIC. For failures within the external interaction, arbitrary repeating of the message send or receive may be required. This is inevitable in any recovery scheme, and is permitted in the behavior for our claim. 3. Internal interaction between a persistent and a transactional component: The n+1st message is a transactional request from a Pcom to a Tcom or a transactional reply from a Tcom to the requesting Pcom. If this message is neither the Pcom’s commit request nor the Tcom’s commit reply then this n+1st message does not require any guarantee as the transaction is not (yet) committed. If it is the Pcom’s commit request, then the Pcom guarantees its state to be persistent as of the point when the commit request is ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 314 • R. Barga et al. sent; so the commit request message can be repeated if necessary. Recall, however, that the Tcom may still abort the transaction; then it is up to the Pcom to handle this outcome and the Pcom is able to do so because of its state being persistent. The final case to consider is when the n+1st message is the Tcom’s commit reply. At this point the Tcom promises that both its state and the reply message are persistent. So the guarantee in our claim is extended to the n+1st message and covers also, implicitly through the persistent state of the Pcom, all reply messages that may have been sent within the transaction boundaries (in the case that the transaction comprises multiple request/reply steps). Theorem 2 is the basis for building multi-tier systems with message, component state, and data recovery with failure masking. However, it does not capture important pragmatic issues. It says nothing about when CIC or ICIC contracts are released, and garbage collection and log truncation can be performed at the components. Before we describe these, we must discuss our underlying implementation techniques. We will return to this issue in Section 5. Inability to mask send or receive failures can occur only with a failure during an external interaction. This is possible with any conceivable recovery algorithm without special hardware support. An example of possible hardware support (providing testable state) for output messages is an ATM for dispensing cash in which a mechanical counter records when money is dispensed. Output messages (e.g., cash) are guaranteed to be delivered exactly once, that is only when the counter is in the correct state. 5. IMPLEMENTING RECOVERY CONTRACTS Interaction contracts and implementation measures are separate layers in our framework. A system can provide strong contracts, in the sense of Theorem 2, for all bilateral interactions while implementing some of them with little or no overhead. Indeed, there are many potential ways for a collection of components to support CICs. Here we outline one such way to do this; more details can be found in Barga et al. [2002]. 5.1 Log Management Data servers have the hardest logging requirements because they are usually heavily utilized, support many concurrent “users,” maintain valuable enterprise data, and are carefully managed for high availability. In addition to the usual logging for persistent data (see Section 2.1), the data component, a Tcom, needs to log only the final reply message for a caller’s commit request, with enough information so that it can recognize duplicate commit request messages, and the server log needs to be forced before sending this final reply. Aborted transactions require no log forcing. Because SQL session states such as cursors or temporary tables can span transaction boundaries, we also provide persistent state for session components. The server maintains this information as state that is covered by interaction contracts. Phoenix/ODBC [Barga et al. 2000; Barga and Lomet 2001] retain this state to provide persistent sessions. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 315 Asynchronous message receives require logging (but no log-forcing), logical logging being sufficient for CIC interactions. Logical log entries capture the nondeterministic interleaving and uniquely identify sender and message, but do not contain message contents. Logging for sent messages can be either physical, including message contents, or logical. CICs require that the server force its log to include the (chronologically ordered) log records that ensure the persistence of a sent message before actually sending the message, when received messages have arrived in nondeterministic order. The advantage of CICs versus ICICs in reducing recovery overhead shows up with application servers and clients. For these components, often (but not necessarily) the only nondeterminism is the result of user input or data server interactions. Further, these components usually have little reason for using ICICs. What such components need to do for a CIC is to guarantee that replay will recreate their state and sent messages. In the absence of nondeterminism, this is frequently possible without forcing the log at interactions between system components. Only user interactions need to be force-logged as external interactions. For interactions with data servers (i.e., Tcoms), Pcoms (application servers or clients) need to ensure their state persistence as of the time of the committransaction request. If the transaction consists of a sequence of request/reply interactions, the Pcom needs to create log entries for the earlier Tcom replies and force the log before sending the commit request. Otherwise (i.e., for transactions with a single invocation request, e.g., to execute a stored procedure, and single reply) no forced logging is needed, unless the commit request is preceded by nondeterministic events that have to be tracked. If the Pcom issues a rollback request, no force logging is needed. The Pcom needs application logic for aborted transactions in any event. 5.2 Component Restart after Failure After a failure, each Pcom performs local recovery that reincarnates the component at its most recent installation point and replays the component log from there. The log is scanned in order, interpreting log entries to recreate persistent data and component state. To recreate component state, data reads, and other nondeterministic events are replayed from the log and the appropriate information, reconstructed via recovery, is fed to the component. This information can be from the local log, or requested of other components. The component is re-executed between message receives and other events. All Pcoms (data and application servers and clients) use this procedure to recover both component state and sent messages. Once a Pcom is recovered, it resumes normal operation. Part of this is to periodically resend committed-interaction messages that a receiver has not yet made stable. For a stable interaction, the message is only resent when the receiver explicitly asks for it, so it still must be persist. For an installed interaction (an ICIC is promptly installed), no action is needed, as the message contents are stable at the receiver. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 316 • R. Barga et al. 5.3 Recovery Independence and Garbage Collection We want to avoid one component’s recovery forcing a second component to perform an expensive recovery when the second has not failed. We want “isolated” component recovery, that is, no cascading restarts typical of many “optimistic” fault-tolerance algorithms [Alvisi and Marzullo 1995]. Interoperating components providing cross-organizational e-services are frequently autonomous, and cascading restarts are absolutely unacceptable. Nonetheless, an isolated component must resend messages as long as its contracts are not released. A solution is the volatile message lookup table (MLT) [Lomet and Weikum 1998] that records in main memory all uninstalled sent messages. These messages can be resent without component replay or reading the log. The MLT is rebuilt during recovery if the component fails; so it can always be present during normal execution. With complex multi-tier systems spanning autonomous organizations, components must be able to recover without reliance on other potentially less reliable or less trusted components. This autonomous recovery [Lomet and Weikum 1998] for the server in a client-server setting can be generalized to component ensembles. A component of an ensemble may rely on trusted ensemble components, but wants to be autonomous of components outside the ensemble. One example ensemble is a data server and application server at an e-commerce site, clients being outside the ensemble. An immediately committed interaction contract with immediate forced logging, similar to the Pcom side of an external interaction contract, produces this autonomy. Garbage collection is important for all components because they need to discard information from the MLT to reclaim memory and truncate the log to reclaim log space. It is critical for server components to ensure fast restart and thus high availability. Contracts with other components can hamper garbage collection. Therefore, another facet of component autonomy is to ensure that log and MLT entries kept on behalf of other components can be dropped in a reasonable time. 5.4 Performance Impact In this section we illustrate the potential performance of interaction contracts using the advanced multi-tier e-services scenario illustrated in Figure 1 (Subsection 1.3). In this example, client, Expedia web server and application server, and Amadeus and Sabre application servers, support sessions that are Pcoms, while all database servers support sessions that are Tcoms. Amadeus and Sabre interactions are handled via ICIC forced logging. But messages leading to a purchase that are directed to the lower-tier Expedia application server are treated as CICs, and are not forced. Interactions with database servers are treated as TICs. The bilateral interaction contracts for our e-service are set up as follows: [user⇔client] The client handles user input and output with XICs via prompt forced logging. Current Internet browsers do not provide native support for logging, but can be enhanced with a plug-in or applet. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 317 [client⇔database server] Interactions between the client and the database server are handled with TICs. The database server commits modifications to the permanent and shared database when sending its final reply to the client, and must force log this final reply message to ensure its persistence. [client⇔Expedia web server] Between client and upper tier web server, client request and server reply are handled with CICs. Forced logging is not required as client XIC logging captures all nondeterminism. [Expedia web server⇔Expedia application server] Between Expedia web server and application server, requests and replies are handled with CICs. No forced logging is required as, again, client XIC logging has captured all nondeterminism. [Expedia web server⇔external application server] Between the upper tier application web server and lower tier external application servers, ICICs that require forced logging by both the Expedia web server and external application servers are used to capture the potential nondeterminism as these application servers belong to other organizations and are thus autonomous. [application server→database server] Requests from application server to database server are transactional, and require a TIC. Because the application server is without nondeterminism, forced logging of individual requests is not required. A commit request exposes the effects of application server execution via changes to database server state, hence this state must persist. However, since prior ICICs with Expedia server or client have already captured all nondeterminism, forced logging is not required. [database server→application server] Finally, a database server commits modifications to a shared database when sending its final reply to the application server, exposing changes to other application servers. Thus the TIC requires a persistent reply message. Hence, this final reply (i.e., the return value for the SQL “commit work”) must be force logged, which can also capture the database server’s committed transaction updates. The contracts identified above are necessary for system-wide recoverability. The database server may also require effective garbage collection and independent recovery. Specifically, the database server can treat its transaction ending reply to the application server as an immediately committed interaction so that it can discard messages once it knows that the application server has received them, and therefore subsequently freely truncate its log. The number of forced log writes dominates the cost of our protocols in the above scenario. Let the user session consist of u input messages and u output messages, and let the client generate one request to its database server and x requests to the Expedia server for each user’s input message. In turn, Expedia will create y requests per incoming request to each of the three application servers, and let each of the external application servers create z requests per ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 318 • R. Barga et al. incoming request to its database server. Under these assumptions:  Pessimistic message logging requires a total of 2u + 4u + 4ux + 12uxy + 12uxyz forced log writes, a forced log write for each message by both sender and receiver.  Our protocol with the system configured as above, permits a potential reduction in the number of forced log writes to only u + u + 0 + 12uxy + 3uxyz, a saving of 4u + 4ux + 9uxyz disk I/Os. 6. IMPLEMENTATION OF INTERACTION CONTRACTS FOR INTERNET E-BUSINESS SESSIONS This section describes a prototype system coined EOS (for Exactly-Once E-Service) [Shegalov et al. 2002] that implements the interaction contract protocols for a three-tier web service architecture: an Internet browser as client, an HTTP server with a servlet engine as middle-tier application server, and a database system as backend data server. Specifically, we have built the prototype using IE5 as browser, Apache as HTTP server, and PHP as servlet engine; the data server can be any ODBC-compliant database system (e.g., SQL Server, Oracle, or DB2/UDB). The prototype currently supports an XIC between the user and the browser, a CIC between the browser and the mid-tier application server, and a TIC between the application server and the data server (where the implementation of the last part is not yet completed). Building the prototype required extensions to the IE5 environment in the form of JavaScript code in dynamic HTML pages (DHTML), modifications of the source code of the PHP session management in the Zend engine [PHP; ZEND], and modifications of the ODBC-related PHP functions as well as additional stored procedures in the underlying database. The main emphasis of the following description is on implementing (i) the XIC behavior at the IE browser, and (ii) the CIC behavior in the middle-tier Internet application server, which are the most innovative aspects of our work. 6.1 Browser Extensions To implement an XIC between the user (an Xcom) and her Internet browser session (a Pcom), one needs to extend or modify browser behavior so that certain types of user interactions (and also HTTP get or post requests to application servers and their replies) can be logged to stable storage. This raises issues of how to intercept the relevant events and how to embed the extra XIC code in the run-time environment of the browser. The solution that we came up with embeds special JavaScript code in a normal HTML page, and this DHTML (dynamic HTML) code realizes the XIC behavior. Our usage protocol requires that the user start interacting with an e-service by first visiting an initialization web page of that service, for example, a greetings page. The reply from this HTTP request contains our JavaScript code. The code is inserted into the HTML page returned by the server transparently to the application PHP program that ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 319 constructed the HTML page; this is done by our modifications to the Zend engine (see Subsection 6.2). The inserted JavaScript code exploits extensible event handling of IE5 [MSDN LIBRARY: PERSISTENCE]. It registers for various kinds of events, most importantly, the filling in of form entries, clicking on buttons, and positioning of scrollbars (and, optionally, the opening or closing of additional windows); IE5 automatically invokes the associated event-handling code upon these events. Note that our code is invoked only after the events have been processed by the browser itself and any other JavaScript code that may be already present in the application’s original HTML page. Figure 7 shows fragments of our browser extension code. The main function of our event-handling code stubs is to log the updates to the browser state (e.g., the user having clicked a button or filled in a form entry). This logging is done by modifying a so-called XML store, which is an XML structure managed by IE on the client’s disk in a way similar to a persistent cookie. This feature is provided by IE with a default persistence behavior called “userData Behavior.” HTML elements with attached userData behavior provide the methods for accessing the individual elements of the XML store. The XML object associated with the XML store can also be accessed and manipulated using the XML DOM parser, which is natively supported by IE [MSDN LIBRARY: PERSISTENCE]. To force our log entries in the XML store to stable storage, we simply call the “pagestate.save” method, which triggers IE to write the XML store to disk. For the CIC between browser and web application server, all relevant state information (i.e., each input field, whose value is passed to the web server, so that a completely identical HTTP request can be generated or “replayed” again) and rendering details (the window area viewed or edited by the user) are logged to an XML store associated with the current session step. Replies to HTTP requests are also logged, and they will be reflected in the browser state that is reconstructed upon recovery. Client state survives failures as follows. When the browser fails and the user restarts it and revisits the same e-service initialization page, she will be automatically redirected by the web server to the last visited (i.e., most recent) page of her interrupted conversation and our JavaScript code will be reloaded. The JavaScript code is set up to first look for an XML store previously saved on the client machine. If the XML store exists, its contents are used to recover input field values and replay all relevant events on windows, buttons, and forms, so that the user would not see any difference to the state immediately before the failure. 6.2 Application Server Modifications for CIC To implement a CIC for the web application server, the main issues to be addressed were the virtualization of message ids and the logging of HTTP requests and replies as well as session state information at the server side. We were able to realize this while modifying only the session management module of the ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 320 • R. Barga et al. Fig. 7. Fragments of the DHTML code embedded in the reply from the recovery-enabled web server. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 321 PHP Zend engine, hence avoiding source code modifications to the regular infrastructure on the server. For virtualization, we used our own message sequence numbers (MSNs) that are unique and consecutive within an application session. An application session, which is independent of any TCP session, might consist of several steps, for example adding items to a shopping cart, steps that constitute a “logical (and stateful) session.” MSNs are added to the HTTP request and reply messages in the form of additional cookies. The modified Zend engine force-logs all outgoing HTTP replies, tagging each of them with the MSN included in its header. This is done by writing this information as additional session variables to the PHP session state file. Note that there is no need to force-log the incoming HTTP request, as the client already promises the persistence of this message as the CIC sender. The session state file is accessible to all clones of PHP server processes that are controlled by the Apache web server. If multiple Apache servers run in a computer cluster for the same IP address, the file must be shared among all nodes in the cluster. This technique ensures that we do not depend on “sticky” connections between HTTP clients and PHP server processes. For a server to proactively recover servlet results after a server crash, as opposed to having servlet replays only when prompted by resent client requests, we may optionally log incoming HTTP requests along with the PHP variables filled by HTTP get or post parameters and all session variables that have been registered up to this point. The log record for an HTTP request already contains the name of the invoked PHP program and its input parameters, which are either form variables or encoded in the URL. This captures the initial state of the servlet execution. Since the servlet is PWD, no other logging is needed for correctness. We can now describe how the modified Zend engine handles the various exceptions and recovery situations:  Upon receiving an HTTP get or post request carrying a cookie with an MSN, we test for a duplicate request by checking the log. If it is a resent request and the corresponding servlet has already terminated and produced an HTTP reply, the reply is retrieved from the log and sent to the client. If the servlet has died and no reply is available, it is restarted. If we had logged the state of session variables during the prior incarnation of the servlet, its replay would start from the last completed installation point.  When an HTTP reply is sent to an unresponsive client (e.g., the client does not send a TCP ACK), the server simply ignores this but is prepared to receive a duplicate HTTP request at a later point. When this client-initiated prompting happens, the server resends the reply. As the reply itself is stably logged at the server, it is guaranteed to persist across server failures. Once the client acknowledges the receipt of the HTTP reply by issuing another HTTP request within the same application session or invokes a servlet with a session destroy function call, the server can garbage-collect the previous step’s log entry. To alleviate the potential danger that the server cannot safely discard log entries for clients with users who have intentionally aborted sessions ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 322 • R. Barga et al. Fig. 8. Pseudocode for application server logging and recovery. or simply walked away, we can enhance the JavaScript code that we embed in the HTTP replies to react to “onAbort” browser events and to timeouts: the code would simply send an explicit “abort session” message as a final HTTP request to the web application server. When a PHP process fails while executing a servlet on behalf of a client’s HTTP request, recovery is initiated when the client repeats its request (as part of the CIC behavior) or the user hits the “Refresh” button. The resent request is handled by the next available PHP process, which first performs a duplicate elimination test, possibly replays the servlet execution, and finally (re-)sends the reply. When Apache or the entire computer fails, the same thing happens after the restart of Apache. So recovery is automatic, but we rely on the client re-initiating the request rather than on server initiative. This behavior carries over to a web server farm on a computer cluster; failover to another node in the cluster is automatic as long as clients resend requests. The logging and recovery logic of our modified Zend engine are summarized in pseudocode form in Figure 8. 6.3 Performance To evaluate the run-time overhead of our failure-masking techniques and exactly-once guarantees, we performed measurements with Apache/1.3.20 and the Zend engine (PHP/4.0.6) running on a PC with a 1 GHz Intel Pentium III and 256 MB memory under Windows2000. The load on this web application server was generated by a synthetic HTTP request generator (Microsoft Web Application Stress Tool). The generator simulated conversations with n steps, each of which simply sent three string parameters as form fields, and a simple PHP program incremented a counter registered as a session variable and returned its value to the client. For simplicity we did not involve any data server in this setup. There were no human user interactions or simulated think times in this experiment. Table I shows the total elapsed time, between the first request ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 323 Table I. Elapsed Time and CPU Time for n-step Conversations With and Without CIC Original Zend engine Modified Zend engine with CIC Server CPU Server CPU N Elapsed time [sec] time [sec] Elapsed time [sec] time [sec] 1 0.01893 0.01412 0.01921 0.01435 5 0.09378 0.07052 0.09507 0.07156 10 0.18183 0.14380 0.18610 0.14363 Table II. Multi-User Response Time and Throughput for n-Step Conversations With and Without CIC Original Zend engine Modified Zend engine with CIC Response time[sec] Server throughput Response time [sec] Server throughput N for n-step session [sessions/sec] for n-step session [sessions/sec] 1 0.07872 62.35 0.08946 54.96 5 0.39475 12.434 0.44865 10.96 10 0.79597 6.168 0.80234 5.522 and the last reply as seen by the client, and the CPU time on the server side for n = 1, 5, 10 steps, comparing the original Zend engine to the modified Zend engine with CIC behavior for exactly-once guarantee. The figures show that the overhead of our CIC implementation is almost negligible, with respect to both user-perceived latency and increased CPU time. We also performed multi-user measurements where the HTTP request driver was replicated on 5 different client machines, each of which generated requests to the same web application server without simulating any think times (i.e., using a closed system model). Table II shows the measured average response time and throughput in terms of the simulated n-step user sessions. The figures show that the performance degradation is less than 10 percent and thus well within the range of acceptable overhead. 6.4 Further Considerations The CIC exactly-once guarantee between web browser and application server is very useful when impatient users click a commit/buy/checkout button multiple times. This particular difficulty could, to a large extent, also be avoided by better design of the user interface in the browser. Applications could and, in our opinion, should be written so that a clicked button is deactivated, possibly highlighted in a special color, until the corresponding HTTP reply is received. With a XIC-enabled browser and a CIC with the application server, the request would be guaranteed to be executed exactly once, yet the user would be prevented from creating unnecessary “noise.” For the human user such a deactivated button that is reactivated upon receiving the reply serves as “eyeball-testable state.” Note that such a better GUI alone would not eliminate all kinds of problematic situations caused by browser or application server failures in the middle of an application session, and surely does not obviate the need for interaction contracts. Our interaction contract protocols developed in this paper are orthogonal and complementary to more robust user interfaces. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 324 • R. Barga et al. Another dimension that is mostly orthogonal to interaction contracts is security. When secure communication channels are used, say HTTP over SSL (known as HTTPS), the underlying encryption is transparent to our protocols. In particular, as our resource virtualization never refers to any actual TCP session ids, the message sequence numbers and application session ids used by our protocol are not at all affected by the physical transport layer underneath. The situation is not quite that obvious with regard to authentication. While standard password-based authentication is no problem, authentication schemes with digital signatures based on keys that must not be used more than once could possibly interfere with the replay techniques of our recovery methods. If replaying a lost message involved using the same key a second time, this could present a security or privacy leak. The solution again lies in a flexible mapping of virtual to physical resources. Our log records refer only to virtual message and session ids; when a message is replayed its virtual id is mapped to a new physical id so that the underlying signature scheme would automatically use a new key as needed. Finally, very advanced cryptographic protocols, for example, for anonymous payments or legally binding electronic contracts (with mathematically provable tracking of the various parties’ behavior including any attempts of unfair behavior), can again be viewed as part of the Internet application itself, and our interaction contracts are orthogonal to the security issues. Overall, we believe that by layering the interaction contracts on top of whatever security and privacy measures are used, failure-masking remains unaffected by the security protocols and, likewise, no security leaks are introduced by the failure-handling protocols. 7. INDUSTRIAL RELEVANCE 7.1 Phoenix/App We have also implemented the recovery guarantees framework, as part of the Phoenix project on robust applications [MSR PHOENIX], in a system we call Phoenix/App [Barga et al. 2003]. In Phoenix/App we integrate interaction contracts into the Microsoft .Net runtime environment [MSDN LIBRARY: .NET; Williams 2002], allowing programmers to build persistent component-based applications without requiring modifications to their applications. To use Phoenix/App, an application programmer simply registers component classes as Pcoms or Tcoms. At runtime, applications are embedded into the .Net runtime and the .Net interceptor calls Phoenix/App code that captures all method calls and returns between components. Calls and replies between Pcoms are treated as CICs, while calls and replies between Pcom and Tcom are treated as TIC’s, and calls between a Pcom and any other component (an Xcom) are treated as XICs. Phoenix/App uses a log manager to create log records, flushing them to disk as necessary, and to handle log truncation. In the event of a system failure, Phoenix/App masks the error from the application, which can be written in any language executing on the Microsoft CLR (common language runtime), and automatically recovers any failed components from log records using redo recovery. The result is a persistent component-based application ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 325 that can survive system failures, without any special application code or operator intervention. While our current implementation of the recovery guarantees framework is .Net specific, the techniques are relevant to other object middleware environments such as CORBA or J2EE. 7.2 Comparison to EJB Another component framework to which interaction contracts and recovery guarantees could potentially be added would be J2EE with its concept of Enterprise Java Beans (EJB). EJB includes the concepts of entity beans, which allow persistent data from relational databases to be mapped into an object view and manipulated through this view (i.e., the bean interface), and session beans, which combine multiple method invocations on encapsulated objects into a stateful session. These two types of beans would be natural components to enhance with recovery guarantees in order to simplify the failure handling code that has to be written by the bean implementer. To a first approximation, session beans are candidates for the CIC or ICIC protocol, turning a bean into a persistent component, and entity beans are candidates for the caller side of the TIC protocol, turning a bean into a persistent component that interacts with a transactional component, namely, the underlying database server. An open issue, however, is to what extent this could be done without changing any of the EJB interfaces, and how such conceivable extensions compare to other component models (such as CORBA or .Net). As of now [SUN 2001], the recovery capabilities stated in the EJB specification are very limited and involve significant amounts of explicit programming. For session beans, EJB includes a method, ejbPassivate, for saving the conversational state of a stateful bean onto persistent storage. However, this is merely a mechanism: the bean implementer must provide code for it, and it is up to the surrounding “beans container” when to invoke it. Furthermore, the use of this method is restricted; for example, it must not be called when program control resides in the bean nor when the bean has an open transaction with a data server (or, equivalently, when it is called, the bean implementer has to make sure that all transactions and JDBC sessions are closed). For entity beans, atomicity and persistence of the underlying database operations are guaranteed through the corresponding database systems and the Java Transaction Service (JTS) for the coordination of distributed transactions. However, when a bean implementer caches persistent data and manipulates it in the bean for efficiency, all updates on cached data are not subject to transactional control and it is the implementer’s responsibility to include explicit code for writing back data to the underlying databases. So the capabilities currently provided by EJB are still far from being able to mask failures to application programmers and end users. 8. CONCLUSIONS In this article we have developed a general framework for recovery guarantees in multitier Internet applications. The novel notion of committed interaction contracts, with exactly-once execution and best possible failure masking, is particularly useful for e-services, which currently may exhibit unexpected and ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 326 • R. Barga et al. undesirable behavior due to failures, the consequences of which can greatly inconvenience the human user. Interaction contracts avoid these problems and can contribute to the construction of more dependable e-services. For practical viability, it is important that our protocols have been designed to minimize logging overhead and to provide fast recovery. Our prototype implementation for a PHP-based web application server is a proof of concept and shows that the overhead of our protocols is acceptable. The fact that this implementation required surprisingly few changes to the source code of the PHP Zend engine (and relatively small special DHTML code on the browser side) indicates that our conceptual framework provides the “right” abstractions and can be easily adapted to real software environments. Our framework applies to a wide spectrum of general multi-tier architectures, whether underlying components are database systems, other forms of data managers such as mail servers, application servers, message queues or workflow servers, or any application component. Our implementation of Phoenix/App shows that interaction contracts can be integrated with existing middleware architectures. In such a setting, recovery guarantees for persistent components complement the established notion of transactional components and provide value-added failure masking. In principle, our framework can be adapted to other middleware architectures such as J2EE and would provide additional benefits in these environments. The key benefit of our contribution lies in masking failures not only to end users but also, to a large extent, to application programmers, thus largely relieving them of the need to write explicit code to cope with system failures. This simplifies application development, makes code more easily maintainable, and generally reduces the cost of application software lifecycles. Automatic and efficient application recovery as provided by our protocols also improves the availability of e-services as perceived by end users. With very fast recovery, all transient failures and temporary outages would ideally be masked to internet users, but the internet infrastructure exhibits other idiosyncrasies and barely understood phenomena, such as load bursts, queueing delays, and timeouts, that affect the users’ perception of whether a service is working well or not. Our long-term vision is to provide comprehensive qualityof-service guarantees for Internet-based e-services that include availability and responsiveness, as well as “world-wide” failure masking. REFERENCES ALVISI, L. AND MARZULLO, K. 1995. Message logging: Pessimistic, optimistic, and causal. In Proceedings of the 15th International Conference on Distributed Computing Systems, Vancouver, Canada, May30–June 2, 1995. IEEE Computer Society, Los Alamitos, CA, 229–236. BARGA, R., LOMET, D., AND WEIKUM, G. 2002. Recovery guarantees for general multi-tier applications. In Proceedings of the 18th International Conference on Data Engineering, San Jose, CA, February 26–March 1, 2002. IEEE Computer Society, Los Alamitos, CA, 543–554. BARGA, R., LOMET, D., AGRAWAL, S. AND BABY, T. 2000. Persistent client-server database sessions. In Proceedings (Lecture Notes in Computer Science, 1777) of the 7th International Conference on Extending Database Technology, Constance, Germany, March 2000, C. Zaniolo, P. C. Lockemann, M. H. Scholl and T. Grust, Eds. Springer, Berlin and Heidelberg, Germany, 462– 477. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. Recovery Guarantees for Internet Applications • 327 BARGA, R. AND LOMET, D. 2001. Measuring and optimizing a system for persistent database sessions. In Proceedings of the 17th International Conference on Data Engineering, Heidelberg, Germany, April 2001. IEEE Computer Society, Los Alamitos, CA, 21–30. BARGA, R., LOMET, D., PAPARIZOS, S., YU, H., AND CHANDRASEKARAN, S. 2003. Persistent applications via automatic recovery. In Proceedings of the 17th International Database Engineering and Applications Symposium, Hong Kong, China, July 2003. IEEE Computer Society, Los Alamitos, CA, 258–267. BARTLETT, J. F. 1981. A NonStop kernel. In Proceedings (Operating System Review 15(5)) of the 8th Symposium on Operation Systems Principles, Asilomar, CA, December 1981. ACM, New York, 22–29. BERNSTEIN, P. A., HSU, M., AND MANN, B. 1990. Implementing recoverable requests using queues. In Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data, Atlantic City, NJ, June 1990, H. Garcia-Molina and H. V. Jagadish, Eds. ACM, New York, 112– 122. BERNSTEIN, P. A. AND NEWCOMER, E. 1996. Principles of Transaction Processing, Morgan Kaufmann, 1996. BORG, A., BLAU, W., GRAETSCH, W., HERRMANN, F., AND OBERLE, W. 1989. Fault tolerance under UNIX. ACM Transactions on Computer Systems 7, 1, 1–24. CRISTIAN, F. 1991. Understanding fault-tolerant distributed systems. Comm. ACM 34, 2, 56–78. DEBULL 2001. IEEE Bulletin of the Technical Committee on Data Engineering 24, 1. Special Issue on Infrastructure for Advanced E-Services. DUTTA, K., VANDERMEER, D., DATTA, A., RAMAMRITHAM K. 2001. User action recovery in internet SAGAs (iSAGAs). In Proceedings (Lecture Notes in Computer Science 2193) of the 2nd International Workshop on Technologies for E-Services (TES), Rome, Italy, September 2001, F. Casati, D. Georgakopoulos and M.-C. Shan, Eds. Springer, Heidelberg and Berlin, Germany, 132–146. ELNOZAHY, E. N., ALVISI, L., WANG, Y., AND JOHNSON, D. B. 2002. A survey of rollback-recovery protocols in message-passing systems. ACM Comput. Surv. 34, 3, 375–408. FREYTAG, J. C., CRISTIAN, F., AND KAHLER ¨ , B. 1987. Masking system crashes in database application programs. In Proceedings of 13th International Conference on Very Large Data Bases, Brighton, UK, September 1987, P. M. Stocker, W. Kent, and P. Hammersley, Eds. Morgan Kaufmann, 407– 416. FRøLUND, S. AND GUERRAOUI R. 2000. Implementing e-transactions with asynchronous replication. In Proceedings of 2000 International Conference on Dependable Systems and Networks, New York, NY, June 2000. IEEE Computer Society, Los Alamitos, CA, 449–458. FU, X., BULTAN, T., HULL, R., SU, J. 2001. Verification of vortex workflows. In Proceedings (Lecture Notes in Computer Science 2031) of the 7th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, Genoa, Italy, April 2001, Tiziana Margaria and Wang Yi, Eds. Springer, Berlin and Heidelberg, 143–157. GRAY, J. AND REUTER A. 1993. Transaction Processing: Concepts and Techniques. Morgan Kaufmann. HAREL, D. AND GERY, E. 1997. Executable object modeling with statecharts. IEEE Comput. 30, 7, 31–42. HUANG, Y. AND WANG, Y.-M. 1995. Why optimistic message logging has not been used in telecommunications systems. In Proceedings of the 25th International Symposium on Fault-Tolerant Computing Systems, Pasadena, CA, June 1995. IEEE Computer Society, Washington, D.C., 459. JOHNSON, D. B. AND ZWAENEPOEL, W. 1987. Sender-based message logging. In Proceedings of the 7th International Symposium on Fault-Tolerant Computing, Pittsburgh, PA, July 1987. IEEE Computer Society, 14–19. KIM, W. 1984. Highly available systems for database applications. ACM Comput. Surv. 16, 1, 71–98. LOMET, D. 1998. Persistent applications using generalized redo recovery. In Proceedings of the 14th International Conference on Data Engineering, Sydney, Australia, Orlando, FL, February 1998. IEEE Computer Society, Los Alamitos, CA, 154–163. LOMET, D. AND WEIKUM, G. 1998. Efficient transparent application recovery in client-server information systems. In Proceedings of 1998 ACM SIGMOD International Conference on Management of Data, Seattle, WA, June 1998, L. M. Haas and A. Tiwary, Eds. ACM, New York, NY, 460–471. ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004. 328 • R. Barga et al. LOMET, D. AND TUTTLE, M. 1999. Logical logging to extend recovery to new domains. In Proceedings of 1999 ACM SIGMOD International Conference on Management of Data, Philadelphia, PA, June 1999, A. Delis, C. Faloutsos, S. Ghandeharizadeh, Eds. ACM, New York, NY, 73–84. LUO, M.-Y. AND YANG, C.-S. 2001. Constructing zero-loss Web services. In Proceedings IEEE INFOCOM 2001 of the 20th Joint International Conference of the IEEE Computer and Communication Societies on Computer Communications, Anchorage, AK, April 2001. IEEE, Los Alamitos, CA, 1781–1790. MOHAN, C., ET AL. 1992. ARIES: A transaction recovery method supporting fine-granularity locking and partial rollback using write-ahead logging. ACM Trans. on Database Syst. 17, 1, 94–162. MSDN LIBRARY: PERSISTENCE. Microsoft Internet Explorer Persistence Overview. http://msdn. microsoft.com/workshop/author/persistence/overview.asp. MSDN LIBRARY: .NET. .NET Remoting Overview. http://msdn.microsoft.com/library/default.asp? url=/library/en-us/cpguide/html/cpconnetremotingoverview.asp. MSR PHOENIX. Phoenix: Making Applications Robust. http://www.research.microsoft.com/research/ db/phoenix/. OMG: CORBA 2000. Fault Tolerant CORBA Spec V1.0. http://cgi.omg.org/cgi-bin/doc?ptc/00-04- 04. OMG: UML 1999. OMG Unified Modeling Language (UML) Version 1.3.http://www.rational. com/uml. PEDREGAL-MARTIN, C. AND RAMAMRITHAM, K. 1999. Recovery guarantees in mobile systems. In Proceedings of the ACM International Workshop on Data Engineering for Wireless and Mobile Access, Seattle, WA, August 1999. ACM, New York, NY, 22–29. PEDREGAL-MARTIN, C., RAMAMRITHAM, K. 2001. Guaranteeing recoverability in electronic commerce. In Proceedings of the 3rd International Workshop on Advanced Issues of E-Commerce and Web-based Information Systems, San Juan, CA, June 2001. IEEE Computer Society, Los Alamitos, CA, 144–155. PHP. PHP Documentation and Downloads. http://www.php.net. POPOVICI, A., SCHULDT, H., AND SCHEK, H.-J. 2000. Generation and verification of heterogeneous purchase processes. In Proceedings of the 1st International Workshop on Technologies for EServices, Cairo, Egypt, September 2000, 5–22. SCHULDT, H., POPOVICI, A., AND SCHEK, H.-J. 2000. Automatic generation of reliable e-commerce payment processes. In Proceedings of the 1st International Conference on Web Information Systems Engineering, Hong Kong, China, June 2000, Q. Li, Z. M. Ozsoyoglu, R. Wagner, Y. ¨ Kambayashi, and Y. Zhang, Eds. IEEE Computer Society, Los Alamitos, CA, 434–441. SHEGALOV, G., WEIKUM, G., BARGA, R., AND LOMET, D. 2002. EOS: Exactly-once E-Service Middleware (Demo Paper). In Proceedings of the 28th International Conference on Very Large Data Bases, Hong Kong, China, August 2002, P. A. Bernstein, Y. E. Ioaninidis, R. Ramakrishnan, D. Papadias, Eds. Morgan Kaufmann, 1043–1046. SUN 2001. Enterprise Java Beans Specification, Version 2.0, http://java.sun.com/products/ ejb/docs.html. TYGAR, J. D. 1998. Atomicity versus anonymity—Distributed transactions for electronic commerce. In Proceedings of the 24th International Conference on Very Large Data Bases, New York, NY, August 1998, A. Gupta, O. Shmueli, and J. Widom, Eds. Morgan Kaufmann, 1–12. WEIKUM, G. AND VOSSEN, G. 2001. Transactional Information Systems—Theory, Algorithms, and the Practice of Concurrency Control and Recovery. Morgan Kaufmann, San Francisco, CA, 2001. WILLIAMS, M. 2002. Microsoft Visual C# .NET. Microsoft Press, Redmond, WA. ZEND. Zend Engine. http://www.zend.com. Received February 2003; revised April 2003; accepted July 2003 ACM Transactions on Internet Technology, Vol. 4, No. 3, August 2004.Static Typing for Ruby on Rails Jong-hoon (David) An Computer Science Department University of Maryland College Park, USA davidan@cs.umd.edu Avik Chaudhuri Computer Science Department University of Maryland College Park, USA avik@cs.umd.edu Jeffrey S. Foster Computer Science Department University of Maryland College Park, USA jfoster@cs.umd.edu Abstract—Ruby on Rails (or just “Rails”) is a popular web application framework built on top of Ruby, an objectoriented scripting language. While Ruby’s powerful features such as dynamic typing help make Rails development extremely lightweight, this comes at a cost. Dynamic typing in particular means that type errors in Rails applications remain latent until run time, making debugging and maintenance harder. In this paper, we describe DRails, a novel tool that brings static typing to Rails applications to detect a range of run time errors. DRails works by translating Rails programs into pure Ruby code in which Rails’s numerous implicit conventions are made explicit. We then discover type errors by applying DRuby, a previously developed static type inference system, to the translated program. We ran DRails on a suite of applications and found that it was able to detect several previously unknown errors. Keywords-Ruby; Ruby on Rails; scripting languages; type systems; web frameworks I. INTRODUCTION Web application frameworks have become indispensable for rapid web development. One very popular framework is Ruby on Rails (or just “Rails”), which is built on top of Ruby, an object-oriented scripting language. While Ruby allows development in Rails to be extremely lightweight, Ruby’s dynamic typing means that type errors in Ruby programs, and hence Rails programs, can remain latent until run time. Recently, we have been developing Diamondback Ruby (DRuby), a new static type inference system for ordinary Ruby code [1], [2]. We would like to bring the same type inference to Rails to catch common programming bugs in Rails programs. Unfortunately, by itself, DRuby would be essentially useless on Rails code. The Rails framework uses a significant amount of highly dynamic, low-level class and method manipulation as part of its “convention over configuration” [3] design. This kind of code is essentially unanalyzable with DRuby (and with any typical static analysis). We also cannot simply omit the framework during analysis of an application, since we would then miss most of the application’s behavior. In this paper, we address this issue with DRails, a novel tool that brings DRuby’s type inference to Rails. The key insight behind DRails is that we can make implicit Rails conventions explicit through a Rails-to-Ruby transformation, and then analyze the resulting programs with DRuby. Type errors in the transformed programs indicate type errors in the original Rails applications. As far as we are aware, DRails is the first tool to bring static typing to Rails. Furthermore, we expect that DRails’s transformation can serve as a frontend for other static analyses on Rails programs, and the idea of analyzing programs by transformation can be applied to other code development frameworks as well. We evaluated DRails by running it on a suite of 11 Rails programs gathered from a variety of sources. DRails found 12 previously unknown errors that can cause crashes or unintended behavior at run time. DRails also identified 2 examples of questionable coding practice. The fact that DRails could find these errors is particularly surprising since Rails applications are often thoroughly tested during development using Rails’s built-in testing infrastructure. Furthermore, DRails reported 57 false positives; about half of them were due to known incompleteness issues in DRuby, and we expect most of the others to be eliminated with minor extensions to DRails. II. OVERVIEW Rails is built on top of Ruby, an object-oriented scripting language [4]. Rails uses a model-view-controller (MVC) architecture [5], in which any web request by the client results in a call to some method in a controller, which in turn uses a model to perform database accesses and eventually returns a view, i.e., the text of a web page, as the response. To illustrate how Rails works and the challenges of reasoning about Rails applications, we will develop a small program called catalog that maintains an online product catalog. The database for catalog tracks a set of companies, each of which has a set of products. In turn, each product has a name plus a longer textual description. Internally, catalog has two models (Company and Product), two controllers (CompaniesController and ProductsController), and one view (views/companies/info.html.erb). We next discuss the code for these various components, potential bugs that might appear in this simple application, and how DRails would help detect these errors statically. Due to space constraints, our discussion is incomplete; more details can be found in our companion technical report [6]. 2009 IEEE/ACM International Conference on Automated Software Engineering 1527-1366/09 $29.00 © 2009 IEEE DOI 10.1109/ASE.2009.80 578 592 590 db/schema.rb 1 create table ‘‘companies’’ do |t| t.string ‘‘name’’ end 2 create table ‘‘products’’ do |t| 3 t.integer ‘‘company id’’ 4 t.string ‘‘name’’ 5 t.string ‘‘description’’ 6 end models/company.rb 7 class Company < ActiveRecord::Base 8 has many :products 9 validates uniqueness of :name 10 end controllers/companies controller.rb 11 class CompaniesController < ActionController::Base 12 def info() 13 @company = Company.find by name (params[:name]) 14 end 15 end views/companies/info.html.erb 16 <h2><%= @company.name %></h2> 17 <% @company.products.each do |product| %> 18 <p><%= product.name + ”:” + product.description%></p> 19 <% end %> controllers/products controller.rb 20 class ProductsController < ActionController::Base 21 before filter :authorize, :only ⇒ :change 22 def info 23 company = Product.find(params[:id]).company 24 redirect to :controller ⇒ ‘‘companies’’, :action ⇒ ‘‘info’’, 25 :name ⇒ company.name 26 end 27 def change 28 @product.description = params[:description] 29 @product.save 30 info 31 end 32 private 33 def authorize 34 @product = Product.find(params[:id]) 35 return @product.company.name == session[:user] ? nil : info 36 end 37 end Figure 1. Rails application catalog source code A. Models The first listing in Fig. 1 shows db/schema.rb, which is a Ruby file that is auto-generated from the database table. (The code for a Rails application is split across several subdirectories, including db/ for the database, and models/, views/, and controllers/ for the correspondingly named components.) This file records the names of the tables and the fields of each row: the companies table has a name field, and the products table has name and description. In Rails, each row in a table is mirrored as an instance of a model class (or, just “model”), which must be defined by a file in the models/ directory. The second listing in Fig. 1 shows the Company class, corresponding to the companies table. (We omit the Product class from the figure due to lack of space.) Note the singular/plural relationship between model (Company) and table (companies) names. Rails uses the information from schema.rb to automatically add field setter and getter methods to the models, among other things. For example, it creates methods name() and name=() to Company to get and set the corresponding field name. Because these and other methods are created implicitly by Rails, and since Ruby has no static type checking, it is easy to make a mistake in calling such a method and not realize it during development. In DRails, we transform the initial program, explicitly generating Ruby code corresponding to auto-generated methods. For example, the Company model is modified as follows: class Company < ActiveRecord::Base attr accessor :id, :name ... The calls to attr accessor create methods to read and write fields @id and @name. We pass the transformed code to DRuby, which can then check that uses of these accessors are type correct. Models not only have methods added to them based on the database schema, but they also inherit from the Rails class ActiveRecord::Base (as shown on line 7; < indicates inheritance). This class defines a variety of useful methods, including several that tell Rails about relationships between tables. In our example, each company can have many products, indicated by the call on line 8, which adds methods products() and products=() (note the pluralization) to Company. For these methods to function, Rails requires that the company id field declared on line 3 exist. To type programs that use this feature or similar features, DRails needs to add the implied method definitions and the implied calls to the program. For example, the has many call on line 8 is transformed into the following set of type annotations: class Company < ActiveRecord::Base ##% products : () → HasManyCollection<Product> ... ##% products= : \ ##% (Array<Product>) → HasManyCollection<Product> ... end Here the getter method products returns a collection of Product objects. The setter method products= takes an array of Products objects and returns the updated collection. Next, if a model instance is updated or created, the save() method (inherited from ActiveRecord::Base) is called to commit it to the database. This method will reject objects whose validation methods fail. For example, line 9 calls validates uniqueness of :name to require the name 579 593 591 field of a company to be unique across all companies. Programmers can also define custom validation methods that include arbitrary Ruby code, but we have omitted this due to space limitations. DRails ensures that such behaviors are correctly captured in the analysis by inserting explicit calls to validation methods in appropriate places. There are also a few other implicit model conventions that DRails makes explicit. One important case is find by x(y), which, if called, returns the first occurrence of a record whose x field has value y, as shown in line 13 of Fig. 1. There is one such method, plus one find by all x method, for each possible field. DRails adds type annotations for these methods to the model, e.g., since Company has a field name, DRails adds annotations for find by name and find all by name to class Company. B. Controllers and Views In Rails, the actions available in a web application are defined as methods of controller classes. The third listing in Fig. 1 shows CompaniesController, which, as do other controllers, inherits from ActionController::Base. This controller defines an action info that allows clients to list the products belonging to a particular company. This action is invoked whenever the client requests a URL beginning with “server/companies/info”, and it expects a parameter name to be passed as part of the POST or GET request. When info is called, it finds the Company row whose name matches params[:name], the requested name, and stores it in field @company (line 13). The last step of an action is often a call to render, which displays a view. In this case, info includes no such call, so Rails automatically calls render :info to display the view with the same name as the controller. The corresponding view is shown as the fourth listing in Fig. 1. As is typical, this view is written as an .html.erb file, which contains HTML with embedded Ruby code. Here, text between <% and %> is interpreted verbatim as Ruby code, and text between <%= and %> is interpreted as a Ruby expression that produces a string to be output in the resulting web page. For example, line 16 shows a second-level heading whose content is the value of @company.name; recall @company was set by the controller, so it is an interesting design decision that Rails allows it to be accessed here. Similarly, lines 17–19 contain Ruby code to iterate through the company’s products and render each one. The last listing in Fig. 1 defines a more complex controller, ProductsController, with several actions. The first one, info (lines 22–26), computes the company of the product given by the parameter id and then uses redirect to to pass control to the info action of CompaniesController (lines 11– 15), specifying the company’s name. As we discussed above, this in turn calls render :info (lines 16–19). It is possible to call redirect to several times before eventually calling render, allowing control to flow through several controllers before eventually displaying a view. The change action (lines 27–31) allows a product description to be updated. However, we only want to allow authorized users to make such changes. Thus, on line 21 we call before filter to specify that the authorize action should always be run before change. Note that authorize is declared private (line 32), so it cannot be called directly as an action. When authorize is called, it looks up the product to be modified (line 34) and checks whether the user logged into the current session (stored in session[:user]; this is established elsewhere (not shown)) matches the name of the company of that product (line 35). If so, then authorize evaluates to nil, and control passes to change, which updates the product description (line 28), commits the change to the database (line 29), and then calls info to show the product listing screen (line 30). Otherwise, authorize calls info, and since that ends in a redirect to, the action change will never be rendered. Like models, controllers and views can have errors that are hard to detect. First, view file names could have the wrong extension, in which case Rails may be unable to find them, causing crashes or unintended behavior. Second, a (perhaps implicit) call to render could go to a non-existent view. Third, as control flows get complex, with actions inserted before other actions with filters, and actions in one controller calling actions in another, it is easy to make a typo in the method name for a filter. For example, writing :authorized instead of :authorize on line 21 will crash the program. Or, making a mistake in a redirect to call (say, by writing ‘‘company’’ instead of ‘‘companies’’ on line 24, or @company = ... rather than company = ... on line 13) will also result in an unexpected behavior. Analogously to the transformation of models, DRails performs code transformation for views and controllers. To fully reason about views, we first need to able to analyze the Ruby code embedded in HTML. Our solution is to use Markaby for Rails [7] to parse the views and produce regular Ruby classes that generate the same dynamic web pages. (Note that while Markaby worked as-is on small examples, we needed to make major improvements to apply it to our suite of programs in Section III.) We call this process Rubyifying the view. For example, here is the result of Rubifying views/companies/info.html.erb of Fig. 1, slightly simplified for discussion purposes: module CompaniesView include ActionView::Base def info Rubify.h2 do Rubify.text(@company.name) end @company.products.each do |product| Rubify.p do Rubify.text(product.name + ”:” + product.description) end end end end Here the method name info is based on the view name info. The calls to Rubify’s methods output strings containing the appropriate HTML; notice that the calls are intermixed with 580 594 592 regular Ruby code. For example, Rubify.h2 do... end creates the second-level heading on line 16 of Fig. 1. We created this method as part of module CompaniesView, where the module name was derived from the file’s location under views/. Rails does approximately the same thing, implicitly creating a CompaniesView class from the view. Summing up, even an application as simple as catalog contains many opportunities for error. DRails can find many Rails errors by transforming the original program and running DRuby on the result. III. IMPLEMENTATION AND EXPERIMENTS DRails comprises approximately 1,700 lines of OCaml and 2,000 lines of Ruby. DRails begins by combining all the separate files of the Rails application into one large program. DRails parses the program into the Ruby Intermediate Language (RIL), a subset of Ruby that is designed to be easy to analyze and transform [8]. Then DRails instruments this program to capture arguments passed to Rails API calls. The program is loaded with Ruby, and the resulting instrumentation output is fed back into DRails and used to transform the combined program, making uses of Rails’s conventions explicit. This transformed program is passed to DRuby along with base.rb, a file that gives type signatures to remaining Rails API methods, and stub files containing type signatures for any external libraries. DRuby performs type inference and emits warnings for any errors it finds. We evaluated DRails by running it on 11 Rails applications that we obtained from various sources including RubyForge and OpenSourceRails. Fig. 2 summarizes our results. The first group of columns gives the size of each application, in terms of source code lines (counted with wc); the size in kilobytes of the RIL control-flow graph after parsing the model, controllers, and similar files and Rubifying the views; and the size in kilobytes of the RIL control-flow graph after full transformation. This increase shows that there is a significant amount of code that Rails produces by convention. Due to current limitations of DRails, we needed to make some small changes to the applications. We manually closed unbalanced HTML tags so that Rubified code is well-formed (R), flattened nested directory structures to avoid confusion in matching class names to files (H), transformed non-literal arguments to render and redirect to into case statements to capture all routing behaviors (I), and manually added required files to config/environment.rb (B). The results of running DRails on these programs are tabulated in the last two groups of columns in Fig. 2. We ran DRails on an AMD Athlon 4600 processor with 4GB of memory. We break down the running times of DRails into DRails-only time, DRuby time, and the total time. The reported running times are the average of three runs. The DRails-only step is fairly fast across all the applications, and most of the running time is due to DRuby. We manually categorized DRuby’s error reports into four categories: errors (E), reports that corresponds to bugs that may crash the program at run time or cause unintentional behavior; warnings (W), reports for code that behaves correctly at run time, but uses suspicious programming practice; deprecated (D), reports of uses of Rails features no longer available in Rails 2.x; and false positives (F) that do not correspond to actual bugs. We found 12 errors in the applications. Eight of the errors, six in lohimedia and two in onyx, are due to programmer misunderstandings of Ruby’s syntax. For example, lohimedia contains the code: flash[:notice] = ‘‘You do not have...’’ + ”...” Here the programmer intends for the string on the second line to be concatenated with the first line. In Ruby, however, line breaks affect parsing, so the string on the first line is assigned to flash[:notice]. Then the second line results in a call to the unary method + with a string argument, which is a type error. Because Ruby is dynamically typed, errors like this can remain latent until run-time, whereas DRuby (and DRails) can find such bugs statically. The other two errors in onyx are due to the following embedded Ruby code: <% @any more = Post.find(:first, :offset ⇒ (@offset.to i + @posts per page.to i) + 1, :limit ⇒ 1)%> Here DRuby reports that Post, which the programmer seems to be treating as a model, is undefined, as indeed it is. One error in diamondlist is due to invoking the nonexistent method << on a Hash. (A method with that name does exist in Array, perhaps explaining the error.) The other error in diamondlist occurs in call to render in which the specified view, top bar, does not exist. Finally, boxroom has an interesting error in one of its models due to a call to an undefined method password confirmation. This method name is commonly used by convention in Rails applications, but it is only available if the user declares both password and password confirmation fields, usually by calling attr accessor. However, in this case the programmer instead calls attr accessible on these fields, which has completely different semantics. We found 2 warnings in total. The first warning is due to a call to a method that is not in Rails documentation but actually part of a Rails definition. We are not sure whether this method should have been documented or is meant to be private. The second warning occurs due to a method call whose block argument has the wrong type signature [1]. We found 72 uses of deprecated constructs that operate correctly on older versions of Rails but cause run-time errors on Rails 2.x. benchmarks. DRails reported 57 false positives due to: limitations in DRuby’s annotation language; features not handled by DRails; inconsistent variable scoping due to 581 595 593 CFG sizes (kb) Patches (#) Running times (s) Errors (#) LoC Before After R H I B DRails DRuby Total E W D F depot 997 139 358 · · · 1 2.30 9.74 12.04 · 1 1 1 moo 838 143 402 4 · · 3 2.45 18.76 21.21 · · · 3 pubmgr 943 196 548 · · · 1 3.00 26.41 29.41 · · · · rtplan 1,480 273 697 · · · 2 3.47 26.65 30.12 · · 6 1 amethyst 1,183 264 729 · · · 4 3.53 39.03 42.56 · · · 1 diamondlist 1,415 265 786 4 2 · 1 4.10 23.81 27.91 2 · · 2 chuckslist 1,447 329 883 1 · 9 4 4.08 52.23 56.31 · 1 2 14 boxroom 2,330 376 959 6 · 1 2 4.16 87.23 91.39 1 · 27 6 onyx 2,228 484 1,190 6 1 · 1 5.62 79.75 85.37 3 · · 1 mystic 2,822 639 1,525 13 · 5 1 6.38 146.40 152.78 · · · 11 lohimedia 11,106 1,290 3,331 9 · 2 3 14.01 662.95 676.96 6 · 36 17 Figure 2. Experimental results newly introduced Ruby code during Rubification; and runtime type tests that DRuby cannot analyze. Threats to Validity We should emphasize that DRails by no means checks for all possible errors in Rails programs, e.g., clearly Rails programs can have errors that are unrelated to types. Beyond that, there are several potential threats to the validity of our experimental results. First, our type signatures for the Rails API could be overly general, allowing calls that might fail at runtime. Second, DRails’s modeling of the Rails API is incomplete and could be slightly inaccurate. Third, our categorization of some of DRails’s errors might be incorrect, e.g., we may have classified code as erroneous that actually behaves correctly at runtime. Finally, there could be bugs in DRuby that cause it to unsoundly miss type errors. IV. RELATED WORK Most existing work on static analysis of web applications focuses on verification of security properties. Lam et al. [9] combine static analysis with model checking to verify that information-flow patterns are satisfied in Java-like programs. Huang et al. [10] use a lattice-based static analysis algorithm derived from type systems and typestate to ensure similar information-flow properties. The tool TAJ [11] performs taint analysis of web applications written in Java, and uses novel program slicing techniques to handle reflective calls and flows through containers. The tool Pixy [12] performs alias analysis for PHP and finds security vulnerabilities in web applications written in PHP. Xie and Aiken [13] address the same problem, and present a static analysis algorithm based on symbolic evaluation to handle dynamic features of PHP. The key differences between all of these systems and DRails is our focus on static typing and Ruby on Rails, a combination we believe we are the first to study. V. CONCLUSION In this paper, we presented DRails, a novel static analysis tool for Rails applications. DRails works by translating Rails applications into pure Ruby code in which the automation provided by Rails’s sophisticated internal machinery is made explicit. We then apply DRuby, a static type inference system for Ruby, to the result. We showed that static typing catches a variety of bugs in Rails applications, and we believe we are the first to bring static typing to Rails. REFERENCES [1] M. Furr, J. An, J. S. Foster, and M. Hicks, “Static Type Inference for Ruby,” in OOPS Track, SAC, 2009. [2] M. Furr, J. An, and J. S. Foster, “Profile-guided static typing for dynamic scripting languages,” in OOPSLA, 2009, to appear. [3] “Ruby on Rails,” 2009, http://rubyonrails.org. [4] D. Flanagan and Y. Matsumoto, The Ruby Programming Language. O’Reilly Media, Inc, 2008. [5] E. Gamma, R. Helm, R. Johnson, and J. Vlissides, Design Patterns. Addison-Wesley, 1995. [6] J. An, A. Chaudhuri, and J. S. Foster, “Static typing for ruby on rails,” University of Maryland, College Park, Tech. Rep., 2009, http://www.cs.umd.edu/∼davidan/papers/drails.pdf. [7] “Markaby for Rails,” 2006, http://redhanded.hobix.com/ inspect/MarkabyforRails.html. [8] M. Furr, J. An, J. S. Foster, and M. Hicks, “The ruby intermediate language,” in DLS, 2009, to appear. [9] M. S. Lam, M. Martin, B. Livshits, and J. Whaley, “Securing web applications with static and dynamic information flow tracking,” in PEPM, 2008, pp. 3–12. [10] Y.-W. Huang, F. Yu, C. Hang, C.-H. Tsai, D.-T. Lee, and S.-Y. Kuo, “Securing web application code by static analysis and runtime protection,” in WWW, 2004, pp. 40–52. [11] O. Tripp, M. Pistoia, S. J. Fink, M. Sridharan, and O. Weisman, “TAJ: Effective taint analysis for Java,” in PLDI, 2009, to appear. [12] N. Jovanovic, C. Kruegel, and E. Kirda, “Precise alias analysis for static detection of web application vulnerabilities,” in PLAS, 2006, pp. 27–36. [13] Y. Xie and A. Aiken, “Static detection of security vulnerabilities in scripting languages,” in USENIX Security, 2006, pp. 179–192. 582 596 594GAINS: Genetic Algorithms for Increasing Net Sales of a Mobile Reverse Demand Communication System Michael H Wolk Department of Computer Science Colby College Waterville, ME 04901 mhwolk@colby.edu ABSTRACT In this paper, I describe MRDCOM, a mobile reverse-demand communication system for the pizza industry. The web-based system will support student (buyer) sign-up and will be capable of sending buyers discount food offers through mobile phone text messaging technology. Pizza sellers will be able to easily purchase pizza sales offers that will then be sent only to a relevant sub-group of users. A component of MRDCOM, GAINS, is an offline genetic algorithm (GA) learning component designed to model the offers that are accepted. The rule set derived from the GA component will be used to raise user satisfaction by sending offers to users most likely to accept them. GAINS is expected to increase the accepted offer rate will provide economic benefits for students, pizza sellers, and the administrators of the MRDCOM system. Categories and Subject Descriptors I.2.6[Artificial Intelligence]: Learning. General Terms Algorithms, Design, Economics, Experimentation, Human Factors. Keywords food, Genesis, genetic algorithms, Internet, pizza, Ruby On Rails. 1. INTRODUCTION 1.1 Motivation College students consume a lot of pizza. Students are constantly ordering pizza products for delivery, and most colleges have a considerable number of pizza sellers clustered within the general vicinity of campus. Like any industry, pizza restaurants are sometimes subjected to drops in demand that can lead to lower than expected revenues. The proposed reverse demand communication system is designed to help mitigate revenue losses by facilitating the communication of discount product offers to students' mobile phones. Some of the system design choices have been guided by the goal of offering a product that is economically advantageous to student buyers, pizza sellers, and the operator of the system. Therefore, the intent is to offer a strong return on investment (ROI) to pizza restaurants and to provide student buyers with a good deal. It is in the best interest of pizza restaurants and the system operator to retain as many users as possible. The more users, the greater the likelihood of finding individuals interested in buying pizza at all times. I propose a business model of charging a small fee per number of pizzas that a restaurant chooses to sell. Thus, it is advantageous to investigate how to minimize student buyer account cancellations (rate of attrition). In particular, I wish to investigate ways to send pizza offers primarily to students likely to find them relevant at a particular point in time. Most mobile phone owners must pay to receive text messages in the United States, and I want to diminish attrition by sending offers they are likely to consider relevant. 1.2 Project Goals The Mobile Reverse Demand Communication System (MRDCOM) is a testable prototype capable of facilitating business-side offer purchases and buyer-side responses to offers. I have added Genetic Algorithms for Increasing Net Sales (GAINS) to MRDCOM, to investigate the extent to which genetic algorithms might be used to assist in determining who and how many people should be sent a given offer. Figure 1. A cheese pizza like those offered through MRDCOM. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. GECCO'07, July 7-11, 2007, London, England, United Kingdom. Copyright 2007 ACM 978-1-59593-698-1/07/0007...$5.00. 2961 2. BACKGROUND I have not been able to find a similar system to the reverse demand communication system, but there are web-based systems with related features. AccuWeather, a weather forecasting company sells "AccuWeather.com Messenger™", a pay text message service that sends weather forecasts to subscribers' mobile phones. While subscription statistics are unavailable, this suggests there is a market for services utilizing text-messaging technology. Businesses such as Priceline.com and Hotwire.com are examples of alternative sales systems that have succeeded in the marketplace. Priceline's 2006 gross profit of over $400 million USD suggests a significant level of consumer adoption. The mobile reverse demand communication system is not a so-called opaque sales system like Priceline's Name Your Own Price® product, but the hotel rooms sold on Priceline are discounted because of low demand much like the pizzas offered through MRDCOM. In the case of low demand for pizza, however, sales must be made within minutes instead of the lag of several days possible for hotel room sales. Figure 2. Priceline.com offers steep discounts through a nontraditional purchasing system, 2007. The use of GAs was supported by literature [1] which suggested their viability for classification tasks. I also found further materials [2] which supported this assertion. 3. SYSTEM DESIGN 3.1 Ruby on Rails The Ruby on Rails framework [3] is a web development framework written in the object oriented Ruby Programming Language. Ruby itself is simple to use, and I found Ruby code easier to modify than alternatives like Java because of the fewer characters needed to express concepts as well as the code’s higher readability. The Ruby on Rails framework was chosen because of its many built-in features. While there is a learning curve, I believe that I have saved time by using the framework to implement the mailing system and some of the basic create, read, update, and delete (CRUD) functionality of the system. Furthermore, I have taken advantage of the framework’s ActiveRecord system. By using ActiveRecord, I was able to set up a back-end SQL database without having to write one line of SQL. The time saved there has been invested in implementing more sophisticated parts of the system. 3.2 Genesis I am using Genesis [4], the Genetic Search Implementation System, to implement the GAINS component of MRDCOM. 3.3 System Implementation The system was designed to be visually appealing to users familiar with Web 2.0 sites. I modified an external layout in order to achieve my aesthetic goals. Overall, I designed simple site pages and maintained a minimalist design philosophy. The system is not intended to be a content provider or a destination site. Its dual purposes are to allow users to enter and update their personal preferences and to provide a place for students to quickly accept offers. 3.4 Data Models Colleges are defined by a college name, zip code, city name, and state abbreviation. They are related to dorms, which belong to a specific college. A dorm is defined by a name and the key value of its associated college. I implemented the system so that multiple colleges can have dorms with the same name, but no college can have more than one dorm with a specified name. While the use of zip codes and state abbreviations limits the system to the U.S. university system, it would be trivial to modify the college model to accommodate for schools in places outside of the United States like Canada. This change, however, is beyond the goals of the current system. Mobile providers must be modeled so that I can build the email addresses needed to send text messages. A text message can be sent to a mobile phone by sending an email to the concatenation of the phone number and a carrier-specific string. As a result, buyers must select a mobile phone provider during the sign-up process. The mobile provider data model contains a name for the phone provider and an email string which is the text appended to a user's phone number in order to produce a valid email address. A student model representing a student buyer holds relevant data including college, dorm, mobile phone number, mobile phone provider, and preference information on when the user is willing to accept offers. The student model also includes demographic information including computer operating system, class year, gender, and academic area of study. Academic area of study is more broad than a major. They include: The Arts, Natural Science, Social Science, Mathematics, and Engineering. For each offer, a user who is sent a text message is given an offer transaction entry. These entries specify the seller of the pizza, the time of the offer, and whether or not the user accepted the offer. 3.5 GAINS and Algorithms The prototype system has been simplified to sell only cheese pizzas at one price. This simplifying assumption made it possible to get a basic version of MRDCOM running within a shorter time-frame than if I tried to implement everything at once. Later versions, however, will possibly include variable pricing and a range of products. 2962 A simple pizza selection algorithm would send an offer to a portion of the users who have indicated in their time preferences that they are interested in receiving offers. Simple Algorithm Use Case 1. A pizza restaurant requests to sell n pizzas to college x within the next ten minutes. 2. The system searches the user database and selects all students of college x who have agreed to receive offers at the current time. 3. The system selects a subset of the students from Step 2 to receive offers. 4. Offer text messages are forwarded to the students selected in Step 3. Step 3 of the simple algorithm is intentionally vague. One basic implementation for the selection process would be to randomly choose a subset of students to receive an offer. A random algorithm might at times not send enough offers that are needed to sell all pizzas while at other times it might send too many. One significant issue is that this algorithm might consistently send offers to a user who always declines them. A more informed implementation could use user data and past behavior to better predict the number of offers to be sent as well as the users who should receive them. GAINS uses genetic algorithms to analyze past behavior and help predict future user decisions. MRDCOM is configured to generate a file containing data corresponding to each offer message sent to student users. The data captures time information, a user's demographic data, and whether the offer was accepted or ignored. Table 1 shows the structure of the bit strings that GAINS uses to model the offer data. The day of the week and time are encoded as simple binary strings. Days are encoded using a string of seven bits where one is selected. “1000000” represents Monday and “0010000” refers to Wednesday. The hour of the day is noted the same way but with a 24 bit string. This bit encoding allows the GA string to specify, for example, a subset of offers on Mondays and Wednesdays at 7, 8, and 9pm, specifying more than one possibility for each attribute. A student's class year has six possible options: Freshman, Sophomore, Junior, Senior, Graduate, and Other. A PC Preference attribute captures whether a user claims to use a Windows, Mac, or Other operating system. Students are required to select their major area from a list of general areas of study. Location types describe the surrounding environment of a school and include: rural, town, city, and big city. Finally, the bit string states whether the offer was accepted. The GA string totals 52 bits. Table 1. The GA string representation 0 1 2 3 4 Day Of Week (7) Time (24) Class Year(6) Gender(2) PC-Pref(3) 5 6 7 Area of Study (5) LocationType(4) Offer Accepted(1) The use of class year, gender, and especially computer preference may seem strange. Including class year makes it possible to account for different user behavior over time. If, for example, younger students are more comfortable with using MRDCOM, this is a way to capture this difference in behavior. Similarly, it is possible that there is some difference in buying habits between genders and accounting for this information may help improve the user selection process. Finally, PC preference is a way to indirectly divide users into different groups. There has been research [5] supporting differences in the buying behavior of PC and Mac users. It might be the case that there are unique buying and lifestyle characteristics for owners of different types of computers. Perhaps Linux users stay up all night and love pizza. If this is the case, the PC preference variable will help to capture these behaviors. The GAINS component of the system consists of learning a rule set to describe the buyer groups most likely to accept offers under different circumstances. GAINS uses the GA string representation illustrated in Table 1 to evolve descriptions of subsets of the data; odds ratio [6] is used as the fitness function. As a preliminary step in development, GAINS finds the one best rule (describing the strongest subset in the data); future work will extend GAINS to find complete rule sets. The data for GAINS consists of information about a specific offer and the user who was sent the offer. This includes the time of the offer, whether the offer was accepted, the school where the offer was sent, and demographics on the user. These consist of the demographic qualities of the student model. 4. RESULTS Although still in development, GAINS is expected to increase the relevance of system offers by sending the right offers to the right student users. At this time, I do not have real use data to test GAINS and instead am testing if it is capable of making these distinctions using simulated data. I am creating a large artificial data set of user behavior and plan on giving different demographic groups arbitrary behavioral patterns. For example, city dwelling male Mac users might generally accept offers from only nine to eleven in the evening. By creating different demographic profiles, I wish to test and refine GAINS to determine if it is able to identify patterns that can improve the offer group selection process. 5. CONCLUSION MRDCOM is a unique solution to a problem that exists in many industries. My implementation is designed to target the college pizza market. Yet, there are other markets, such as the electrical power industry, that could also benefit from software that is capable of linking price-sensitive buyers and sellers during momentary drops in demand. I believe that a large student user base is key to making MRDCOM a success. Furthermore, I would like to have a diverse population of users so that pizza restaurants have a better likelihood of selling pizzas at all times. These two qualities are the motivating forces behind GAINS. With the GAINS component, the system can learn how different demographic characteristics affect behavior and then use this knowledge to guide the selection of users to receive offers. If GAINS can learn useful rule sets, it may be able to mitigate attrition and therefore improve MRDCOM. 2963 6. ACKNOWLEDGMENTS I would like to thank Clare Bates Congdon and Adam Fischbach for all their help and support. I would also like to thank the developers of Ruby, the Ruby on Rails framework, and Genesis. The cheese pizza photo is by flickr user Roboppy and is licensed under a Creative Commons Attribution-NonCommercial 2.0 license. 7. REFERENCES [1]Goldberg, D. E. 1989 Genetic Algorithms in Search, Optimization and Machine Learning. 1st. Addison-Wesley Longman Publishing Co., Inc. [2] Holland, John. Genetic Algorithms, in Scientific American, July 1992. [3] Ruby on Rails web site. web-based information repository, 2007. http://www.rubyonrails.org [4]J. J. Grefenstette. A user’s guide to GENESIS. Technical report, Navy Center for Applied Research in AI, Washington, DC, 1987. Source code available at http://www.cs.cmu.edu/afs/ cs/project/ai-repository/ai/areas/genetic/ga/systems/genesis/. [5] Fried, Ian. Are Mac users smarter?, News article, CNET News.com, 2002. Article available at http://news.com.com/2100-1040-943519.html/. [6]M. S. Kramer, Clinical Epidemiology and Biostatistics: A Primer for Clinical Investigators and Decision-Makers. Berlin: Springer-Verlag, 1988. 2964 