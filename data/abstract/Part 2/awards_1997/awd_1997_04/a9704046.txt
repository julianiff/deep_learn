Title       : SGER: Using Text Coherence and Verbal Valence in Long- Distance N-grams
Type        : Award
NSF Org     : IIS 
Latest
Amendment
Date        : December 26,  1996  
File        : a9704046

Award Number: 9704046
Award Instr.: Standard Grant                               
Prgm Manager: Gary W Strong                           
	      IIS  DIV OF INFORMATION & INTELLIGENT SYSTEMS
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : January 15,  1997   
Expires     : December 31,  1997   (Estimated)
Expected
Total Amt.  : $50000              (Estimated)
Investigator: Daniel S. Jurafsky jurafsky@colorado..edu  (Principal Investigator current)
Sponsor     : U of Colorado Boulder
	      3100 Marine Street, Room 481
	      Boulder, CO  803090572    303/492-6221

NSF Program : 6845      HUMAN COMPUTER INTER PROGRAM
Fld Applictn: 0104000   Information Systems                     
Program Ref : 9216,9237,HPCC,
Abstract    :
              ***  Building better speech recognizers requires augmenting n-gram  grammars
              with sophisticated yet probabilistic linguistic  knowledge.  This project is
              building probabilistic models of two  important pieces of syntactic/semantic
              knowledge: verb-argument  constraints and semantic text coherence.(1) Verbs
              place strong  constraints on the syntax and semantics of their arguments.  This
               project is computing probabilities for the different argument  structures that
              can co-occur with different verbs, and using  these probabilities to augment
              standard trigram language  models.(2) Texts and discourses tend to be
              semantically  coherent;in particular the words that occur in a text tend to be 
              semantically related to each other.  This project is applying a  model of word
              meaning called Latent Semantic Analysis (LSA) to  ASR LMs.  In LSA, a
              word-similarity metric is defined by  computing a large matrix of word
              co-occurrence probabilities,  which are then smoothed via Singular Value
              Decomposition,  resulting in a generalized measure of semantic word-similarity.
               Trigram models can then increase the probability that similar  words will
              occur near each other.  Building these two stochastic  models of linguistic
              knowledge, besides possible application in  speech recognition LMs, word-sense
              disambiguation, or parsing,  also helps bridge the gap between the structural
              models used in  linguistics and the statistical models of speech
              engineering.***
