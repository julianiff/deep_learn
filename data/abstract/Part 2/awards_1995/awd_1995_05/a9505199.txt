Title       : Mathematical Sciences: Approximation, Estimation, and Computation Properties of
               Neural Networks and Related Parsimonious Models
Type        : Award
NSF Org     : DMS 
Latest
Amendment
Date        : June 20,  1995      
File        : a9505199

Award Number: 9505199
Award Instr.: Standard Grant                               
Prgm Manager: James L. Rosenberger                    
	      DMS  DIVISION OF MATHEMATICAL SCIENCES       
	      MPS  DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN
Start Date  : July 1,  1995       
Expires     : June 30,  1999       (Estimated)
Expected
Total Amt.  : $120000             (Estimated)
Investigator: Lee K. Jones Lee_Jones@uml.edu  (Principal Investigator current)
              Yuly Makovoz  (Co-Principal Investigator current)
Sponsor     : Univ of Mass Lowell
	      600 Suffolk Street
	      Lowell, MA  018543602    978/934-4705

NSF Program : 1269      STATISTICS
Fld Applictn: 0000099   Other Applications NEC                  
              21        Mathematics                             
Program Ref : 0000,OTHR,
Abstract    :
               Proposals:  DMS 9505199   PIs: Lee Jones and Yuly Makovoz  Institution:
              University of Massachusetts at Lowell  Title:   APPROXIMATION, ESTIMATION, AND
              COMPUTATION PROPERTIES                 OF NEURAL NETWORKS AND RELATED
              PARSIMONIOUS MODELS         Abstract:    Artificial neural networks and related
              parsimonious models for function   approximation and estimation have attracted
              recent attention in science   and engineering.  Work by the authors has
              uncovered several interesting   aspects of these methods.  Approximation bounds
              have been obtained by   methods taken from the probability theory of empirical
              processes,   including bounds on the average squared error and the maximal
              error of   neural network and related approximations.  These approximation
              bounds   reveal a rate of convergence that is insensitive to the dimension of
              the   input space for certain nonparametric (infinite dimensional) classes of  
              functions, specified via the closure of convex hulls of finite dimensional 
              families of functions.  As a consequence accurate statistical estimation   of
              functions in these nonparametric classes is possible without recourse   to
              exponentially large sample sizes.  Unfortunately, computation of neural   net
              estimates can be an extremely difficult task.  The investigators study   how
              the problems of accurate approximation, estimation, and computation are  
              intertwined.  In this research they investigate fundamental mathematical,  
              statistical, and computational limits of the capacity to approximate and   to
              estimate these functions accurately by computationally feasible   algorithms.  
               Empirical modeling techniques used in a variety of scientific and  engineering
              tasks deal with the problem of how to combine a large  number of observable
              quantities to best predict or approximate   a response variable.   The input -
              response relation may be described   by a rather complicated function, and it
              may be desirable to   approximate it by a combination of a small number of   
              elementary, comparatively s impler, functions. These models   differ from
              classical techniques in approximation  and statistical estimation in that the
              functions that are combined   are not fixed in advance, but rather selected and
              adjusted according   to what is known or observed concerning the intended
              response variable   so as to provide the best fit.  The investigators are
              quantifying the   mathematical and statistical advantages of these  adjustable
              selections.  Artificial neural networks and related techniques are at the heart
               of modern models for adaptive and high performance computation.  The
              investigators study the limits of what is computationally   feasible with these
              models.  The ubiquity of requirements for accurate  prediction and empirical
              modeling for use of the scientific method  in general and for nationally
              strategic topics in particular are  motivating factors in this research.
