Title       : Computational Studies of Parameter Setting in Language
Type        : Award
NSF Org     : BCS 
Latest
Amendment
Date        : November 26,  1997  
File        : a9511167

Award Number: 9511167
Award Instr.: Continuing grant                             
Prgm Manager: Catherine N. Ball                       
	      BCS  DIVISION OF BEHAVIORAL AND COGNITIVE SCI
	      SBE  DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE
Start Date  : November 1,  1995   
Expires     : October 31,  1999    (Estimated)
Expected
Total Amt.  : $295000             (Estimated)
Investigator: Kenneth N. Wexler wexler@psyche.mit.edu  (Principal Investigator current)
              Edward A. Gibson  (Co-Principal Investigator current)
Sponsor     : MIT
	      77 Massachusetts Avenue
	      Cambridge, MA  021394307    617/253-1000

NSF Program : 1311      LINGUISTICS
Fld Applictn: 0000099   Other Applications NEC                  
              84        Linguistics                             
Program Ref : 9218,HPCC,
Abstract    :
              The problem of developing an appropriate and workable  computational theory of
              parameter-setting in language is a  central problem for linguistic theory,
              language acquisition,  learning theory and cognitive science in general. The
              purpose of  this grant is to make a significant increase, compared to  previous
              studies, in the size and scope of the syntactic  parameter spaces, and to do
              computational parameter-setting  investigations of these parameter spaces. We
              plan to investigate  the learning of a space of 256 grammars or larger, based
              on 8  parameters (plus a "lexical" parameter).   The major questions that we
              will investigate include:  1. With a larger set of parameters, does the theory
              of  parameter-setting work (i.e., for all natural languages stated in  terms of
              those parameters, does the algorithm converge)?  2. Do different algorithms
              work better than others?   3. What are the kinds of special assumptions about
              markedness or  default values that work in the specific parameter spaces that
              we  study? Are there general principles of markedness or default  values that
              seem to apply to many different parameters? Are  default values necessary in
              general?  4. What happens to computational results in parameter-setting  when
              we "scale up" ? That is, if a certain pattern of results  obtains for a
              parameter-space, do we find that this pattern  remains when the space is
              embedded in a larger space? Or were the  results artifacts of the smaller
              space?  5. Suppose that an algorithm converges on the correct 
              parameter-settings for a class of parameter-settings that we know  is
              instantiated (i.e., there are natural languages which exhibit  these
              parameter-settings), but doesn't converge for some other  parameter-settings.
              Can we show that these parameter-settings are  not instantiated in natural
              language, so that in fact there are  reasons of learnability that some
              parameter-settings don't show  up?   6. How fast do particular algorithms
              converge? This can be stated  in terms of the number of examples t hat need to
              be given. Are  some algorithms more realistic than others in this regard?  7.
              What is the relation of the properties of the algorithms to  empirical research
              in language acquisition? Can early properties  of acquisition be shown to
              relate to the algorithms?
