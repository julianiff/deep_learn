Title       : Visual Speech and Face Recognition: Common Information Attributes
Type        : Award
NSF Org     : BCS 
Latest
Amendment
Date        : April 9,  1999      
File        : a9617047

Award Number: 9617047
Award Instr.: Continuing grant                             
Prgm Manager: Joseph L. Young                         
	      BCS  DIVISION OF BEHAVIORAL AND COGNITIVE SCI
	      SBE  DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE
Start Date  : April 15,  1997     
Expires     : March 31,  2000      (Estimated)
Expected
Total Amt.  : $230025             (Estimated)
Investigator: Lawrence Rosenblum rosenblu@citrus.ucr.edu  (Principal Investigator current)
Sponsor     : U of Cal Riverside
	      Office of Research Affairs
	      Riverside, CA  925210217    909/787-5535

NSF Program : 1180      HUMAN COGNITION & PERCEPTION
Fld Applictn: 0116000   Human Subjects                          
Program Ref : 0000,OTHR,
Abstract    :
              9617047  ROSENBLUM  A series of experiments will test the relation between 
              speechreading (lipreading) and face recognition.  Traditionally,  the functions
              of recognizing speech and recognizing the speaker  have been thought to involve
              separate processes and informational  attributes.  However, recent observations
              with auditory speech  suggest that these two functions might not be as separate
              as once  thought.  The same question arises for the visual domain:  How do 
              speechreading and face recognition functions interact?  The first  set of
              experiments will test if facial familiarity facilitates  speechreading.  The
              second series of experiments will explore if  the relationship between
              speechreading and face recognition could  be based on the use of common visual
              information for articulator  movements.  The experiments in this series will
              make use of a  point-light technique which involves placing small illuminated 
              dots on an otherwise darkened face.  This technique serves to  isolate
              articulator movement information and can convey a great  deal of speech
              information.  The third series of experiments will  test whether face
              processing and speechreading depend on the two  cerebral hemispheres in the
              same or different ways.  These  experiments will vary the amount of dynamic
              information available  in the stimulus by using static, moving, and point-light
              images.   The results of these experiments will be illuminating about  theories
              of face and speech perception as well as the general  issue of the separability
              of perceptual processes.  The  experiments should also help determine the
              salient information  for speechreading and face recognition.  This research
              should  ultimately be useful for designing speech and face recognition  devices
              as well as speechreading programs for the hearing  impaired.  ***
