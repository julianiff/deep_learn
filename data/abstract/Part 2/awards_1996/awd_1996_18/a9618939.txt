Title       : STIMULATE: A Unified Framework for Multimodal Conversational Behaviors in
               Interactive Humanoid Agents
Type        : Award
NSF Org     : IIS 
Latest
Amendment
Date        : April 15,  1999     
File        : a9618939

Award Number: 9618939
Award Instr.: Continuing grant                             
Prgm Manager: Ephraim P. Glinert                      
	      IIS  DIV OF INFORMATION & INTELLIGENT SYSTEMS
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : March 1,  1997      
Expires     : February 28,  2001   (Estimated)
Expected
Total Amt.  : $725226             (Estimated)
Investigator: Justine M. Cassell justine@media.mit.edu  (Principal Investigator current)
Sponsor     : MIT
	      77 Massachusetts Avenue
	      Cambridge, MA  021394307    617/253-1000

NSF Program : 6845      HUMAN COMPUTER INTER PROGRAM
Fld Applictn: 0104000   Information Systems                     
Program Ref : 9139,HPCC,
Abstract    :
              Humans  communicate using speech, prosodic cues,  hand  gestures,  gaze  and 
              facial expression.  The focus of this project  is  the  interaction  among
              these modalities in humans and in  interactive  humanoid  communicating 
              agents.  The  goal  is  to  be  able  to  synthesize  and  understand  natural
              face-to-face  conversational  behavior --  that is, spontaneous gesture and
              facial movements in  the  context of speech with intonation. Two converging 
              lines  of  research  will  be  carried out:  first, a  series  of  empirical 
              experiments on the interaction among these modalities,  including  experiments 
              on  the  association between gestural  features  and  motion   verbs,   the  
              relationship  between   turn-taking   and  information  structure, the
              interaction between facial  movements  and gesture, and the distribution of
              listener-looking-at-speaker-  gesture.   Second,  the continued development of 
              an  interactive  humanoid   agent,  including  generating  gestural  features  
              in  association with dialogue, refining dialogue generation  to  take  into
              account turn-taking, distributing conversational load across  speech, gesture,
              and facial expression, and replacing the current  system of gesture input by
              dataglove with a stereo vision system.  The  results  of this research will
              contribute to theory  of  the  interaction  between  verbal and nonverbal 
              modalities  in  human  interaction, and aid in the development of autonomous
              agents  and  humanoid  interfaces  that depend on natural human  communicative 
              behavior for increased efficiency, naturalness and flexibility of  human -
              computer interaction.
