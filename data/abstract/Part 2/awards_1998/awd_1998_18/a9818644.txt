Title       : The Econometrics of Evaluating Social Programs
Type        : Award
NSF Org     : SES 
Latest
Amendment
Date        : March 7,  2002      
File        : a9818644

Award Number: 9818644
Award Instr.: Standard Grant                               
Prgm Manager: Laura Razzolini                         
	      SES  DIVN OF SOCIAL AND ECONOMIC SCIENCES    
	      SBE  DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE
Start Date  : April 1,  1999      
Expires     : September 30,  2002  (Estimated)
Expected
Total Amt.  : $142110             (Estimated)
Investigator: Guido Imbens imbens@econ.ucla.edu  (Principal Investigator current)
Sponsor     : NBER
	      1050 Massachusetts Avenue
	      Cambridge, MA  021385317    617/868-3900

NSF Program : 1320      ECONOMICS
Fld Applictn: 
Program Ref : 0000,OTHR,
Abstract    :
              


Consider a government contemplating the implementation of a training
              program. The decision to
 implement the program depends on the assessment of
              its likely effectiveness. Often the policy maker has available data from a
              previous implementation of the same program to inform this decision. There are
              two steps involved in exploiting data from previous training programs in
              predicting the effectiveness of the new program. First is the problem of using
              the data from the previous program to evaluate the effectiveness of that
              specific program. The second step is to generalize the results obtained for the
              old program to the new program. The current proposal focusses on the second
              step.
The investigator discusses how this problem is related to the standard
              evaluation problem, and how propensity score methods can be useful. He proposes
              developing extensions of these methods that were developed for the binary
              treatment case to allow for multivalued and continuous treatments. He also
              intends to apply these methods to two different data sets. In both cases
              randomized assignment was used so unbiased estimates of the true average effect
              are available to judge the performance of estimators based on the proposed
              methodology.



